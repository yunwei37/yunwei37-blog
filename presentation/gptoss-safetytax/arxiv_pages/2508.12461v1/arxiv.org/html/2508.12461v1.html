<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Is GPT-OSS Good? A Comprehensive Evaluation of OpenAI’s Latest Open Source Models ∗: Equal contribution. †: Corresponding author: Junhao Song (junhao.song23@imperial.ac.uk)</title>
<!--Generated on Sun Aug 17 18:24:02 2025 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="../../cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="../static/browse/0.3.4/css/ar5iv.0.8.2.min.css" rel="stylesheet" type="text/css"/>
<link href="../static/browse/0.3.4/css/ar5iv-fonts.0.8.2.min.css" rel="stylesheet" type="text/css"/>
<link href="../static/browse/0.3.4/css/latexml_styles.0.8.2.css" rel="stylesheet" type="text/css"/>
<script src="../../cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="../../cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="../static/browse/0.3.4/js/addons_new.js"></script>
<script src="../static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="
Large language models,  gpt-oss,  model evaluation,  benchmarking,  reasoning models,  mixture of experts,  performance analysis
" lang="en" name="keywords"/>
<base href=""/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="2508.12461v1.html#S1" title="In Is GPT-OSS Good? A Comprehensive Evaluation of OpenAI’s Latest Open Source Models ∗: Equal contribution. †: Corresponding author: Junhao Song (junhao.song23@imperial.ac.uk)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">I </span><span class="ltx_text ltx_font_smallcaps">Introduction</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="2508.12461v1.html#S2" title="In Is GPT-OSS Good? A Comprehensive Evaluation of OpenAI’s Latest Open Source Models ∗: Equal contribution. †: Corresponding author: Junhao Song (junhao.song23@imperial.ac.uk)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">II </span><span class="ltx_text ltx_font_smallcaps">Related Work</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="2508.12461v1.html#S2.SS1" title="In II Related Work ‣ Is GPT-OSS Good? A Comprehensive Evaluation of OpenAI’s Latest Open Source Models ∗: Equal contribution. †: Corresponding author: Junhao Song (junhao.song23@imperial.ac.uk)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">II-A </span><span class="ltx_text ltx_font_italic">Large Language Models Evaluation</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="2508.12461v1.html#S2.SS2" title="In II Related Work ‣ Is GPT-OSS Good? A Comprehensive Evaluation of OpenAI’s Latest Open Source Models ∗: Equal contribution. †: Corresponding author: Junhao Song (junhao.song23@imperial.ac.uk)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">II-B </span><span class="ltx_text ltx_font_italic">Code Generation Assessment</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="2508.12461v1.html#S2.SS3" title="In II Related Work ‣ Is GPT-OSS Good? A Comprehensive Evaluation of OpenAI’s Latest Open Source Models ∗: Equal contribution. †: Corresponding author: Junhao Song (junhao.song23@imperial.ac.uk)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">II-C </span><span class="ltx_text ltx_font_italic">Multilingual Model Evaluation</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="2508.12461v1.html#S2.SS4" title="In II Related Work ‣ Is GPT-OSS Good? A Comprehensive Evaluation of OpenAI’s Latest Open Source Models ∗: Equal contribution. †: Corresponding author: Junhao Song (junhao.song23@imperial.ac.uk)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">II-D </span><span class="ltx_text ltx_font_italic">Mixture of Experts Architectures</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="2508.12461v1.html#S2.SS5" title="In II Related Work ‣ Is GPT-OSS Good? A Comprehensive Evaluation of OpenAI’s Latest Open Source Models ∗: Equal contribution. †: Corresponding author: Junhao Song (junhao.song23@imperial.ac.uk)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">II-E </span><span class="ltx_text ltx_font_italic">Benchmark Limitations and Criticisms</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="2508.12461v1.html#S2.SS6" title="In II Related Work ‣ Is GPT-OSS Good? A Comprehensive Evaluation of OpenAI’s Latest Open Source Models ∗: Equal contribution. †: Corresponding author: Junhao Song (junhao.song23@imperial.ac.uk)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">II-F </span><span class="ltx_text ltx_font_italic">Open Source Model Ecosystem</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="2508.12461v1.html#S2.SS7" title="In II Related Work ‣ Is GPT-OSS Good? A Comprehensive Evaluation of OpenAI’s Latest Open Source Models ∗: Equal contribution. †: Corresponding author: Junhao Song (junhao.song23@imperial.ac.uk)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">II-G </span><span class="ltx_text ltx_font_italic">Performance Scaling Laws</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="2508.12461v1.html#S2.SS8" title="In II Related Work ‣ Is GPT-OSS Good? A Comprehensive Evaluation of OpenAI’s Latest Open Source Models ∗: Equal contribution. †: Corresponding author: Junhao Song (junhao.song23@imperial.ac.uk)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">II-H </span><span class="ltx_text ltx_font_italic">Advances in Evaluation Methodology</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="2508.12461v1.html#S2.SS9" title="In II Related Work ‣ Is GPT-OSS Good? A Comprehensive Evaluation of OpenAI’s Latest Open Source Models ∗: Equal contribution. †: Corresponding author: Junhao Song (junhao.song23@imperial.ac.uk)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">II-I </span><span class="ltx_text ltx_font_italic">Conversational Evaluation</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="2508.12461v1.html#S3" title="In Is GPT-OSS Good? A Comprehensive Evaluation of OpenAI’s Latest Open Source Models ∗: Equal contribution. †: Corresponding author: Junhao Song (junhao.song23@imperial.ac.uk)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">III </span><span class="ltx_text ltx_font_smallcaps">Methodology</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="2508.12461v1.html#S3.SS1" title="In III Methodology ‣ Is GPT-OSS Good? A Comprehensive Evaluation of OpenAI’s Latest Open Source Models ∗: Equal contribution. †: Corresponding author: Junhao Song (junhao.song23@imperial.ac.uk)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">III-A </span><span class="ltx_text ltx_font_italic">Model Selection</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="2508.12461v1.html#S3.SS2" title="In III Methodology ‣ Is GPT-OSS Good? A Comprehensive Evaluation of OpenAI’s Latest Open Source Models ∗: Equal contribution. †: Corresponding author: Junhao Song (junhao.song23@imperial.ac.uk)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">III-B </span><span class="ltx_text ltx_font_italic">Benchmark Selection</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="2508.12461v1.html#S3.SS3" title="In III Methodology ‣ Is GPT-OSS Good? A Comprehensive Evaluation of OpenAI’s Latest Open Source Models ∗: Equal contribution. †: Corresponding author: Junhao Song (junhao.song23@imperial.ac.uk)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">III-C </span><span class="ltx_text ltx_font_italic">Evaluation Protocol</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="2508.12461v1.html#S3.SS4" title="In III Methodology ‣ Is GPT-OSS Good? A Comprehensive Evaluation of OpenAI’s Latest Open Source Models ∗: Equal contribution. †: Corresponding author: Junhao Song (junhao.song23@imperial.ac.uk)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">III-D </span><span class="ltx_text ltx_font_italic">Statistical Analysis</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="2508.12461v1.html#S3.SS5" title="In III Methodology ‣ Is GPT-OSS Good? A Comprehensive Evaluation of OpenAI’s Latest Open Source Models ∗: Equal contribution. †: Corresponding author: Junhao Song (junhao.song23@imperial.ac.uk)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">III-E </span><span class="ltx_text ltx_font_italic">Quality Assurance</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="2508.12461v1.html#S3.SS6" title="In III Methodology ‣ Is GPT-OSS Good? A Comprehensive Evaluation of OpenAI’s Latest Open Source Models ∗: Equal contribution. †: Corresponding author: Junhao Song (junhao.song23@imperial.ac.uk)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">III-F </span><span class="ltx_text ltx_font_italic">Limitations and Ethical Considerations</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="2508.12461v1.html#S4" title="In Is GPT-OSS Good? A Comprehensive Evaluation of OpenAI’s Latest Open Source Models ∗: Equal contribution. †: Corresponding author: Junhao Song (junhao.song23@imperial.ac.uk)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">IV </span><span class="ltx_text ltx_font_smallcaps">Experimental Setup</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="2508.12461v1.html#S4.SS1" title="In IV Experimental Setup ‣ Is GPT-OSS Good? A Comprehensive Evaluation of OpenAI’s Latest Open Source Models ∗: Equal contribution. †: Corresponding author: Junhao Song (junhao.song23@imperial.ac.uk)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">IV-A </span><span class="ltx_text ltx_font_italic">Hardware Infrastructure</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="2508.12461v1.html#S4.SS2" title="In IV Experimental Setup ‣ Is GPT-OSS Good? A Comprehensive Evaluation of OpenAI’s Latest Open Source Models ∗: Equal contribution. †: Corresponding author: Junhao Song (junhao.song23@imperial.ac.uk)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">IV-B </span><span class="ltx_text ltx_font_italic">Model Configuration</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="2508.12461v1.html#S4.SS2.SSS1" title="In IV-B Model Configuration ‣ IV Experimental Setup ‣ Is GPT-OSS Good? A Comprehensive Evaluation of OpenAI’s Latest Open Source Models ∗: Equal contribution. †: Corresponding author: Junhao Song (junhao.song23@imperial.ac.uk)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">IV-B1 </span>Generation Parameters</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="2508.12461v1.html#S4.SS3" title="In IV Experimental Setup ‣ Is GPT-OSS Good? A Comprehensive Evaluation of OpenAI’s Latest Open Source Models ∗: Equal contribution. †: Corresponding author: Junhao Song (junhao.song23@imperial.ac.uk)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">IV-C </span><span class="ltx_text ltx_font_italic">Data Preparation</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="2508.12461v1.html#S4.SS3.SSS1" title="In IV-C Data Preparation ‣ IV Experimental Setup ‣ Is GPT-OSS Good? A Comprehensive Evaluation of OpenAI’s Latest Open Source Models ∗: Equal contribution. †: Corresponding author: Junhao Song (junhao.song23@imperial.ac.uk)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">IV-C1 </span>Dataset Loading and Preprocessing</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="2508.12461v1.html#S4.SS3.SSS2" title="In IV-C Data Preparation ‣ IV Experimental Setup ‣ Is GPT-OSS Good? A Comprehensive Evaluation of OpenAI’s Latest Open Source Models ∗: Equal contribution. †: Corresponding author: Junhao Song (junhao.song23@imperial.ac.uk)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">IV-C2 </span>Data Quality Assurance</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="2508.12461v1.html#S4.SS4" title="In IV Experimental Setup ‣ Is GPT-OSS Good? A Comprehensive Evaluation of OpenAI’s Latest Open Source Models ∗: Equal contribution. †: Corresponding author: Junhao Song (junhao.song23@imperial.ac.uk)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">IV-D </span><span class="ltx_text ltx_font_italic">Execution Pipeline</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="2508.12461v1.html#S4.SS4.SSS1" title="In IV-D Execution Pipeline ‣ IV Experimental Setup ‣ Is GPT-OSS Good? A Comprehensive Evaluation of OpenAI’s Latest Open Source Models ∗: Equal contribution. †: Corresponding author: Junhao Song (junhao.song23@imperial.ac.uk)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">IV-D1 </span>Automated Evaluation Workflow</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="2508.12461v1.html#S4.SS4.SSS2" title="In IV-D Execution Pipeline ‣ IV Experimental Setup ‣ Is GPT-OSS Good? A Comprehensive Evaluation of OpenAI’s Latest Open Source Models ∗: Equal contribution. †: Corresponding author: Junhao Song (junhao.song23@imperial.ac.uk)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">IV-D2 </span>Error Handling and Recovery</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="2508.12461v1.html#S4.SS5" title="In IV Experimental Setup ‣ Is GPT-OSS Good? A Comprehensive Evaluation of OpenAI’s Latest Open Source Models ∗: Equal contribution. †: Corresponding author: Junhao Song (junhao.song23@imperial.ac.uk)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">IV-E </span><span class="ltx_text ltx_font_italic">Monitoring and Logging</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="2508.12461v1.html#S4.SS5.SSS1" title="In IV-E Monitoring and Logging ‣ IV Experimental Setup ‣ Is GPT-OSS Good? A Comprehensive Evaluation of OpenAI’s Latest Open Source Models ∗: Equal contribution. †: Corresponding author: Junhao Song (junhao.song23@imperial.ac.uk)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">IV-E1 </span>Performance Monitoring</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="2508.12461v1.html#S4.SS5.SSS2" title="In IV-E Monitoring and Logging ‣ IV Experimental Setup ‣ Is GPT-OSS Good? A Comprehensive Evaluation of OpenAI’s Latest Open Source Models ∗: Equal contribution. †: Corresponding author: Junhao Song (junhao.song23@imperial.ac.uk)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">IV-E2 </span>Logging Infrastructure</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="2508.12461v1.html#S4.SS6" title="In IV Experimental Setup ‣ Is GPT-OSS Good? A Comprehensive Evaluation of OpenAI’s Latest Open Source Models ∗: Equal contribution. †: Corresponding author: Junhao Song (junhao.song23@imperial.ac.uk)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">IV-F </span><span class="ltx_text ltx_font_italic">Experimental Controls</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="2508.12461v1.html#S4.SS6.SSS1" title="In IV-F Experimental Controls ‣ IV Experimental Setup ‣ Is GPT-OSS Good? A Comprehensive Evaluation of OpenAI’s Latest Open Source Models ∗: Equal contribution. †: Corresponding author: Junhao Song (junhao.song23@imperial.ac.uk)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">IV-F1 </span>Randomisation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="2508.12461v1.html#S4.SS6.SSS2" title="In IV-F Experimental Controls ‣ IV Experimental Setup ‣ Is GPT-OSS Good? A Comprehensive Evaluation of OpenAI’s Latest Open Source Models ∗: Equal contribution. †: Corresponding author: Junhao Song (junhao.song23@imperial.ac.uk)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">IV-F2 </span>Blind Evaluation Protocol</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="2508.12461v1.html#S5" title="In Is GPT-OSS Good? A Comprehensive Evaluation of OpenAI’s Latest Open Source Models ∗: Equal contribution. †: Corresponding author: Junhao Song (junhao.song23@imperial.ac.uk)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">V </span><span class="ltx_text ltx_font_smallcaps">Results and Analysis</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="2508.12461v1.html#S5.SS1" title="In V Results and Analysis ‣ Is GPT-OSS Good? A Comprehensive Evaluation of OpenAI’s Latest Open Source Models ∗: Equal contribution. †: Corresponding author: Junhao Song (junhao.song23@imperial.ac.uk)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">V-A </span><span class="ltx_text ltx_font_italic">Performance Overview</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="2508.12461v1.html#S5.SS2" title="In V Results and Analysis ‣ Is GPT-OSS Good? A Comprehensive Evaluation of OpenAI’s Latest Open Source Models ∗: Equal contribution. †: Corresponding author: Junhao Song (junhao.song23@imperial.ac.uk)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">V-B </span><span class="ltx_text ltx_font_italic">Knowledge and Understanding</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="2508.12461v1.html#S5.SS3" title="In V Results and Analysis ‣ Is GPT-OSS Good? A Comprehensive Evaluation of OpenAI’s Latest Open Source Models ∗: Equal contribution. †: Corresponding author: Junhao Song (junhao.song23@imperial.ac.uk)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">V-C </span><span class="ltx_text ltx_font_italic">Reasoning and Problem Solving</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="2508.12461v1.html#S5.SS4" title="In V Results and Analysis ‣ Is GPT-OSS Good? A Comprehensive Evaluation of OpenAI’s Latest Open Source Models ∗: Equal contribution. †: Corresponding author: Junhao Song (junhao.song23@imperial.ac.uk)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">V-D </span><span class="ltx_text ltx_font_italic">Code Generation Capabilities</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="2508.12461v1.html#S5.SS5" title="In V Results and Analysis ‣ Is GPT-OSS Good? A Comprehensive Evaluation of OpenAI’s Latest Open Source Models ∗: Equal contribution. †: Corresponding author: Junhao Song (junhao.song23@imperial.ac.uk)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">V-E </span><span class="ltx_text ltx_font_italic">Case Study: Logic Reasoning Task with Response Quality Analysis</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="2508.12461v1.html#S5.SS5.SSS1" title="In V-E Case Study: Logic Reasoning Task with Response Quality Analysis ‣ V Results and Analysis ‣ Is GPT-OSS Good? A Comprehensive Evaluation of OpenAI’s Latest Open Source Models ∗: Equal contribution. †: Corresponding author: Junhao Song (junhao.song23@imperial.ac.uk)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">V-E1 </span>Task Design and Evaluation Framework</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="2508.12461v1.html#S5.SS5.SSS2" title="In V-E Case Study: Logic Reasoning Task with Response Quality Analysis ‣ V Results and Analysis ‣ Is GPT-OSS Good? A Comprehensive Evaluation of OpenAI’s Latest Open Source Models ∗: Equal contribution. †: Corresponding author: Junhao Song (junhao.song23@imperial.ac.uk)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">V-E2 </span>Quality Dimensions Analysis</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="2508.12461v1.html#S5.SS5.SSS3" title="In V-E Case Study: Logic Reasoning Task with Response Quality Analysis ‣ V Results and Analysis ‣ Is GPT-OSS Good? A Comprehensive Evaluation of OpenAI’s Latest Open Source Models ∗: Equal contribution. †: Corresponding author: Junhao Song (junhao.song23@imperial.ac.uk)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">V-E3 </span>Key Findings and Implications</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="2508.12461v1.html#S5.SS5.SSS4" title="In V-E Case Study: Logic Reasoning Task with Response Quality Analysis ‣ V Results and Analysis ‣ Is GPT-OSS Good? A Comprehensive Evaluation of OpenAI’s Latest Open Source Models ∗: Equal contribution. †: Corresponding author: Junhao Song (junhao.song23@imperial.ac.uk)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">V-E4 </span>Response Quality Distribution</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="2508.12461v1.html#S5.SS6" title="In V Results and Analysis ‣ Is GPT-OSS Good? A Comprehensive Evaluation of OpenAI’s Latest Open Source Models ∗: Equal contribution. †: Corresponding author: Junhao Song (junhao.song23@imperial.ac.uk)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">V-F </span><span class="ltx_text ltx_font_italic">Domain-Specific Performance</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="2508.12461v1.html#S5.SS7" title="In V Results and Analysis ‣ Is GPT-OSS Good? A Comprehensive Evaluation of OpenAI’s Latest Open Source Models ∗: Equal contribution. †: Corresponding author: Junhao Song (junhao.song23@imperial.ac.uk)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">V-G </span><span class="ltx_text ltx_font_italic">Conversational Coherence</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="2508.12461v1.html#S5.SS8" title="In V Results and Analysis ‣ Is GPT-OSS Good? A Comprehensive Evaluation of OpenAI’s Latest Open Source Models ∗: Equal contribution. †: Corresponding author: Junhao Song (junhao.song23@imperial.ac.uk)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">V-H </span><span class="ltx_text ltx_font_italic">Computational Efficiency Analysis</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="2508.12461v1.html#S5.SS9" title="In V Results and Analysis ‣ Is GPT-OSS Good? A Comprehensive Evaluation of OpenAI’s Latest Open Source Models ∗: Equal contribution. †: Corresponding author: Junhao Song (junhao.song23@imperial.ac.uk)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">V-I </span><span class="ltx_text ltx_font_italic">Statistical Significance and Reliability</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="2508.12461v1.html#S5.SS10" title="In V Results and Analysis ‣ Is GPT-OSS Good? A Comprehensive Evaluation of OpenAI’s Latest Open Source Models ∗: Equal contribution. †: Corresponding author: Junhao Song (junhao.song23@imperial.ac.uk)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">V-J </span><span class="ltx_text ltx_font_italic">Efficiency Evaluation</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="2508.12461v1.html#S5.SS11" title="In V Results and Analysis ‣ Is GPT-OSS Good? A Comprehensive Evaluation of OpenAI’s Latest Open Source Models ∗: Equal contribution. †: Corresponding author: Junhao Song (junhao.song23@imperial.ac.uk)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">V-K </span><span class="ltx_text ltx_font_italic">Mutlilingual Capability </span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="2508.12461v1.html#S5.SS12" title="In V Results and Analysis ‣ Is GPT-OSS Good? A Comprehensive Evaluation of OpenAI’s Latest Open Source Models ∗: Equal contribution. †: Corresponding author: Junhao Song (junhao.song23@imperial.ac.uk)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">V-L </span><span class="ltx_text ltx_font_italic">Synthesis and Implications</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="2508.12461v1.html#S6" title="In Is GPT-OSS Good? A Comprehensive Evaluation of OpenAI’s Latest Open Source Models ∗: Equal contribution. †: Corresponding author: Junhao Song (junhao.song23@imperial.ac.uk)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">VI </span><span class="ltx_text ltx_font_smallcaps">Conclusion</span></span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Is GPT-OSS Good? A Comprehensive Evaluation of OpenAI’s Latest Open Source Models
<span class="ltx_note ltx_role_thanks"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">thanks: </span><span class="ltx_rule" style="width:108.1pt;height:0.4pt;background:black;display:inline-block;"> </span>
<br class="ltx_break"/><math alttext="*" class="ltx_Math" display="inline" id="m1"><semantics><mo>∗</mo><annotation encoding="application/x-tex">*</annotation></semantics></math>: Equal contribution. 
<br class="ltx_break"/><math alttext="\dagger" class="ltx_Math" display="inline" id="m2"><semantics><mo>†</mo><annotation encoding="application/x-tex">\dagger</annotation></semantics></math>: Corresponding author: Junhao Song (junhao.song23@imperial.ac.uk)</span></span></span>
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">

Ziqian Bi<sup class="ltx_sup">1,2*</sup>,
Keyu Chen<sup class="ltx_sup">1,3*</sup>,
Chiung-Yi Tseng<sup class="ltx_sup">1,4*</sup>,
Danyang Zhang<sup class="ltx_sup">1,5*</sup>,
Tianyang Wang<sup class="ltx_sup">1</sup>,
<br class="ltx_break"/>Hongying Luo<sup class="ltx_sup">1</sup>,
Lu Chen<sup class="ltx_sup">1</sup>,
Junming Huang<sup class="ltx_sup">1</sup>,
Jibin Guan<sup class="ltx_sup">6</sup>,
Junfeng Hao<sup class="ltx_sup">6</sup>,
Junhao Song<sup class="ltx_sup">7†</sup>
<br class="ltx_break"/>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><sup class="ltx_sup">1</sup>AI Agent Lab, Vokram Group, United Kingdom, ai-agent-lab@vokram.com
</span>
<span class="ltx_contact ltx_role_affiliation"><sup class="ltx_sup">2</sup>Purdue University, United States, bi32@purdue.edu
</span>
<span class="ltx_contact ltx_role_affiliation"><sup class="ltx_sup">3</sup>Georgia Institute of Technology, United States, kchen637@gatech.edu
</span>
<span class="ltx_contact ltx_role_affiliation"><sup class="ltx_sup">4</sup>LuxMuse AI, United States, ctseng@luxmuse.ai
</span>
<span class="ltx_contact ltx_role_affiliation"><sup class="ltx_sup">5</sup>ByteDance Inc, United States, joseph.zhang@bytedance.com
</span>
<span class="ltx_contact ltx_role_affiliation"><sup class="ltx_sup">6</sup>University of Minnesota, United States, jguan@umn.edu, ygzhjf85@gmail.com
</span>
<span class="ltx_contact ltx_role_affiliation"><sup class="ltx_sup">7</sup>Imperial College London, United Kingdom, junhao.song23@imperial.ac.uk
</span></span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p">In August 2025, OpenAI released GPT-OSS models, its first open weight large language models since GPT-2 in 2019, comprising two mixture of experts architectures with 120B and 20B parameters. We evaluated both variants against six contemporary open source large language models ranging from 14.7B to 235B parameters, representing both dense and sparse designs, across ten benchmarks covering general knowledge, mathematical reasoning, code generation, multilingual understanding, and conversational ability. All models were tested in unquantised form under standardised inference settings, with statistical validation using McNemar’s test and effect size analysis. Results show that gpt-oss-20B consistently outperforms gpt-oss-120B on several benchmarks, such as HumanEval and MMLU, despite requiring substantially less memory and energy per response. Both models demonstrate mid-tier overall performance within the current open source landscape, with relative strength in code generation and notable weaknesses in multilingual tasks. These findings provide empirical evidence that scaling in sparse architectures may not yield proportional performance gains, underscoring the need for further investigation into optimisation strategies and informing more efficient model selection for future open source deployments.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
Large language models, gpt-oss, model evaluation, benchmarking, reasoning models, mixture of experts, performance analysis

</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p">The past five years have seen a rapid expansion of open source large language models (LLMs), with dense architectures such as Llama and Gemma joined by increasingly capable mixture of experts (MoE) designs. MoE architectures activate only a fraction of parameters per token <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="2508.12461v1.html#bib.bib1" title="">1</a>, <a class="ltx_ref" href="2508.12461v1.html#bib.bib2" title="">2</a>]</cite>, offering a theoretical path to improved compute efficiency and scalability <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="2508.12461v1.html#bib.bib3" title="">3</a>, <a class="ltx_ref" href="2508.12461v1.html#bib.bib4" title="">4</a>, <a class="ltx_ref" href="2508.12461v1.html#bib.bib5" title="">5</a>]</cite>. In August 2025, OpenAI released GPT OSS models, its first open weight models since GPT-2 in 2019 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="2508.12461v1.html#bib.bib6" title="">6</a>]</cite>, introducing MoE variants with 20B and 120B total parameters into a landscape already shaped by competitive offerings from Meta <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="2508.12461v1.html#bib.bib7" title="">7</a>]</cite>, Google <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="2508.12461v1.html#bib.bib8" title="">8</a>]</cite>, DeepSeek <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="2508.12461v1.html#bib.bib9" title="">9</a>]</cite>, Alibaba <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="2508.12461v1.html#bib.bib10" title="">10</a>, <a class="ltx_ref" href="2508.12461v1.html#bib.bib11" title="">11</a>]</cite> and Microsoft <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="2508.12461v1.html#bib.bib12" title="">12</a>]</cite>.</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="663" id="S1.F1.g1" src="2508.12461v1/x1.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span><span class="ltx_text ltx_font_bold">Multi-dimensional performance comparison across eight evaluated models.</span> GPT-OSS models (highlighted) show middle-tier performance with particular strength in code generation but weakness in multilingual tasks.</figcaption>
</figure>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p">While scaling laws have been widely used to guide LLM development, empirical studies on their validity for sparse architectures remain limited. Existing evaluations have often focused on single domains or isolated benchmarks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="2508.12461v1.html#bib.bib13" title="">13</a>, <a class="ltx_ref" href="2508.12461v1.html#bib.bib14" title="">14</a>]</cite>, leaving open the question of whether larger MoE variants consistently outperform smaller ones across diverse capabilities. This gap matters both for understanding the training dynamics of sparsely activated networks and for making informed deployment choices under resource constraints.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p">To address this gap, we position GPT-OSS alongside six contemporary open source LLMs spanning 14.7B to 235B parameters, covering both dense and sparse architectures. We design a controlled evaluation across ten established benchmarks grouped into five capability domains: general understanding, mathematical reasoning, code generation, multilingual comprehension, and conversational ability. This framework enables a direct comparison of performance and efficiency characteristics, isolating the relationship between parameter scale, architecture type and task outcomes.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p">Our analysis serves two purposes. From a research perspective, it tests whether observed performance trends align with established scaling expectations in the MoE setting. From a practical perspective, it examines the cost performance profile of GPT-OSS relative to strong open source baselines, incorporating memory, latency, and energy measurements that are often omitted from academic reports. The results, presented in the following sections, provide evidence relevant to both theoretical modelling of scaling behaviour and the design of efficient deployment strategies in the evolving open source community.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span class="ltx_text ltx_font_smallcaps">Related Work</span>
</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">II-A </span><span class="ltx_text ltx_font_italic">Large Language Models Evaluation</span>
</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p">The evaluation of large language models has evolved significantly since the introduction of transformer architectures. Early work by Radford et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="2508.12461v1.html#bib.bib6" title="">6</a>]</cite> laid the foundation for systematic LLM evaluation through the development of benchmarks like WebText and subsequent iterations. The field has since developed comprehensive evaluation frameworks addressing multiple dimensions of model capabilities.</p>
</div>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p">Hendrycks et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="2508.12461v1.html#bib.bib15" title="">15</a>]</cite> introduced MMLU (Massive Multitask Language Understanding) as a comprehensive benchmark spanning 57 subjects, providing broad coverage of human knowledge domains. This benchmark has become a standard metric for assessing general understanding capabilities, though recent work highlights potential limitations including dataset contamination and the need for more challenging evaluation scenarios.</p>
</div>
<div class="ltx_para" id="S2.SS1.p3">
<p class="ltx_p">The evaluation of mathematical reasoning capabilities has been extensively studied through benchmarks like GSM8K, introduced by Cobbe et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="2508.12461v1.html#bib.bib16" title="">16</a>]</cite>, which tests multi-step problem-solving abilities. Subsequent work by Lewkowycz et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="2508.12461v1.html#bib.bib17" title="">17</a>]</cite> with Minerva demonstrated that specialised training on mathematical content can significantly improve performance, raising questions about the generalisation of reasoning capabilities across domains.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">II-B </span><span class="ltx_text ltx_font_italic">Code Generation Assessment</span>
</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p">The HumanEval benchmark, developed by Chen et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="2508.12461v1.html#bib.bib18" title="">18</a>]</cite>, established a rigorous framework for evaluating code generation capabilities through functional correctness testing. This work has been extended by Austin et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="2508.12461v1.html#bib.bib19" title="">19</a>]</cite> with MBPP (Mostly Basic Python Problems), and by Li et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="2508.12461v1.html#bib.bib20" title="">20</a>]</cite> with additional multilingual code generation benchmarks. Recent studies by Nijkamp et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="2508.12461v1.html#bib.bib21" title="">21</a>]</cite> demonstrate that code-specific pretraining significantly impacts performance, suggesting that general purpose models may face inherent limitations in programming tasks. The emergence of specialised coding models like Codex <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="2508.12461v1.html#bib.bib18" title="">18</a>]</cite> and CodeGen <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="2508.12461v1.html#bib.bib21" title="">21</a>]</cite> has prompted renewed interest in understanding the relationship between model architecture and code generation capabilities. Fried et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="2508.12461v1.html#bib.bib22" title="">22</a>]</cite> showed that instruction-tuned models can achieve competitive performance with significantly fewer parameters, challenging assumptions about scale requirements for code generation tasks.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">II-C </span><span class="ltx_text ltx_font_italic">Multilingual Model Evaluation</span>
</h3>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p">Cross-lingual evaluation has gained prominence with the increasing deployment of LLMs in global contexts. The C-Eval benchmark, introduced by Huang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="2508.12461v1.html#bib.bib23" title="">23</a>]</cite>, provides comprehensive coverage of Chinese language understanding across multiple disciplines. Similar efforts include MMLU variants for other languages and the XTREME benchmark suite for cross-lingual transfer evaluation.</p>
</div>
<div class="ltx_para" id="S2.SS3.p2">
<p class="ltx_p">Research by Winata et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="2508.12461v1.html#bib.bib24" title="">24</a>]</cite> demonstrates that multilingual capabilities often come at the cost of monolingual performance, a phenomenon termed the “curse of multilinguality.” This trade-off is particularly relevant for models like GPT-OSS that aim for broad language coverage without language-specific optimisation.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">II-D </span><span class="ltx_text ltx_font_italic">Mixture of Experts Architectures</span>
</h3>
<div class="ltx_para" id="S2.SS4.p1">
<p class="ltx_p">The mixture of experts paradigm has experienced renewed interest with models like Switch Transformer by Fedus et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="2508.12461v1.html#bib.bib2" title="">2</a>]</cite> and GLaM by Du et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="2508.12461v1.html#bib.bib25" title="">25</a>]</cite>, demonstrating that sparse models can achieve competitive performance with reduced computational requirements. The routing mechanisms and expert specialisation patterns in these models have been extensively studied by Lewis et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="2508.12461v1.html#bib.bib26" title="">26</a>]</cite>, revealing complex dynamics between model capacity and task performance.</p>
</div>
<div class="ltx_para" id="S2.SS4.p2">
<p class="ltx_p">Recent work, such as the branch train merge approach from Artetxe et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="2508.12461v1.html#bib.bib27" title="">27</a>]</cite> and the Expert Choice routing from Zhou et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="2508.12461v1.html#bib.bib28" title="">28</a>]</cite>, has advanced our understanding of optimal MoE configurations. These studies provide context for understanding the performance characteristics of GPT-OSS’s MoE implementation, particularly the unexpected inverse scaling observed between the 120B and 20B variants.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">II-E </span><span class="ltx_text ltx_font_italic">Benchmark Limitations and Criticisms</span>
</h3>
<div class="ltx_para" id="S2.SS5.p1">
<p class="ltx_p">Critical examination of evaluation methodologies has revealed significant limitations in current benchmarking practices. Bowman <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="2508.12461v1.html#bib.bib29" title="">29</a>]</cite> argues that static benchmarks suffer from dataset contamination and overfitting, proposing dynamic evaluation protocols that adapt over time. The work of Raji et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="2508.12461v1.html#bib.bib30" title="">30</a>]</cite> highlights biases in benchmark construction that may favour certain model architectures or training approaches. The challenge of evaluating emergent capabilities not captured by traditional benchmarks has been addressed by Wei et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="2508.12461v1.html#bib.bib13" title="">13</a>]</cite>, who demonstrate that many important model behaviours only manifest at certain scales or with specific prompting strategies. This work suggests that our evaluation of GPT-OSS models may not fully capture their potential, particularly in domains requiring complex reasoning or creativity.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS6">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">II-F </span><span class="ltx_text ltx_font_italic">Open Source Model Ecosystem</span>
</h3>
<div class="ltx_para" id="S2.SS6.p1">
<p class="ltx_p">The open source LLMs ecosystem has expanded dramatically following the release of models like LLaMA by Touvron et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="2508.12461v1.html#bib.bib7" title="">7</a>, <a class="ltx_ref" href="2508.12461v1.html#bib.bib31" title="">31</a>]</cite> and Falcon by Almazrouei et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="2508.12461v1.html#bib.bib32" title="">32</a>]</cite>. These releases have enabled extensive community-driven research and development, leading to rapid improvements in model capabilities and efficiency. The work of Dettmers et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="2508.12461v1.html#bib.bib33" title="">33</a>]</cite> on quantisation techniques has made large models accessible to researchers with limited computational resources.</p>
</div>
<div class="ltx_para" id="S2.SS6.p2">
<p class="ltx_p">The impact of open source models on research velocity has been documented by Bommasani et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="2508.12461v1.html#bib.bib34" title="">34</a>]</cite>, who argue that accessible models accelerate innovation while raising concerns about misuse and safety. The release of GPT-OSS represents a significant addition to this ecosystem, potentially shaping future development and research priorities.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS7">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">II-G </span><span class="ltx_text ltx_font_italic">Performance Scaling Laws</span>
</h3>
<div class="ltx_para" id="S2.SS7.p1">
<p class="ltx_p">Kaplan et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="2508.12461v1.html#bib.bib3" title="">3</a>]</cite> established foundational scaling laws relating model size, dataset size, and compute budget to model performance. Subsequent work by Hoffmann et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="2508.12461v1.html#bib.bib5" title="">5</a>]</cite> with Chinchilla challenged these assumptions, showing that many large models had been significantly undertrained. These findings have important implications for interpreting the performance of GPT-OSS models, particularly the unexpected inverse scaling between the 120B and 20B variants. Research by Schaeffer et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="2508.12461v1.html#bib.bib14" title="">14</a>]</cite> on emergent abilities suggests that performance scaling is more complex than initially understood. The presence of phase transitions and non-monotonic scaling in certain capabilities complicates predictions of model behaviour across scales.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS8">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">II-H </span><span class="ltx_text ltx_font_italic">Advances in Evaluation Methodology</span>
</h3>
<div class="ltx_para" id="S2.SS8.p1">
<p class="ltx_p">Work by Liang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="2508.12461v1.html#bib.bib35" title="">35</a>]</cite> has advanced the development of rigorous evaluation methodologies through the HELM framework, which provides standardised protocols for comprehensive model assessment. The importance of statistical significance testing and effect size reporting has been emphasised by Card et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="2508.12461v1.html#bib.bib36" title="">36</a>]</cite>, addressing reproducibility concerns in NLP research.</p>
</div>
<div class="ltx_para" id="S2.SS8.p2">
<p class="ltx_p">Recent work on contamination detection by Sainz et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="2508.12461v1.html#bib.bib37" title="">37</a>]</cite> highlights the need for careful experimental design and transparent reporting. Our methodology incorporates these considerations through careful sampling, statistical analysis, and comprehensive documentation of experimental procedures.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS9">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">II-I </span><span class="ltx_text ltx_font_italic">Conversational Evaluation</span>
</h3>
<div class="ltx_para" id="S2.SS9.p1">
<p class="ltx_p">The evaluation of conversational capabilities has evolved from single-turn assessment to complex multi-turn dialogue evaluation. The MT-Bench framework by Zheng et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="2508.12461v1.html#bib.bib38" title="">38</a>]</cite> provides a structured evaluation of conversational coherence and helpfulness, while work by Thoppilan et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="2508.12461v1.html#bib.bib39" title="">39</a>]</cite> on LaMDA demonstrates the importance of safety and factual grounding in dialogue systems.</p>
</div>
<div class="ltx_para" id="S2.SS9.p2">
<p class="ltx_p">Research by Roller et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="2508.12461v1.html#bib.bib40" title="">40</a>]</cite> on dialogue model evaluation highlights the challenge of balancing objectives, including engagement, consistency and informativeness. These considerations are particularly relevant for assessing GPT-OSS models’ conversational capabilities, where we observe trade-offs between response quality and computational efficiency.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span class="ltx_text ltx_font_smallcaps">Methodology</span>
</h2>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">III-A </span><span class="ltx_text ltx_font_italic">Model Selection</span>
</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p">Our evaluation encompasses eight contemporary large language models, selected to represent diverse architectural approaches, parameter scales, and development philosophies. The selection criteria, informed by recent survey work <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="2508.12461v1.html#bib.bib41" title="">41</a>, <a class="ltx_ref" href="2508.12461v1.html#bib.bib42" title="">42</a>]</cite>, prioritized: (1) availability through standardised interfaces <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="2508.12461v1.html#bib.bib43" title="">43</a>]</cite>, (2) representation of major research institutions and companies, (3) variation in model sizes from 14.7B to 235B parameters, and (4) inclusion of both dense and sparse architectures <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="2508.12461v1.html#bib.bib2" title="">2</a>, <a class="ltx_ref" href="2508.12461v1.html#bib.bib44" title="">44</a>]</cite>. Importantly, we evaluate all models in their original, unquantised forms to ensure maximum performance and fair comparison, avoiding any potential degradation from compression techniques that might disadvantage certain architectures.</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p">The evaluated models span multiple architectural paradigms. GPT-OSS models employ MoE architectures similar to Switch Transformer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="2508.12461v1.html#bib.bib2" title="">2</a>]</cite> and GShard <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="2508.12461v1.html#bib.bib44" title="">44</a>]</cite>, while dense models like Llama 3.3 follow the traditional transformer architecture <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="2508.12461v1.html#bib.bib45" title="">45</a>]</cite>. Recent work by Rae et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="2508.12461v1.html#bib.bib46" title="">46</a>]</cite> and Chowdhery et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="2508.12461v1.html#bib.bib47" title="">47</a>]</cite> has demonstrated that both approaches can achieve strong performance, though with different computational trade-offs. Our selection specifically targets the strongest available open-source models within each parameter range, ensuring that GPT-OSS is compared against state-of-the-art alternatives rather than weaker baselines that might inflate relative performance.</p>
</div>
<figure class="ltx_table" id="S3.T1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE I: </span>Overview of evaluated models.</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold">Model</span></td>
<td class="ltx_td ltx_align_left ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold">Parameters (B)</span></td>
<td class="ltx_td ltx_align_left ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold">Size (GB)</span></td>
<td class="ltx_td ltx_align_left ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold">Architecture</span></td>
</tr>
<tr class="ltx_tr" style="background-color:#EBEEF5;">
<td class="ltx_td ltx_align_left ltx_border_t" colspan="4" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" style="background-color:#EBEEF5;">MoE Models)</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left" style="padding-left:4.0pt;padding-right:4.0pt;">Qwen 3 235B</td>
<td class="ltx_td ltx_align_left" style="padding-left:4.0pt;padding-right:4.0pt;">
<math alttext="235" class="ltx_Math" display="inline" id="S3.T1.m1"><semantics><mn>235</mn><annotation encoding="application/x-tex">235</annotation></semantics></math> (<span class="ltx_text ltx_font_italic">22</span>)</td>
<td class="ltx_td ltx_align_left" style="padding-left:4.0pt;padding-right:4.0pt;"><math alttext="470" class="ltx_Math" display="inline" id="S3.T1.m2"><semantics><mn>470</mn><annotation encoding="application/x-tex">470</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left" style="padding-left:4.0pt;padding-right:4.0pt;">MoE</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left" style="padding-left:4.0pt;padding-right:4.0pt;">GPT-OSS 120B</td>
<td class="ltx_td ltx_align_left" style="padding-left:4.0pt;padding-right:4.0pt;">
<math alttext="117" class="ltx_Math" display="inline" id="S3.T1.m3"><semantics><mn>117</mn><annotation encoding="application/x-tex">117</annotation></semantics></math> (<span class="ltx_text ltx_font_italic">5.1</span>)</td>
<td class="ltx_td ltx_align_left" style="padding-left:4.0pt;padding-right:4.0pt;"><math alttext="234" class="ltx_Math" display="inline" id="S3.T1.m4"><semantics><mn>234</mn><annotation encoding="application/x-tex">234</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left" style="padding-left:4.0pt;padding-right:4.0pt;">MoE</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left" style="padding-left:4.0pt;padding-right:4.0pt;">Llama 4 Scout</td>
<td class="ltx_td ltx_align_left" style="padding-left:4.0pt;padding-right:4.0pt;">
<math alttext="109" class="ltx_Math" display="inline" id="S3.T1.m5"><semantics><mn>109</mn><annotation encoding="application/x-tex">109</annotation></semantics></math> (<span class="ltx_text ltx_font_italic">17</span>)</td>
<td class="ltx_td ltx_align_left" style="padding-left:4.0pt;padding-right:4.0pt;"><math alttext="218" class="ltx_Math" display="inline" id="S3.T1.m6"><semantics><mn>218</mn><annotation encoding="application/x-tex">218</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left" style="padding-left:4.0pt;padding-right:4.0pt;">MoE</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left" style="padding-left:4.0pt;padding-right:4.0pt;">GPT-OSS 20B</td>
<td class="ltx_td ltx_align_left" style="padding-left:4.0pt;padding-right:4.0pt;">
<math alttext="21" class="ltx_Math" display="inline" id="S3.T1.m7"><semantics><mn>21</mn><annotation encoding="application/x-tex">21</annotation></semantics></math> (<span class="ltx_text ltx_font_italic">3.6</span>)</td>
<td class="ltx_td ltx_align_left" style="padding-left:4.0pt;padding-right:4.0pt;"><math alttext="42" class="ltx_Math" display="inline" id="S3.T1.m8"><semantics><mn>42</mn><annotation encoding="application/x-tex">42</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left" style="padding-left:4.0pt;padding-right:4.0pt;">MoE</td>
</tr>
<tr class="ltx_tr" style="background-color:#EBEEF5;">
<td class="ltx_td ltx_align_left" colspan="4" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" style="background-color:#EBEEF5;">Dense Models</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left" style="padding-left:4.0pt;padding-right:4.0pt;">DeepSeek-R1 70B</td>
<td class="ltx_td ltx_align_left" style="padding-left:4.0pt;padding-right:4.0pt;"><math alttext="70" class="ltx_Math" display="inline" id="S3.T1.m9"><semantics><mn>70</mn><annotation encoding="application/x-tex">70</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left" style="padding-left:4.0pt;padding-right:4.0pt;"><math alttext="140" class="ltx_Math" display="inline" id="S3.T1.m10"><semantics><mn>140</mn><annotation encoding="application/x-tex">140</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left" style="padding-left:4.0pt;padding-right:4.0pt;">Dense (Distilled)</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left" style="padding-left:4.0pt;padding-right:4.0pt;">Llama 3.3 70B</td>
<td class="ltx_td ltx_align_left" style="padding-left:4.0pt;padding-right:4.0pt;"><math alttext="70" class="ltx_Math" display="inline" id="S3.T1.m11"><semantics><mn>70</mn><annotation encoding="application/x-tex">70</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left" style="padding-left:4.0pt;padding-right:4.0pt;"><math alttext="140" class="ltx_Math" display="inline" id="S3.T1.m12"><semantics><mn>140</mn><annotation encoding="application/x-tex">140</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left" style="padding-left:4.0pt;padding-right:4.0pt;">Dense</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left" style="padding-left:4.0pt;padding-right:4.0pt;">Gemma 3 27B</td>
<td class="ltx_td ltx_align_left" style="padding-left:4.0pt;padding-right:4.0pt;"><math alttext="27.4" class="ltx_Math" display="inline" id="S3.T1.m13"><semantics><mn>27.4</mn><annotation encoding="application/x-tex">27.4</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left" style="padding-left:4.0pt;padding-right:4.0pt;"><math alttext="55" class="ltx_Math" display="inline" id="S3.T1.m14"><semantics><mn>55</mn><annotation encoding="application/x-tex">55</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left" style="padding-left:4.0pt;padding-right:4.0pt;">Dense</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left" style="padding-left:4.0pt;padding-right:4.0pt;">Phi-4 Reasoning</td>
<td class="ltx_td ltx_align_left" style="padding-left:4.0pt;padding-right:4.0pt;"><math alttext="14.7" class="ltx_Math" display="inline" id="S3.T1.m15"><semantics><mn>14.7</mn><annotation encoding="application/x-tex">14.7</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left" style="padding-left:4.0pt;padding-right:4.0pt;"><math alttext="29" class="ltx_Math" display="inline" id="S3.T1.m16"><semantics><mn>29</mn><annotation encoding="application/x-tex">29</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left" style="padding-left:4.0pt;padding-right:4.0pt;">Dense</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_tt" colspan="4" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" style="font-size:80%;">*Active = parameters used per forward pass in MoE models.</span></td>
</tr>
</table>
</figure>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">III-B </span><span class="ltx_text ltx_font_italic">Benchmark Selection</span>
</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p">We employ ten complementary benchmark suites spanning diverse cognitive capabilities. The Massive Multitask Language Understanding (MMLU) benchmark <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="2508.12461v1.html#bib.bib15" title="">15</a>]</cite> evaluates broad knowledge across 57 subjects, while GSM8K <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="2508.12461v1.html#bib.bib16" title="">16</a>]</cite> assesses mathematical reasoning following methodologies established by Lewkowycz et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="2508.12461v1.html#bib.bib17" title="">17</a>]</cite>. Code generation capabilities are measured using HumanEval <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="2508.12461v1.html#bib.bib18" title="">18</a>]</cite>, with evaluation protocols refined by recent work from Li et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="2508.12461v1.html#bib.bib20" title="">20</a>]</cite>. Domain-specific evaluation includes FinQA<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="2508.12461v1.html#bib.bib48" title="">48</a>]</cite> for financial reasoning, MedQA<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="2508.12461v1.html#bib.bib49" title="">49</a>]</cite> for medical knowledge assessment, LegalQA<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="2508.12461v1.html#bib.bib50" title="">50</a>]</cite> for legal understanding, and SciQ<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="2508.12461v1.html#bib.bib51" title="">51</a>]</cite> for scientific reasoning. Common-sense reasoning is evaluated through PIQA (Physical Interaction Question Answering)<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="2508.12461v1.html#bib.bib52" title="">52</a>]</cite>, while DialogSum<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="2508.12461v1.html#bib.bib53" title="">53</a>]</cite> assesses conversational summarisation capabilities. Cross-lingual evaluation employs C-Eval <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="2508.12461v1.html#bib.bib23" title="">23</a>]</cite> for Chinese comprehension, complemented by methodologies from XTREME <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="2508.12461v1.html#bib.bib54" title="">54</a>]</cite> and mT5 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="2508.12461v1.html#bib.bib55" title="">55</a>]</cite>. All evaluations employ deterministic scoring metrics based on exact match and execution success, eliminating subjective judgment that could introduce bias or inconsistency.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">III-C </span><span class="ltx_text ltx_font_italic">Evaluation Protocol</span>
</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p">Our evaluation protocol builds upon established best practices from the HELM framework <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="2508.12461v1.html#bib.bib35" title="">35</a>]</cite> and BIG-bench <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="2508.12461v1.html#bib.bib56" title="">56</a>]</cite>. Sampling strategies follow power analysis guidelines from Card et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="2508.12461v1.html#bib.bib36" title="">36</a>]</cite> to ensure statistical validity, while prompt engineering incorporates insights from Brown et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="2508.12461v1.html#bib.bib57" title="">57</a>]</cite>, Wei et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="2508.12461v1.html#bib.bib58" title="">58</a>]</cite>, and Kojima et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="2508.12461v1.html#bib.bib59" title="">59</a>]</cite>. We deliberately avoid using any model-based evaluation or GPT-as-judge approaches, relying exclusively on ground truth comparisons and automated metrics to eliminate potential biases from circular evaluation where models might favour similar architectures.</p>
</div>
<figure class="ltx_figure" id="S3.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="537" id="S3.F2.g1" src="2508.12461v1/x2.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span><span class="ltx_text ltx_font_bold">Parameter-performance relationship</span>. The non-monotonic scaling observed in GPT-OSS variants contradicts predictions from Kaplan et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="2508.12461v1.html#bib.bib3" title="">3</a>]</cite> and suggests architectural inefficiencies.</figcaption>
</figure>
<div class="ltx_para" id="S3.SS3.p2">
<p class="ltx_p">Response processing employs techniques from Holtzman et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="2508.12461v1.html#bib.bib60" title="">60</a>]</cite> for decoding and Welleck et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="2508.12461v1.html#bib.bib61" title="">61</a>]</cite> for consistency checking. Temperature scaling follows recommendations from Ruis et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="2508.12461v1.html#bib.bib62" title="">62</a>]</cite>, with nucleus sampling parameters based on findings from Meister et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="2508.12461v1.html#bib.bib63" title="">63</a>]</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">III-D </span><span class="ltx_text ltx_font_italic">Statistical Analysis</span>
</h3>
<div class="ltx_para" id="S3.SS4.p1">
<p class="ltx_p">Statistical rigour follows guidelines established by Dror et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="2508.12461v1.html#bib.bib64" title="">64</a>]</cite> and Benavoli et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="2508.12461v1.html#bib.bib65" title="">65</a>]</cite> for NLP evaluation. We employ McNemar’s test for paired comparisons <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="2508.12461v1.html#bib.bib66" title="">66</a>]</cite> with Bonferroni correction <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="2508.12461v1.html#bib.bib67" title="">67</a>]</cite>, effect size calculation using Cohen’s d <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="2508.12461v1.html#bib.bib68" title="">68</a>]</cite>, and bootstrap confidence intervals following Efron and Tibshirani <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="2508.12461v1.html#bib.bib69" title="">69</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S3.SS4.p2">
<p class="ltx_p">Recent work by Pimentel et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="2508.12461v1.html#bib.bib70" title="">70</a>]</cite> and Deutsch et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="2508.12461v1.html#bib.bib71" title="">71</a>]</cite> has highlighted the importance of proper statistical testing in NLP, particularly when comparing models with similar performance levels. Our analysis incorporates these recommendations, including variance estimation techniques from Koehn <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="2508.12461v1.html#bib.bib72" title="">72</a>]</cite> and multiple testing corrections from Benjamini and Hochberg <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="2508.12461v1.html#bib.bib73" title="">73</a>]</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">III-E </span><span class="ltx_text ltx_font_italic">Quality Assurance</span>
</h3>
<div class="ltx_para" id="S3.SS5.p1">
<p class="ltx_p">Reproducibility measures follow recommendations from Pineau et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="2508.12461v1.html#bib.bib74" title="">74</a>]</cite> and Dodge et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="2508.12461v1.html#bib.bib75" title="">75</a>]</cite>, including fixed random seeds, version locking, and comprehensive logging. Data contamination detection employs methods from Sainz et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="2508.12461v1.html#bib.bib37" title="">37</a>]</cite> and Magar and Schwartz <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="2508.12461v1.html#bib.bib76" title="">76</a>]</cite>, while evaluation gaming prevention follows guidelines from Dekoninck et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="2508.12461v1.html#bib.bib77" title="">77</a>]</cite>. To address concerns about benchmark overfitting, we include both established benchmarks and recent evaluation sets released after the training cutoff of evaluated models, ensuring that performance differences reflect genuine capabilities rather than memorisation.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS6">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">III-F </span><span class="ltx_text ltx_font_italic">Limitations and Ethical Considerations</span>
</h3>
<div class="ltx_para" id="S3.SS6.p1">
<p class="ltx_p">Our methodology acknowledges limitations identified in prior benchmark critiques <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="2508.12461v1.html#bib.bib29" title="">29</a>, <a class="ltx_ref" href="2508.12461v1.html#bib.bib30" title="">30</a>, <a class="ltx_ref" href="2508.12461v1.html#bib.bib78" title="">78</a>]</cite>. Ethical considerations follow guidelines from Bender et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="2508.12461v1.html#bib.bib79" title="">79</a>]</cite> and Birhane et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="2508.12461v1.html#bib.bib80" title="">80</a>]</cite>, including assessment of potential biases and environmental impact following Strubell et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="2508.12461v1.html#bib.bib81" title="">81</a>]</cite> and Patterson et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="2508.12461v1.html#bib.bib82" title="">82</a>]</cite>. While critics might argue that benchmarks inadequately capture real-world performance, our comprehensive suite spanning knowledge, reasoning, code, and multilingual tasks provides robust evidence of relative model capabilities. The consistency of performance patterns across diverse evaluation domains strengthens confidence in our findings beyond what any single benchmark could provide.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span class="ltx_text ltx_font_smallcaps">Experimental Setup</span>
</h2>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">IV-A </span><span class="ltx_text ltx_font_italic">Hardware Infrastructure</span>
</h3>
<figure class="ltx_table" id="S4.T2">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE II: </span><span class="ltx_text ltx_font_bold">Overall performance summary</span> across all evaluated benchmarks. Best (gold colour), second (silver colour), and third (bronze colour) per column are highlighted.</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold">Model</span></td>
<td class="ltx_td ltx_align_left ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold">MMLU</span></td>
<td class="ltx_td ltx_align_left ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold">GSM8K</span></td>
<td class="ltx_td ltx_align_left ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold">HumanEval</span></td>
<td class="ltx_td ltx_align_left ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold">FinQA</span></td>
<td class="ltx_td ltx_align_left ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold">PIQA</span></td>
<td class="ltx_td ltx_align_left ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold">SciQ</span></td>
<td class="ltx_td ltx_align_left ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold">MedQA</span></td>
<td class="ltx_td ltx_align_left ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold">LegalQA</span></td>
<td class="ltx_td ltx_align_left ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold">DialogSum</span></td>
<td class="ltx_td ltx_align_left ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold">C-Eval</span></td>
<td class="ltx_td ltx_align_left ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold">Avg.</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">Qwen 3 235B</td>
<td class="ltx_td ltx_align_left ltx_border_t" style="background-color:#FFF3B0;padding-left:4.0pt;padding-right:4.0pt;"><math alttext="92" class="ltx_Math" display="inline" id="S4.T2.m1" style="background-color:#FFF3B0;"><semantics><mn mathbackground="#FFF3B0">92</mn><annotation encoding="application/x-tex">92</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_border_t" style="background-color:#F7E1C6;padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" style="background-color:#F7E1C6;"><math alttext="82" class="ltx_Math" display="inline" id="S4.T2.m2"><semantics><mn mathbackground="#F7E1C6">82</mn><annotation encoding="application/x-tex">82</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_left ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;"><math alttext="80" class="ltx_Math" display="inline" id="S4.T2.m3"><semantics><mn>80</mn><annotation encoding="application/x-tex">80</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_border_t" style="background-color:#FFF3B0;padding-left:4.0pt;padding-right:4.0pt;"><math alttext="85" class="ltx_Math" display="inline" id="S4.T2.m4" style="background-color:#FFF3B0;"><semantics><mn mathbackground="#FFF3B0">85</mn><annotation encoding="application/x-tex">85</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_border_t" style="background-color:#FFF3B0;padding-left:4.0pt;padding-right:4.0pt;"><math alttext="88" class="ltx_Math" display="inline" id="S4.T2.m5" style="background-color:#FFF3B0;"><semantics><mn mathbackground="#FFF3B0">88</mn><annotation encoding="application/x-tex">88</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_border_t" style="background-color:#FFF3B0;padding-left:4.0pt;padding-right:4.0pt;"><math alttext="91" class="ltx_Math" display="inline" id="S4.T2.m6" style="background-color:#FFF3B0;"><semantics><mn mathbackground="#FFF3B0">91</mn><annotation encoding="application/x-tex">91</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_border_t" style="background-color:#FFF3B0;padding-left:4.0pt;padding-right:4.0pt;"><math alttext="78" class="ltx_Math" display="inline" id="S4.T2.m7" style="background-color:#FFF3B0;"><semantics><mn mathbackground="#FFF3B0">78</mn><annotation encoding="application/x-tex">78</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_border_t" style="background-color:#FFF3B0;padding-left:4.0pt;padding-right:4.0pt;"><math alttext="82" class="ltx_Math" display="inline" id="S4.T2.m8" style="background-color:#FFF3B0;"><semantics><mn mathbackground="#FFF3B0">82</mn><annotation encoding="application/x-tex">82</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_border_t" style="background-color:#FFF3B0;padding-left:4.0pt;padding-right:4.0pt;"><math alttext="85" class="ltx_Math" display="inline" id="S4.T2.m9" style="background-color:#FFF3B0;"><semantics><mn mathbackground="#FFF3B0">85</mn><annotation encoding="application/x-tex">85</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_border_t" style="background-color:#FFF3B0;padding-left:4.0pt;padding-right:4.0pt;"><math alttext="89" class="ltx_Math" display="inline" id="S4.T2.m10" style="background-color:#FFF3B0;"><semantics><mn mathbackground="#FFF3B0">89</mn><annotation encoding="application/x-tex">89</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_border_t" style="background-color:#FFF3B0;padding-left:4.0pt;padding-right:4.0pt;"><math alttext="85.2" class="ltx_Math" display="inline" id="S4.T2.m11" style="background-color:#FFF3B0;"><semantics><mn mathbackground="#FFF3B0">85.2</mn><annotation encoding="application/x-tex">85.2</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left" style="padding-left:4.0pt;padding-right:4.0pt;">DeepSeek-R1 70B</td>
<td class="ltx_td ltx_align_left" style="background-color:#F7E1C6;padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" style="background-color:#F7E1C6;"><math alttext="88" class="ltx_Math" display="inline" id="S4.T2.m12"><semantics><mn mathbackground="#F7E1C6">88</mn><annotation encoding="application/x-tex">88</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_left" style="background-color:#FFF3B0;padding-left:4.0pt;padding-right:4.0pt;"><math alttext="91" class="ltx_Math" display="inline" id="S4.T2.m13" style="background-color:#FFF3B0;"><semantics><mn mathbackground="#FFF3B0">91</mn><annotation encoding="application/x-tex">91</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left" style="background-color:#FFF3B0;padding-left:4.0pt;padding-right:4.0pt;"><math alttext="88" class="ltx_Math" display="inline" id="S4.T2.m14" style="background-color:#FFF3B0;"><semantics><mn mathbackground="#FFF3B0">88</mn><annotation encoding="application/x-tex">88</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left" style="background-color:#EDEDED;padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" style="background-color:#EDEDED;"><math alttext="82" class="ltx_Math" display="inline" id="S4.T2.m15"><semantics><mn mathbackground="#EDEDED">82</mn><annotation encoding="application/x-tex">82</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_left" style="background-color:#EDEDED;padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" style="background-color:#EDEDED;"><math alttext="85" class="ltx_Math" display="inline" id="S4.T2.m16"><semantics><mn mathbackground="#EDEDED">85</mn><annotation encoding="application/x-tex">85</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_left" style="background-color:#EDEDED;padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" style="background-color:#EDEDED;"><math alttext="87" class="ltx_Math" display="inline" id="S4.T2.m17"><semantics><mn mathbackground="#EDEDED">87</mn><annotation encoding="application/x-tex">87</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_left" style="background-color:#EDEDED;padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" style="background-color:#EDEDED;"><math alttext="75" class="ltx_Math" display="inline" id="S4.T2.m18"><semantics><mn mathbackground="#EDEDED">75</mn><annotation encoding="application/x-tex">75</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_left" style="background-color:#EDEDED;padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" style="background-color:#EDEDED;"><math alttext="79" class="ltx_Math" display="inline" id="S4.T2.m19"><semantics><mn mathbackground="#EDEDED">79</mn><annotation encoding="application/x-tex">79</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_left" style="background-color:#EDEDED;padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" style="background-color:#EDEDED;"><math alttext="81" class="ltx_Math" display="inline" id="S4.T2.m20"><semantics><mn mathbackground="#EDEDED">81</mn><annotation encoding="application/x-tex">81</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_left" style="background-color:#F7E1C6;padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" style="background-color:#F7E1C6;"><math alttext="68" class="ltx_Math" display="inline" id="S4.T2.m21"><semantics><mn mathbackground="#F7E1C6">68</mn><annotation encoding="application/x-tex">68</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_left" style="background-color:#EDEDED;padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" style="background-color:#EDEDED;"><math alttext="82.4" class="ltx_Math" display="inline" id="S4.T2.m22"><semantics><mn mathbackground="#EDEDED">82.4</mn><annotation encoding="application/x-tex">82.4</annotation></semantics></math></span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left" style="padding-left:4.0pt;padding-right:4.0pt;">Phi-4 Reasoning</td>
<td class="ltx_td ltx_align_left" style="background-color:#EDEDED;padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" style="background-color:#EDEDED;"><math alttext="90" class="ltx_Math" display="inline" id="S4.T2.m23"><semantics><mn mathbackground="#EDEDED">90</mn><annotation encoding="application/x-tex">90</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_left" style="background-color:#EDEDED;padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" style="background-color:#EDEDED;"><math alttext="87" class="ltx_Math" display="inline" id="S4.T2.m24"><semantics><mn mathbackground="#EDEDED">87</mn><annotation encoding="application/x-tex">87</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_left" style="background-color:#FFF3B0;padding-left:4.0pt;padding-right:4.0pt;"><math alttext="88" class="ltx_Math" display="inline" id="S4.T2.m25" style="background-color:#FFF3B0;"><semantics><mn mathbackground="#FFF3B0">88</mn><annotation encoding="application/x-tex">88</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left" style="background-color:#F7E1C6;padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" style="background-color:#F7E1C6;"><math alttext="79" class="ltx_Math" display="inline" id="S4.T2.m26"><semantics><mn mathbackground="#F7E1C6">79</mn><annotation encoding="application/x-tex">79</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_left" style="background-color:#F7E1C6;padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" style="background-color:#F7E1C6;"><math alttext="83" class="ltx_Math" display="inline" id="S4.T2.m27"><semantics><mn mathbackground="#F7E1C6">83</mn><annotation encoding="application/x-tex">83</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_left" style="background-color:#F7E1C6;padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" style="background-color:#F7E1C6;"><math alttext="86" class="ltx_Math" display="inline" id="S4.T2.m28"><semantics><mn mathbackground="#F7E1C6">86</mn><annotation encoding="application/x-tex">86</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_left" style="background-color:#F7E1C6;padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" style="background-color:#F7E1C6;"><math alttext="72" class="ltx_Math" display="inline" id="S4.T2.m29"><semantics><mn mathbackground="#F7E1C6">72</mn><annotation encoding="application/x-tex">72</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_left" style="background-color:#F7E1C6;padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" style="background-color:#F7E1C6;"><math alttext="76" class="ltx_Math" display="inline" id="S4.T2.m30"><semantics><mn mathbackground="#F7E1C6">76</mn><annotation encoding="application/x-tex">76</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_left" style="background-color:#F7E1C6;padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" style="background-color:#F7E1C6;"><math alttext="78" class="ltx_Math" display="inline" id="S4.T2.m31"><semantics><mn mathbackground="#F7E1C6">78</mn><annotation encoding="application/x-tex">78</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_left" style="padding-left:4.0pt;padding-right:4.0pt;"><math alttext="56" class="ltx_Math" display="inline" id="S4.T2.m32"><semantics><mn>56</mn><annotation encoding="application/x-tex">56</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left" style="background-color:#F7E1C6;padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" style="background-color:#F7E1C6;"><math alttext="79.5" class="ltx_Math" display="inline" id="S4.T2.m33"><semantics><mn mathbackground="#F7E1C6">79.5</mn><annotation encoding="application/x-tex">79.5</annotation></semantics></math></span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left" style="padding-left:4.0pt;padding-right:4.0pt;">Llama 4 Scout</td>
<td class="ltx_td ltx_align_left" style="padding-left:4.0pt;padding-right:4.0pt;"><math alttext="85" class="ltx_Math" display="inline" id="S4.T2.m34"><semantics><mn>85</mn><annotation encoding="application/x-tex">85</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left" style="background-color:#F7E1C6;padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" style="background-color:#F7E1C6;"><math alttext="85" class="ltx_Math" display="inline" id="S4.T2.m35"><semantics><mn mathbackground="#F7E1C6">85</mn><annotation encoding="application/x-tex">85</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_left" style="padding-left:4.0pt;padding-right:4.0pt;"><math alttext="78" class="ltx_Math" display="inline" id="S4.T2.m36"><semantics><mn>78</mn><annotation encoding="application/x-tex">78</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left" style="padding-left:4.0pt;padding-right:4.0pt;"><math alttext="76" class="ltx_Math" display="inline" id="S4.T2.m37"><semantics><mn>76</mn><annotation encoding="application/x-tex">76</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left" style="padding-left:4.0pt;padding-right:4.0pt;"><math alttext="82" class="ltx_Math" display="inline" id="S4.T2.m38"><semantics><mn>82</mn><annotation encoding="application/x-tex">82</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left" style="background-color:#F7E1C6;padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" style="background-color:#F7E1C6;"><math alttext="84" class="ltx_Math" display="inline" id="S4.T2.m39"><semantics><mn mathbackground="#F7E1C6">84</mn><annotation encoding="application/x-tex">84</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_left" style="padding-left:4.0pt;padding-right:4.0pt;"><math alttext="71" class="ltx_Math" display="inline" id="S4.T2.m40"><semantics><mn>71</mn><annotation encoding="application/x-tex">71</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left" style="padding-left:4.0pt;padding-right:4.0pt;"><math alttext="74" class="ltx_Math" display="inline" id="S4.T2.m41"><semantics><mn>74</mn><annotation encoding="application/x-tex">74</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left" style="padding-left:4.0pt;padding-right:4.0pt;"><math alttext="77" class="ltx_Math" display="inline" id="S4.T2.m42"><semantics><mn>77</mn><annotation encoding="application/x-tex">77</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left" style="background-color:#EDEDED;padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" style="background-color:#EDEDED;"><math alttext="72" class="ltx_Math" display="inline" id="S4.T2.m43"><semantics><mn mathbackground="#EDEDED">72</mn><annotation encoding="application/x-tex">72</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_left" style="padding-left:4.0pt;padding-right:4.0pt;"><math alttext="78.4" class="ltx_Math" display="inline" id="S4.T2.m44"><semantics><mn>78.4</mn><annotation encoding="application/x-tex">78.4</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left" style="padding-left:4.0pt;padding-right:4.0pt;">Llama 3.3 70B</td>
<td class="ltx_td ltx_align_left" style="padding-left:4.0pt;padding-right:4.0pt;"><math alttext="84" class="ltx_Math" display="inline" id="S4.T2.m45"><semantics><mn>84</mn><annotation encoding="application/x-tex">84</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left" style="padding-left:4.0pt;padding-right:4.0pt;"><math alttext="79" class="ltx_Math" display="inline" id="S4.T2.m46"><semantics><mn>79</mn><annotation encoding="application/x-tex">79</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left" style="background-color:#F7E1C6;padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" style="background-color:#F7E1C6;"><math alttext="83" class="ltx_Math" display="inline" id="S4.T2.m47"><semantics><mn mathbackground="#F7E1C6">83</mn><annotation encoding="application/x-tex">83</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_left" style="padding-left:4.0pt;padding-right:4.0pt;"><math alttext="73" class="ltx_Math" display="inline" id="S4.T2.m48"><semantics><mn>73</mn><annotation encoding="application/x-tex">73</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left" style="padding-left:4.0pt;padding-right:4.0pt;"><math alttext="80" class="ltx_Math" display="inline" id="S4.T2.m49"><semantics><mn>80</mn><annotation encoding="application/x-tex">80</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left" style="padding-left:4.0pt;padding-right:4.0pt;"><math alttext="82" class="ltx_Math" display="inline" id="S4.T2.m50"><semantics><mn>82</mn><annotation encoding="application/x-tex">82</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left" style="padding-left:4.0pt;padding-right:4.0pt;"><math alttext="69" class="ltx_Math" display="inline" id="S4.T2.m51"><semantics><mn>69</mn><annotation encoding="application/x-tex">69</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left" style="padding-left:4.0pt;padding-right:4.0pt;"><math alttext="72" class="ltx_Math" display="inline" id="S4.T2.m52"><semantics><mn>72</mn><annotation encoding="application/x-tex">72</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left" style="padding-left:4.0pt;padding-right:4.0pt;"><math alttext="75" class="ltx_Math" display="inline" id="S4.T2.m53"><semantics><mn>75</mn><annotation encoding="application/x-tex">75</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left" style="padding-left:4.0pt;padding-right:4.0pt;"><math alttext="61" class="ltx_Math" display="inline" id="S4.T2.m54"><semantics><mn>61</mn><annotation encoding="application/x-tex">61</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left" style="padding-left:4.0pt;padding-right:4.0pt;"><math alttext="75.8" class="ltx_Math" display="inline" id="S4.T2.m55"><semantics><mn>75.8</mn><annotation encoding="application/x-tex">75.8</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left" style="padding-left:4.0pt;padding-right:4.0pt;">Gemma 3 27B</td>
<td class="ltx_td ltx_align_left" style="padding-left:4.0pt;padding-right:4.0pt;"><math alttext="79" class="ltx_Math" display="inline" id="S4.T2.m56"><semantics><mn>79</mn><annotation encoding="application/x-tex">79</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left" style="padding-left:4.0pt;padding-right:4.0pt;"><math alttext="74" class="ltx_Math" display="inline" id="S4.T2.m57"><semantics><mn>74</mn><annotation encoding="application/x-tex">74</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left" style="padding-left:4.0pt;padding-right:4.0pt;"><math alttext="76" class="ltx_Math" display="inline" id="S4.T2.m58"><semantics><mn>76</mn><annotation encoding="application/x-tex">76</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left" style="padding-left:4.0pt;padding-right:4.0pt;"><math alttext="70" class="ltx_Math" display="inline" id="S4.T2.m59"><semantics><mn>70</mn><annotation encoding="application/x-tex">70</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left" style="padding-left:4.0pt;padding-right:4.0pt;"><math alttext="77" class="ltx_Math" display="inline" id="S4.T2.m60"><semantics><mn>77</mn><annotation encoding="application/x-tex">77</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left" style="padding-left:4.0pt;padding-right:4.0pt;"><math alttext="78" class="ltx_Math" display="inline" id="S4.T2.m61"><semantics><mn>78</mn><annotation encoding="application/x-tex">78</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left" style="padding-left:4.0pt;padding-right:4.0pt;"><math alttext="65" class="ltx_Math" display="inline" id="S4.T2.m62"><semantics><mn>65</mn><annotation encoding="application/x-tex">65</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left" style="padding-left:4.0pt;padding-right:4.0pt;"><math alttext="68" class="ltx_Math" display="inline" id="S4.T2.m63"><semantics><mn>68</mn><annotation encoding="application/x-tex">68</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left" style="padding-left:4.0pt;padding-right:4.0pt;"><math alttext="71" class="ltx_Math" display="inline" id="S4.T2.m64"><semantics><mn>71</mn><annotation encoding="application/x-tex">71</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left" style="padding-left:4.0pt;padding-right:4.0pt;"><math alttext="52" class="ltx_Math" display="inline" id="S4.T2.m65"><semantics><mn>52</mn><annotation encoding="application/x-tex">52</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left" style="padding-left:4.0pt;padding-right:4.0pt;"><math alttext="71.0" class="ltx_Math" display="inline" id="S4.T2.m66"><semantics><mn>71.0</mn><annotation encoding="application/x-tex">71.0</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left" style="padding-left:4.0pt;padding-right:4.0pt;">GPT-OSS 20B</td>
<td class="ltx_td ltx_align_left" style="padding-left:4.0pt;padding-right:4.0pt;"><math alttext="69" class="ltx_Math" display="inline" id="S4.T2.m67"><semantics><mn>69</mn><annotation encoding="application/x-tex">69</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left" style="padding-left:4.0pt;padding-right:4.0pt;"><math alttext="78" class="ltx_Math" display="inline" id="S4.T2.m68"><semantics><mn>78</mn><annotation encoding="application/x-tex">78</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left" style="padding-left:4.0pt;padding-right:4.0pt;"><math alttext="73" class="ltx_Math" display="inline" id="S4.T2.m69"><semantics><mn>73</mn><annotation encoding="application/x-tex">73</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left" style="padding-left:4.0pt;padding-right:4.0pt;"><math alttext="68" class="ltx_Math" display="inline" id="S4.T2.m70"><semantics><mn>68</mn><annotation encoding="application/x-tex">68</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left" style="padding-left:4.0pt;padding-right:4.0pt;"><math alttext="74" class="ltx_Math" display="inline" id="S4.T2.m71"><semantics><mn>74</mn><annotation encoding="application/x-tex">74</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left" style="padding-left:4.0pt;padding-right:4.0pt;"><math alttext="75" class="ltx_Math" display="inline" id="S4.T2.m72"><semantics><mn>75</mn><annotation encoding="application/x-tex">75</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left" style="padding-left:4.0pt;padding-right:4.0pt;"><math alttext="62" class="ltx_Math" display="inline" id="S4.T2.m73"><semantics><mn>62</mn><annotation encoding="application/x-tex">62</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left" style="padding-left:4.0pt;padding-right:4.0pt;"><math alttext="65" class="ltx_Math" display="inline" id="S4.T2.m74"><semantics><mn>65</mn><annotation encoding="application/x-tex">65</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left" style="padding-left:4.0pt;padding-right:4.0pt;"><math alttext="68" class="ltx_Math" display="inline" id="S4.T2.m75"><semantics><mn>68</mn><annotation encoding="application/x-tex">68</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left" style="padding-left:4.0pt;padding-right:4.0pt;"><math alttext="45" class="ltx_Math" display="inline" id="S4.T2.m76"><semantics><mn>45</mn><annotation encoding="application/x-tex">45</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left" style="padding-left:4.0pt;padding-right:4.0pt;"><math alttext="67.7" class="ltx_Math" display="inline" id="S4.T2.m77"><semantics><mn>67.7</mn><annotation encoding="application/x-tex">67.7</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;">GPT-OSS 120B</td>
<td class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;"><math alttext="66" class="ltx_Math" display="inline" id="S4.T2.m78"><semantics><mn>66</mn><annotation encoding="application/x-tex">66</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;"><math alttext="75" class="ltx_Math" display="inline" id="S4.T2.m79"><semantics><mn>75</mn><annotation encoding="application/x-tex">75</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;"><math alttext="71" class="ltx_Math" display="inline" id="S4.T2.m80"><semantics><mn>71</mn><annotation encoding="application/x-tex">71</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;"><math alttext="65" class="ltx_Math" display="inline" id="S4.T2.m81"><semantics><mn>65</mn><annotation encoding="application/x-tex">65</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;"><math alttext="71" class="ltx_Math" display="inline" id="S4.T2.m82"><semantics><mn>71</mn><annotation encoding="application/x-tex">71</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;"><math alttext="72" class="ltx_Math" display="inline" id="S4.T2.m83"><semantics><mn>72</mn><annotation encoding="application/x-tex">72</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;"><math alttext="59" class="ltx_Math" display="inline" id="S4.T2.m84"><semantics><mn>59</mn><annotation encoding="application/x-tex">59</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;"><math alttext="62" class="ltx_Math" display="inline" id="S4.T2.m85"><semantics><mn>62</mn><annotation encoding="application/x-tex">62</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;"><math alttext="65" class="ltx_Math" display="inline" id="S4.T2.m86"><semantics><mn>65</mn><annotation encoding="application/x-tex">65</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;"><math alttext="42" class="ltx_Math" display="inline" id="S4.T2.m87"><semantics><mn>42</mn><annotation encoding="application/x-tex">42</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;"><math alttext="64.8" class="ltx_Math" display="inline" id="S4.T2.m88"><semantics><mn>64.8</mn><annotation encoding="application/x-tex">64.8</annotation></semantics></math></td>
</tr>
</table>
</figure>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p">All experiments were conducted on a compute cluster equipped with 8 NVIDIA H100 80GB GPUs interconnected via NVLink, 1TB of system RAM, running Ubuntu 22.04 LTS. Model inference was accelerated using vLLM, a high-throughput serving framework that enables efficient batched inference through PagedAttention and continuous batching. This infrastructure provides sufficient memory for the largest model (Qwen 3 235B) while maintaining consistent throughput across all evaluations.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">IV-B </span><span class="ltx_text ltx_font_italic">Model Configuration</span>
</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p">All models were evaluated in their original, unquantised form to ensure fair comparison and maximum performance. We specifically selected the strongest available open source model within a similar parameter range to provide a meaningful benchmark for GPT-OSS performance. The 70B parameter models (Llama 3.3 70B, DeepSeek-R1 70B) represent the most competitive alternative in the gpt-oss-120B’s size category, while smaller models such as Gemma 3 27B and Phi-4 Reasoning 14B provide context for the gpt-oss-20B variant. This selection ensures our comparison focuses on state-of-the-art open source models of comparable scale rather than an arbitrary baseline.</p>
</div>
<section class="ltx_subsubsection" id="S4.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">IV-B1 </span>Generation Parameters</h4>
<div class="ltx_para" id="S4.SS2.SSS1.p1">
<p class="ltx_p">Consistent generation parameters were maintained across all models to ensure fair comparison. Temperature setting varied by task type, with 0.7 for creative tasks requiring diversity and 0.1 for factual tasks demanding precision. Sampling parameter included top-p of 0.95 and top-k of 50 to balance coherence with creativity. Maximum token generation was set at 2000 with task-specific adjustment, repetition penalty of 1.1 to prevent redundant outputs.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">IV-C </span><span class="ltx_text ltx_font_italic">Data Preparation</span>
</h3>
<section class="ltx_subsubsection" id="S4.SS3.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">IV-C1 </span>Dataset Loading and Preprocessing</h4>
<div class="ltx_para" id="S4.SS3.SSS1.p1">
<p class="ltx_p">Our evaluation employs ten diverse benchmarks to comprehensively assess model capabilities across multiple cognitive dimensions. Each dataset targets specific competencies, while collectively they provide a holistic view of model performance.</p>
</div>
<div class="ltx_para" id="S4.SS3.SSS1.p2">
<ul class="ltx_itemize" id="S4.I1">
<li class="ltx_item" id="S4.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i1.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">MMLU (Massive Multitask Language Understanding):</span> 14,000+ questions spanning 57 subjects from humanities to STEM, testing broad knowledge and reasoning capabilities across academic disciplines. Stratified sampling ensures balanced representation across all subject areas.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i2.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">GSM8K (Grade School Math):</span> 8,500 linguistically diverse grade school mathematics word problems requiring multi-step reasoning and arithmetic operations. Chain-of-thought prompts were prepared to evaluate reasoning processes.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i3.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">HumanEval:</span> 164 hand-crafted Python programming challenges testing code generation, algorithmic thinking, and understanding of programming concepts. Complete test harness integration enables execution-based evaluation.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i4.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">FinQA:</span> Financial domain question-answering dataset with 8,000+ examples requiring numerical reasoning over financial documents and tables. Tests specialised domain knowledge and quantitative analysis capabilities.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i5.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">PIQA (Physical Interaction QA):</span> 20,000+ questions testing physical commonsense reasoning about everyday situations and object interactions. Evaluates understanding of physical world dynamics.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S4.SS3.SSS1.p3">
<ul class="ltx_itemize" id="S4.I2">
<li class="ltx_item" id="S4.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I2.i1.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">SciQ:</span> 13,700 crowdsourced science exam questions covering physics, chemistry, and biology at middle and high school levels. Assesses scientific knowledge and reasoning abilities.</p>
</div>
</li>
<li class="ltx_item" id="S4.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I2.i2.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">MedQA:</span> 61,000+ medical examination questions from USMLE and other medical licensing exams across multiple countries. Tests specialised medical knowledge and clinical reasoning.</p>
</div>
</li>
<li class="ltx_item" id="S4.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I2.i3.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">LegalQA:</span> Legal reasoning benchmark with 20,000+ questions from bar examinations and legal case studies. Evaluates understanding of legal principles and analytical reasoning.</p>
</div>
</li>
<li class="ltx_item" id="S4.I2.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I2.i4.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">DialogSum:</span> 13,460 dialogues with corresponding summaries, evaluating conversational understanding and summarisation capabilities. Tests the ability to extract key information from multi-turn conversations.</p>
</div>
</li>
<li class="ltx_item" id="S4.I2.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I2.i5.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">C-Eval:</span> Comprehensive Chinese evaluation suite with 13,900+ questions across 52 disciplines, testing multilingual capabilities. Bilingual prompt templates ensure fair cross-lingual evaluation.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS3.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">IV-C2 </span>Data Quality Assurance</h4>
<div class="ltx_para" id="S4.SS3.SSS2.p1">
<ul class="ltx_itemize" id="S4.I3">
<li class="ltx_item" id="S4.I3.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I3.i1.p1">
<p class="ltx_p">Manual inspection of 5% random sample from each dataset</p>
</div>
</li>
<li class="ltx_item" id="S4.I3.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I3.i2.p1">
<p class="ltx_p">Encoding verification for non-ASCII characters</p>
</div>
</li>
<li class="ltx_item" id="S4.I3.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I3.i3.p1">
<p class="ltx_p">Consistency checks for ground truth labels</p>
</div>
</li>
<li class="ltx_item" id="S4.I3.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I3.i4.p1">
<p class="ltx_p">Removal of corrupted or ambiguous examples</p>
</div>
</li>
</ul>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">IV-D </span><span class="ltx_text ltx_font_italic">Execution Pipeline</span>
</h3>
<section class="ltx_subsubsection" id="S4.SS4.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">IV-D1 </span>Automated Evaluation Workflow</h4>
<div class="ltx_para" id="S4.SS4.SSS1.p1">
<p class="ltx_p">Our evaluation pipeline implements a five-stage process for systematic assessment. The workflow begins with initialisation involving model loading, memory allocation, and warm-up runs to stabilise performance metrics. Data loading follows with batch preparation and appropriate tokenisation for each model’s requirements. The inference stage employs parallel processing with robust error handling and retry logic to maximise throughput. Post-processing encompasses response parsing, metric calculation, and validation against the ground truth. Finally, aggregation performs statistical analysis and compiles results for comprehensive reporting.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS4.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">IV-D2 </span>Error Handling and Recovery</h4>
<div class="ltx_para" id="S4.SS4.SSS2.p1">
<p class="ltx_p">Robust error handling ensures evaluation reliability despite potential failures. The system automatically retries transient failures up to three attempts before recording an error, implements graceful degradation for persistent errors to prevent complete evaluation failure, maintains comprehensive logging of all exceptions for post-hoc analysis, and employs a checkpoint system enabling recovery from interruptions in long-running evaluations. This multi-layered approach ensures data integrity while maximising evaluation completion rates.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">IV-E </span><span class="ltx_text ltx_font_italic">Monitoring and Logging</span>
</h3>
<section class="ltx_subsubsection" id="S4.SS5.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">IV-E1 </span>Performance Monitoring</h4>
<div class="ltx_para" id="S4.SS5.SSS1.p1">
<p class="ltx_p">Real-time monitoring tracks critical performance indicators throughout evaluation. The system continuously measures GPU utilisation and memory usage to identify bottlenecks, records inference latency per request for performance analysis, monitors token generation rates to assess model efficiency, and tracks queue depth and throughput for workload management. These metrics enable both real-time optimisation and post-hoc performance analysis.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS5.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">IV-E2 </span>Logging Infrastructure</h4>
<div class="ltx_para" id="S4.SS5.SSS2.p1">
<p class="ltx_p">Comprehensive logging captures all aspects of the evaluation process. Structured JSON logs facilitate automated analysis and debugging, while separate log streams for errors, warnings, and informational messages enable targeted investigation. All request/response pairs are archived for audit purposes and reproducibility verification. Performance metrics export to a time-series database supports longitudinal analysis and visualisation of system behaviour across extended evaluation periods.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS6">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">IV-F </span><span class="ltx_text ltx_font_italic">Experimental Controls</span>
</h3>
<section class="ltx_subsubsection" id="S4.SS6.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">IV-F1 </span>Randomisation</h4>
<div class="ltx_para" id="S4.SS6.SSS1.p1">
<p class="ltx_p">Randomisation protocols minimise systematic bias in our evaluation. Models are evaluated in random order within each benchmark to prevent ordering effects, and test examples are shuffled to eliminate position-dependent biases. This approach ensures that observed performance differences reflect genuine model capabilities rather than experimental artefacts.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS6.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">IV-F2 </span>Blind Evaluation Protocol</h4>
<div class="ltx_para" id="S4.SS6.SSS2.p1">
<p class="ltx_p">To ensure the objectivity of our findings, particularly for benchmarks requiring manual quality checks, we implemented a blind evaluation protocol. This methodology is crucial for mitigating cognitive biases in human raters, such as confirmation bias or the expectancy effect, where prior knowledge of a model’s identity could influence assessment <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="2508.12461v1.html#bib.bib83" title="">83</a>]</cite>. Our protocol involved three primary safeguards: first, all model outputs presented for manual review were fully anonymised, concealing their source. Second, automated scoring was utilised wherever feasible to minimise subjective human judgment. Finally, all post-hoc statistical analyses were conducted without access to model labels until the final compilation of results. This rigorous approach ensures that our reported outcomes are based solely on model performance, free from any preconceptions.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span class="ltx_text ltx_font_smallcaps">Results and Analysis</span>
</h2>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">V-A </span><span class="ltx_text ltx_font_italic">Performance Overview</span>
</h3>
<figure class="ltx_figure" id="S5.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="397" id="S5.F3.g1" src="2508.12461v1/x3.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span><span class="ltx_text ltx_font_bold">Performance rankings across benchmark categories using general prompts.</span> Error bars represent 95% confidence intervals following bootstrap methodology from Efron and Tibshirani <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="2508.12461v1.html#bib.bib69" title="">69</a>]</cite>. Llama-4-Scout scores low due to the triggered security feature preventing the model from responding to general prompts.</figcaption>
</figure>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p">Running eight models through five benchmark types gave us results that don’t fit neat patterns. Like Srivastava et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="2508.12461v1.html#bib.bib56" title="">56</a>]</cite> and Liang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="2508.12461v1.html#bib.bib35" title="">35</a>]</cite> found, parameter count doesn’t predict performance. GPT-OSS breaks this rule spectacularly: the gpt-oss-120B model loses to its gpt-oss-20B sibling consistently, which shouldn’t happen according to scaling laws <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="2508.12461v1.html#bib.bib3" title="">3</a>, <a class="ltx_ref" href="2508.12461v1.html#bib.bib5" title="">5</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S5.SS1.p2">
<p class="ltx_p">Wei et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="2508.12461v1.html#bib.bib84" title="">84</a>]</cite> and McKenzie et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="2508.12461v1.html#bib.bib85" title="">85</a>]</cite> saw similar inverse scaling in other contexts, but not this broadly. Stats check per Dror et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="2508.12461v1.html#bib.bib64" title="">64</a>]</cite>: p <math alttext="&lt;" class="ltx_Math" display="inline" id="S5.SS1.p2.m1"><semantics><mo>&lt;</mo><annotation encoding="application/x-tex">&lt;</annotation></semantics></math> 0.01, Cohen’s d = 0.73, so it’s real (not noise). This messes with what Ganguli et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="2508.12461v1.html#bib.bib86" title="">86</a>]</cite> said about predictable scaling.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">V-B </span><span class="ltx_text ltx_font_italic">Knowledge and Understanding</span>
</h3>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p">MMLU is basically the go-to test for general knowledge <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="2508.12461v1.html#bib.bib15" title="">15</a>, <a class="ltx_ref" href="2508.12461v1.html#bib.bib56" title="">56</a>]</cite>, and GPT-OSS doesn’t do great. Looking at different subjects, we see patterns like what Jiang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="2508.12461v1.html#bib.bib87" title="">87</a>]</cite> found: MoE architectures have weird knowledge gaps.</p>
</div>
<div class="ltx_para" id="S5.SS2.p2">
<p class="ltx_p">Digging deeper, the knowledge gaps are predictable and Roberts et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="2508.12461v1.html#bib.bib88" title="">88</a>]</cite> saw this coming. GPT-OSS handles STEM okay (72% mathematics, 74% physics) but tanks on humanities and professional stuff. Same issue Kandpal et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="2508.12461v1.html#bib.bib89" title="">89</a>]</cite> found: what you train on is what you get.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">V-C </span><span class="ltx_text ltx_font_italic">Reasoning and Problem Solving</span>
</h3>
<div class="ltx_para" id="S5.SS3.p1">
<p class="ltx_p">Mathematics problems via GSM8K <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="2508.12461v1.html#bib.bib16" title="">16</a>]</cite> and extras from Hendrycks et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="2508.12461v1.html#bib.bib15" title="">15</a>]</cite> went better for GPT-OSS. Chain-of-thought prompting <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="2508.12461v1.html#bib.bib58" title="">58</a>, <a class="ltx_ref" href="2508.12461v1.html#bib.bib59" title="">59</a>]</cite> boosted scores by 15%, which matches what Wang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="2508.12461v1.html#bib.bib90" title="">90</a>]</cite> found about reasoning tricks.</p>
</div>
<figure class="ltx_table" id="S5.T3">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE III: </span>Mathematical reasoning performance breakdown</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold">Model</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold">Basic</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold">Multi-step</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold">Word</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold">CoT Gain</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td" style="padding-left:4.0pt;padding-right:4.0pt;"></td>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold">Arithmetic</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold">Algebra</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold">Problems</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">(%)</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">DeepSeek-R1 70B</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="background-color:#FFF3B0;padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" style="background-color:#FFF3B0;">95</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="background-color:#FFF3B0;padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" style="background-color:#FFF3B0;">92</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="background-color:#FFF3B0;padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" style="background-color:#FFF3B0;">88</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">+8</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left" style="padding-left:4.0pt;padding-right:4.0pt;">Phi-4 Reasoning</td>
<td class="ltx_td ltx_align_center" style="background-color:#EDEDED;padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" style="background-color:#EDEDED;">93</span></td>
<td class="ltx_td ltx_align_center" style="background-color:#EDEDED;padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" style="background-color:#EDEDED;">89</span></td>
<td class="ltx_td ltx_align_center" style="background-color:#EDEDED;padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" style="background-color:#EDEDED;">82</span></td>
<td class="ltx_td ltx_align_center" style="background-color:#F7E1C6;padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" style="background-color:#F7E1C6;">+12</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left" style="padding-left:4.0pt;padding-right:4.0pt;">Llama 4 Scout</td>
<td class="ltx_td ltx_align_center" style="background-color:#F7E1C6;padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" style="background-color:#F7E1C6;">92</span></td>
<td class="ltx_td ltx_align_center" style="background-color:#F7E1C6;padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" style="background-color:#F7E1C6;">86</span></td>
<td class="ltx_td ltx_align_center" style="background-color:#F7E1C6;padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" style="background-color:#F7E1C6;">77</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">+10</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left" style="padding-left:4.0pt;padding-right:4.0pt;">GPT-OSS 20B</td>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">85</td>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">79</td>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">71</td>
<td class="ltx_td ltx_align_center" style="background-color:#FFF3B0;padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" style="background-color:#FFF3B0;">+15</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;">GPT-OSS 120B</td>
<td class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;">83</td>
<td class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;">77</td>
<td class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;">68</td>
<td class="ltx_td ltx_align_center ltx_border_bb" style="background-color:#EDEDED;padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" style="background-color:#EDEDED;">+14</span></td>
</tr>
</table>
</figure>
<figure class="ltx_figure" id="S5.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="287" id="S5.F4.g1" src="2508.12461v1/x4.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span><span class="ltx_text ltx_font_bold">Performance distribution</span> across evaluation categories (collected from published benchmark results). Analysis methodology follows BIG-bench protocols <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="2508.12461v1.html#bib.bib56" title="">56</a>]</cite> with visualization inspired by Burnell et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="2508.12461v1.html#bib.bib91" title="">91</a>]</cite>.</figcaption>
</figure>
<div class="ltx_para" id="S5.SS3.p2">
<p class="ltx_p">The nature of errors provides insight into architectural limitations, supporting theories proposed by Dziri et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="2508.12461v1.html#bib.bib92" title="">92</a>]</cite> on reasoning failures in transformers. Both GPT-OSS models consistently fail on problems requiring numerical precision maintenance, particularly those involving unit conversions, echoing observations by Razeghi et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="2508.12461v1.html#bib.bib93" title="">93</a>]</cite> on numerical reasoning challenges.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">V-D </span><span class="ltx_text ltx_font_italic">Code Generation Capabilities</span>
</h3>
<div class="ltx_para" id="S5.SS4.p1">
<p class="ltx_p">Code generation, evaluated through HumanEval <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="2508.12461v1.html#bib.bib18" title="">18</a>]</cite> and MBPP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="2508.12461v1.html#bib.bib19" title="">19</a>]</cite>, emerges as a relative strength for GPT-OSS models. This success aligns with findings from Xu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="2508.12461v1.html#bib.bib94" title="">94</a>]</cite> and Chen et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="2508.12461v1.html#bib.bib18" title="">18</a>]</cite> on the relationship between model architecture and code generation capabilities.</p>
</div>
<div class="ltx_para" id="S5.SS4.p2">
<p class="ltx_p">The efficiency of GPT-OSS 20B in generating correct solutions with minimal tokens contradicts assumptions from Li et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="2508.12461v1.html#bib.bib20" title="">20</a>]</cite> about the relationship between model size and code conciseness. Analysis of error patterns reveals consistency with findings from Zhang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="2508.12461v1.html#bib.bib95" title="">95</a>]</cite> on common failure modes in neural code generation, particularly in handling edge cases and complex data structures.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">V-E </span><span class="ltx_text ltx_font_italic">Case Study: Logic Reasoning Task with Response Quality Analysis</span>
</h3>
<div class="ltx_para" id="S5.SS5.p1">
<p class="ltx_p">To provide deeper insights into model behaviour beyond traditional accuracy metrics, we conducted a detailed evaluation using a logic puzzle that admits multiple valid solutions. This case study illustrates not only reasoning capabilities but also critical aspects of response quality that affect real-world usability.</p>
</div>
<section class="ltx_subsubsection" id="S5.SS5.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">V-E1 </span>Task Design and Evaluation Framework</h4>
<div class="ltx_para" id="S5.SS5.SSS1.p1">
<p class="ltx_p">The logic puzzle involves three individuals with distinct preferences for colours, constrained by five logical rules. This task was specifically chosen because it admits exactly two valid solutions, allowing us to assess models’ ability to recognise logical ambiguity, a higher order reasoning capability often overlooked in standard benchmarks. Our evaluation framework extends beyond binary correctness to encompass multiple quality dimensions that critically impact practical deployment.</p>
</div>
<figure class="ltx_table" id="S5.T4">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE IV: </span><span class="ltx_text ltx_font_bold">Logic reasoning task.</span> Comprehensive performance and multi-dimensional quality assessment.
Green = “Excellent” (qualitative). Blue = best numeric value (higher for speed, lower for tokens).</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" rowspan="2" style="padding:0.75pt 4.0pt;"><span class="ltx_text ltx_font_bold">Model</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="2" style="padding:0.75pt 4.0pt;"><span class="ltx_text ltx_font_bold">Task Perf.</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding:0.75pt 4.0pt;"><span class="ltx_text ltx_font_bold">Resp. Len</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="4" style="padding:0.75pt 4.0pt;"><span class="ltx_text ltx_font_bold">Quality Dimensions</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" rowspan="2" style="padding:0.75pt 4.0pt;"><span class="ltx_text ltx_font_bold">Overall</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" style="padding:0.75pt 4.0pt;"><span class="ltx_text ltx_font_bold">Efficiency</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center" style="padding:0.75pt 4.0pt;">Correct</td>
<td class="ltx_td ltx_align_center ltx_border_r" style="padding:0.75pt 4.0pt;">Sol.</td>
<td class="ltx_td ltx_align_center ltx_border_r" style="padding:0.75pt 4.0pt;">(chars)</td>
<td class="ltx_td ltx_align_center" style="padding:0.75pt 4.0pt;">Len App.</td>
<td class="ltx_td ltx_align_center" style="padding:0.75pt 4.0pt;">Read.</td>
<td class="ltx_td ltx_align_center" style="padding:0.75pt 4.0pt;">Clarity</td>
<td class="ltx_td ltx_align_center ltx_border_r" style="padding:0.75pt 4.0pt;">Concise.</td>
<td class="ltx_td ltx_align_right" style="padding:0.75pt 4.0pt;">Tokens</td>
<td class="ltx_td ltx_align_right" style="padding:0.75pt 4.0pt;">tok/s</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding:0.75pt 4.0pt;">GPT-OSS 120B</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding:0.75pt 4.0pt;">✓</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:0.75pt 4.0pt;">Both</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:0.75pt 4.0pt;">2,399</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="background-color:#C6EFCE;padding:0.75pt 4.0pt;"><span class="ltx_text" style="background-color:#C6EFCE;">Excellent</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="background-color:#C6EFCE;padding:0.75pt 4.0pt;"><span class="ltx_text" style="background-color:#C6EFCE;">Excellent</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="background-color:#C6EFCE;padding:0.75pt 4.0pt;"><span class="ltx_text" style="background-color:#C6EFCE;">Excellent</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="background-color:#FFEB9C;padding:0.75pt 4.0pt;"><span class="ltx_text" style="background-color:#FFEB9C;">Good</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="background-color:#C6EFCE;padding:0.75pt 4.0pt;"><span class="ltx_text" style="background-color:#C6EFCE;">Excellent</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" style="padding:0.75pt 4.0pt;">1,716</td>
<td class="ltx_td ltx_align_right ltx_border_t" style="padding:0.75pt 4.0pt;">128.3</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_r" style="padding:0.75pt 4.0pt;">GPT-OSS 20B</td>
<td class="ltx_td ltx_align_center" style="padding:0.75pt 4.0pt;">✓</td>
<td class="ltx_td ltx_align_center ltx_border_r" style="padding:0.75pt 4.0pt;">One</td>
<td class="ltx_td ltx_align_center ltx_border_r" style="padding:0.75pt 4.0pt;">2,387</td>
<td class="ltx_td ltx_align_center" style="background-color:#C6EFCE;padding:0.75pt 4.0pt;"><span class="ltx_text" style="background-color:#C6EFCE;">Excellent</span></td>
<td class="ltx_td ltx_align_center" style="background-color:#FFEB9C;padding:0.75pt 4.0pt;"><span class="ltx_text" style="background-color:#FFEB9C;">Good</span></td>
<td class="ltx_td ltx_align_center" style="background-color:#C6EFCE;padding:0.75pt 4.0pt;"><span class="ltx_text" style="background-color:#C6EFCE;">Excellent</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" style="background-color:#FFEB9C;padding:0.75pt 4.0pt;"><span class="ltx_text" style="background-color:#FFEB9C;">Good</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" style="background-color:#FFEB9C;padding:0.75pt 4.0pt;"><span class="ltx_text" style="background-color:#FFEB9C;">Good</span></td>
<td class="ltx_td ltx_align_right" style="padding:0.75pt 4.0pt;">3,218</td>
<td class="ltx_td ltx_align_right" style="background-color:#BDD7EE;padding:0.75pt 4.0pt;"><span class="ltx_text ltx_font_bold" style="background-color:#BDD7EE;">178.0</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_r" style="padding:0.75pt 4.0pt;">Gemma 3 27B</td>
<td class="ltx_td ltx_align_center" style="padding:0.75pt 4.0pt;">✓</td>
<td class="ltx_td ltx_align_center ltx_border_r" style="padding:0.75pt 4.0pt;">One</td>
<td class="ltx_td ltx_align_center ltx_border_r" style="padding:0.75pt 4.0pt;">2,787</td>
<td class="ltx_td ltx_align_center" style="background-color:#C6EFCE;padding:0.75pt 4.0pt;"><span class="ltx_text" style="background-color:#C6EFCE;">Excellent</span></td>
<td class="ltx_td ltx_align_center" style="background-color:#FFEB9C;padding:0.75pt 4.0pt;"><span class="ltx_text" style="background-color:#FFEB9C;">Good</span></td>
<td class="ltx_td ltx_align_center" style="background-color:#C6EFCE;padding:0.75pt 4.0pt;"><span class="ltx_text" style="background-color:#C6EFCE;">Excellent</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" style="background-color:#FFE6CC;padding:0.75pt 4.0pt;"><span class="ltx_text" style="background-color:#FFE6CC;">Medium</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" style="background-color:#FFEB9C;padding:0.75pt 4.0pt;"><span class="ltx_text" style="background-color:#FFEB9C;">Good</span></td>
<td class="ltx_td ltx_align_right" style="background-color:#BDD7EE;padding:0.75pt 4.0pt;"><span class="ltx_text ltx_font_bold" style="background-color:#BDD7EE;">986</span></td>
<td class="ltx_td ltx_align_right" style="padding:0.75pt 4.0pt;">83.6</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_r" style="padding:0.75pt 4.0pt;">Llama 4 Scout</td>
<td class="ltx_td ltx_align_center" style="padding:0.75pt 4.0pt;">✓</td>
<td class="ltx_td ltx_align_center ltx_border_r" style="padding:0.75pt 4.0pt;">One</td>
<td class="ltx_td ltx_align_center ltx_border_r" style="padding:0.75pt 4.0pt;">3,249</td>
<td class="ltx_td ltx_align_center" style="background-color:#FFEB9C;padding:0.75pt 4.0pt;"><span class="ltx_text" style="background-color:#FFEB9C;">Good</span></td>
<td class="ltx_td ltx_align_center" style="background-color:#FFEB9C;padding:0.75pt 4.0pt;"><span class="ltx_text" style="background-color:#FFEB9C;">Good</span></td>
<td class="ltx_td ltx_align_center" style="background-color:#C6EFCE;padding:0.75pt 4.0pt;"><span class="ltx_text" style="background-color:#C6EFCE;">Excellent</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" style="background-color:#FFE6CC;padding:0.75pt 4.0pt;"><span class="ltx_text" style="background-color:#FFE6CC;">Medium</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" style="background-color:#FFEB9C;padding:0.75pt 4.0pt;"><span class="ltx_text" style="background-color:#FFEB9C;">Good</span></td>
<td class="ltx_td ltx_align_right" style="padding:0.75pt 4.0pt;">1,275</td>
<td class="ltx_td ltx_align_right" style="padding:0.75pt 4.0pt;">106.6</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_r" style="padding:0.75pt 4.0pt;">Llama 3.3 70B</td>
<td class="ltx_td ltx_align_center" style="padding:0.75pt 4.0pt;">×</td>
<td class="ltx_td ltx_align_center ltx_border_r" style="padding:0.75pt 4.0pt;">Wrong</td>
<td class="ltx_td ltx_align_center ltx_border_r" style="padding:0.75pt 4.0pt;">5,506</td>
<td class="ltx_td ltx_align_center" style="background-color:#FFE6CC;padding:0.75pt 4.0pt;"><span class="ltx_text" style="background-color:#FFE6CC;">Medium</span></td>
<td class="ltx_td ltx_align_center" style="background-color:#FFE6CC;padding:0.75pt 4.0pt;"><span class="ltx_text" style="background-color:#FFE6CC;">Medium</span></td>
<td class="ltx_td ltx_align_center" style="background-color:#FFC7CE;padding:0.75pt 4.0pt;"><span class="ltx_text" style="background-color:#FFC7CE;">Poor</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" style="background-color:#FFC7CE;padding:0.75pt 4.0pt;"><span class="ltx_text" style="background-color:#FFC7CE;">Poor</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" style="background-color:#FFC7CE;padding:0.75pt 4.0pt;"><span class="ltx_text" style="background-color:#FFC7CE;">Poor</span></td>
<td class="ltx_td ltx_align_right" style="padding:0.75pt 4.0pt;">1,414</td>
<td class="ltx_td ltx_align_right" style="padding:0.75pt 4.0pt;">43.3</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_r" style="padding:0.75pt 4.0pt;">DeepSeek-R1 70B</td>
<td class="ltx_td ltx_align_center" style="padding:0.75pt 4.0pt;">✓</td>
<td class="ltx_td ltx_align_center ltx_border_r" style="padding:0.75pt 4.0pt;">Both</td>
<td class="ltx_td ltx_align_center ltx_border_r" style="padding:0.75pt 4.0pt;">26,589</td>
<td class="ltx_td ltx_align_center" style="background-color:#FFC7CE;padding:0.75pt 4.0pt;"><span class="ltx_text" style="background-color:#FFC7CE;">Poor</span></td>
<td class="ltx_td ltx_align_center" style="background-color:#FFC7CE;padding:0.75pt 4.0pt;"><span class="ltx_text" style="background-color:#FFC7CE;">Poor</span></td>
<td class="ltx_td ltx_align_center" style="background-color:#FFEB9C;padding:0.75pt 4.0pt;"><span class="ltx_text" style="background-color:#FFEB9C;">Good</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" style="background-color:#FFC7CE;padding:0.75pt 4.0pt;"><span class="ltx_text" style="background-color:#FFC7CE;">Poor</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" style="background-color:#FFC7CE;padding:0.75pt 4.0pt;"><span class="ltx_text" style="background-color:#FFC7CE;">Poor</span></td>
<td class="ltx_td ltx_align_right" style="padding:0.75pt 4.0pt;">6,827</td>
<td class="ltx_td ltx_align_right" style="padding:0.75pt 4.0pt;">36.3</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_r" style="padding:0.75pt 4.0pt;">Phi-4 Reasoning</td>
<td class="ltx_td ltx_align_center" style="padding:0.75pt 4.0pt;">✓</td>
<td class="ltx_td ltx_align_center ltx_border_r" style="padding:0.75pt 4.0pt;">Both</td>
<td class="ltx_td ltx_align_center ltx_border_r" style="padding:0.75pt 4.0pt;">27,459</td>
<td class="ltx_td ltx_align_center" style="background-color:#FFC7CE;padding:0.75pt 4.0pt;"><span class="ltx_text" style="background-color:#FFC7CE;">Poor</span></td>
<td class="ltx_td ltx_align_center" style="background-color:#FFC7CE;padding:0.75pt 4.0pt;"><span class="ltx_text" style="background-color:#FFC7CE;">Poor</span></td>
<td class="ltx_td ltx_align_center" style="background-color:#FFEB9C;padding:0.75pt 4.0pt;"><span class="ltx_text" style="background-color:#FFEB9C;">Good</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" style="background-color:#FFC7CE;padding:0.75pt 4.0pt;"><span class="ltx_text" style="background-color:#FFC7CE;">Poor</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" style="background-color:#FFC7CE;padding:0.75pt 4.0pt;"><span class="ltx_text" style="background-color:#FFC7CE;">Poor</span></td>
<td class="ltx_td ltx_align_right" style="padding:0.75pt 4.0pt;">7,348</td>
<td class="ltx_td ltx_align_right" style="padding:0.75pt 4.0pt;">111.2</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" style="padding:0.75pt 4.0pt;">Qwen 3 235B</td>
<td class="ltx_td ltx_align_center ltx_border_bb" style="padding:0.75pt 4.0pt;">×</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" style="padding:0.75pt 4.0pt;">Incomplete</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" style="padding:0.75pt 4.0pt;">132,137</td>
<td class="ltx_td ltx_align_center ltx_border_bb" style="background-color:#FFC7CE;padding:0.75pt 4.0pt;"><span class="ltx_text" style="background-color:#FFC7CE;">Poor</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" style="background-color:#FFC7CE;padding:0.75pt 4.0pt;"><span class="ltx_text" style="background-color:#FFC7CE;">Poor</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" style="background-color:#FFC7CE;padding:0.75pt 4.0pt;"><span class="ltx_text" style="background-color:#FFC7CE;">Poor</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" style="background-color:#FFC7CE;padding:0.75pt 4.0pt;"><span class="ltx_text" style="background-color:#FFC7CE;">Poor</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" style="background-color:#FFC7CE;padding:0.75pt 4.0pt;"><span class="ltx_text" style="background-color:#FFC7CE;">Poor</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" style="padding:0.75pt 4.0pt;">41,080</td>
<td class="ltx_td ltx_align_right ltx_border_bb" style="padding:0.75pt 4.0pt;">46.9</td>
</tr>
</table>
</figure>
</section>
<section class="ltx_subsubsection" id="S5.SS5.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">V-E2 </span>Quality Dimensions Analysis</h4>
<figure class="ltx_figure" id="S5.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="540" id="S5.F5.g1" src="2508.12461v1/x5.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span><span class="ltx_text ltx_font_bold">Performance heatmap</span> across model-benchmark combinations.</figcaption>
</figure>
<figure class="ltx_figure" id="S5.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="480" id="S5.F6.g1" src="2508.12461v1/x6.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span><span class="ltx_text ltx_font_bold">Detailed performance analysis</span> of GPT-OSS models across task subcategories.</figcaption>
</figure>
<div class="ltx_para" id="S5.SS5.SSS2.p1">
<p class="ltx_p">The comprehensive quality assessment reveals distinct patterns across four critical dimensions, as shown in <span class="ltx_text ltx_font_bold">Table <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:Table:Logic_Reasoning_Task</span></span>. <span class="ltx_text ltx_font_italic">Length appropriateness</span> evaluates whether responses fall within the empirically determined optimal range of 1,000-3,000 characters for complex reasoning tasks, with models exceeding this threshold by an order of magnitude receiving poor ratings. <span class="ltx_text ltx_font_italic">Human readability</span> assesses structural clarity, logical flow, and the presence of clean, well-formatted output versus exposed internal reasoning chains or computational traces. <span class="ltx_text ltx_font_italic">Answer clarity</span> measures how effectively the model communicates its solution independent of correctness, while <span class="ltx_text ltx_font_italic">conciseness</span> quantifies the absence of unnecessary repetition, verbosity, or redundant explanations.</p>
</div>
<div class="ltx_para" id="S5.SS5.SSS2.p2">
<p class="ltx_p">The stark contrast between models is particularly evident in the relationship between correctness and quality. DeepSeek-R1 70B and Phi-4 Reasoning, despite correctly identifying both valid solutions, received poor overall quality ratings due to their excessive verbosity, exposing over 26,000 characters of internal reasoning that severely impairs usability. Conversely, gpt-oss-120B achieved the same logical completeness while maintaining excellent quality scores across all dimensions, demonstrating that superior reasoning need not compromise presentation quality.</p>
</div>
<figure class="ltx_figure" id="S5.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="407" id="S5.F7.g1" src="2508.12461v1/x7.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span><span class="ltx_text ltx_font_bold">Token count distribution across all models on aggregated datasets.</span> Analysis reveals distinct response length patterns, with GPT-OSS models exhibiting notably concise outputs compared to reasoning optimised architectures.</figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="S5.SS5.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">V-E3 </span>Key Findings and Implications</h4>
<div class="ltx_para" id="S5.SS5.SSS3.p1">
<p class="ltx_p">The evaluation revealed that while 75% of models successfully solved the logic puzzle, response quality varied dramatically. Notably, gpt-oss-120B demonstrated superior performance by recognising both valid solutions while maintaining excellent response quality with concise, well-structured output. This contrasts sharply with models featuring explicit chain-of-thought mechanisms (DeepSeek-R1, Phi-4 Reasoning), which exposed internal reasoning processes resulting in responses exceeding 26,000 characters, a 10-fold increase that severely impacts readability.</p>
</div>
<div class="ltx_para" id="S5.SS5.SSS3.p2">
<p class="ltx_p">The most pathological case, Qwen 3 235B, generated over 132,000 characters with massive repetition patterns, including hundreds of instances of the phrase “Final Answer,” despite being the largest model evaluated. This highlights a critical insight: parameter count alone does not guarantee response quality or even basic task completion.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S5.SS5.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">V-E4 </span>Response Quality Distribution</h4>
<div class="ltx_para" id="S5.SS5.SSS4.p1">
<p class="ltx_p">Analysis of response characteristics revealed an optimal range of 2,000-3,000 characters for complex reasoning tasks, balancing completeness with human readability. Models within this range consistently achieved higher quality scores across dimensions of length appropriateness, readability, clarity, and conciseness. The GPT-OSS models’ performance in this task demonstrates their ability to produce human-friendly outputs without sacrificing correctness, a crucial factor for practical deployment often overlooked in traditional benchmarking.</p>
</div>
<div class="ltx_para" id="S5.SS5.SSS4.p2">
<p class="ltx_p">This case study underscores a fundamental tension in current LLM design between reasoning transparency and usability. While exposing internal reasoning processes may aid in verification and debugging, our findings suggest that most practical applications benefit from what we term “selective transparency” and maintaining internal reasoning while presenting only refined conclusions to users. The GPT-OSS models’ balanced approach in this regard represents a pragmatic design choice that prioritises end-user experience without compromising solution quality.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S5.SS6">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">V-F </span><span class="ltx_text ltx_font_italic">Domain-Specific Performance</span>
</h3>
<div class="ltx_para" id="S5.SS6.p1">
<p class="ltx_p">Evaluation across specialised domains reveals varied capabilities, as illustrated in <span class="ltx_text ltx_font_bold">Fig <a class="ltx_ref" href="2508.12461v1.html#S5.F5" title="Figure 5 ‣ V-E2 Quality Dimensions Analysis ‣ V-E Case Study: Logic Reasoning Task with Response Quality Analysis ‣ V Results and Analysis ‣ Is GPT-OSS Good? A Comprehensive Evaluation of OpenAI’s Latest Open Source Models ∗: Equal contribution. †: Corresponding author: Junhao Song (junhao.song23@imperial.ac.uk)"><span class="ltx_text ltx_ref_tag">5</span></a></span>. Financial reasoning through FinQA shows GPT-OSS models achieving moderate performance, with the gpt-oss-120B at 84% accuracy. Scientific reasoning (SciQ), medical knowledge (MedQA), and legal understanding (LegalQA) benchmarks demonstrate consistent mid-tier performance. Physical reasoning through PIQA and conversational summarisation via DialogSum further confirms the models’ general-purpose nature without exceptional strength in any particular domain.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS7">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">V-G </span><span class="ltx_text ltx_font_italic">Conversational Coherence</span>
</h3>
<div class="ltx_para" id="S5.SS7.p1">
<p class="ltx_p">Multi-turn dialogue evaluation through MT-Bench <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="2508.12461v1.html#bib.bib38" title="">38</a>]</cite> reveals moderate conversational capabilities for GPT-OSS models. The degradation pattern analysis, following methodologies from See et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="2508.12461v1.html#bib.bib96" title="">96</a>]</cite>, reveals increasing confusion after 4-5 conversation turns.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS8">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">V-H </span><span class="ltx_text ltx_font_italic">Computational Efficiency Analysis</span>
</h3>
<div class="ltx_para" id="S5.SS8.p1">
<p class="ltx_p">The relationship between computational resources and performance reveals critical insights consistent with work by Strubell et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="2508.12461v1.html#bib.bib81" title="">81</a>]</cite> and Patterson et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="2508.12461v1.html#bib.bib82" title="">82</a>]</cite> on model efficiency. gpt-oss-120B’s 3.2<math alttext="\times" class="ltx_Math" display="inline" id="S5.SS8.p1.m1"><semantics><mo>×</mo><annotation encoding="application/x-tex">\times</annotation></semantics></math> resource requirement for a negative performance delta represents a fundamental failure in scaling efficiency, supporting arguments by Tay et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="2508.12461v1.html#bib.bib97" title="">97</a>]</cite> on the importance of efficiency-aware model design.</p>
</div>
<div class="ltx_para" id="S5.SS8.p2">
<p class="ltx_p">Following the reporting protocols of Henderson et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="2508.12461v1.html#bib.bib98" title="">98</a>]</cite> and Schwartz et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="2508.12461v1.html#bib.bib99" title="">99</a>]</cite>, node-level power instrumentation shows that gpt-oss-20B uses <span class="ltx_text ltx_font_bold">2.6<math alttext="\times" class="ltx_Math" display="inline" id="S5.SS8.p2.m1"><semantics><mo>×</mo><annotation encoding="application/x-tex">\times</annotation></semantics></math> less energy per completed response</span> at target accuracy than gpt-oss-120B, attributable to reduced active parameters per token and a smaller KV cache. This reinforces the practical relevance of efficiency metrics when comparing architectures of different scales.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS9">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">V-I </span><span class="ltx_text ltx_font_italic">Statistical Significance and Reliability</span>
</h3>
<div class="ltx_para" id="S5.SS9.p1">
<p class="ltx_p">Rigorous statistical analysis following best practices from Dror et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="2508.12461v1.html#bib.bib64" title="">64</a>]</cite> and Card et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="2508.12461v1.html#bib.bib36" title="">36</a>]</cite> confirms the reliability of our findings. McNemar’s test <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="2508.12461v1.html#bib.bib66" title="">66</a>]</cite> with Bonferroni correction <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="2508.12461v1.html#bib.bib67" title="">67</a>]</cite> yields p-values below 0.05 for all reported differences. Effect sizes, calculated following Cohen <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="2508.12461v1.html#bib.bib68" title="">68</a>]</cite> and interpreted according to Sawilowsky <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="2508.12461v1.html#bib.bib100" title="">100</a>]</cite>, range from medium (d = 0.52) to large (d = 1.84).</p>
</div>
<div class="ltx_para" id="S5.SS9.p2">
<p class="ltx_p">Bootstrap confidence intervals <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="2508.12461v1.html#bib.bib69" title="">69</a>]</cite> constructed through 1000 iterations remain narrow, with a maximum width of ±2.1%, confirming stable estimates consistent with recommendations from Koehn <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="2508.12461v1.html#bib.bib72" title="">72</a>]</cite>. Inter-rater reliability achieves <math alttext="\kappa" class="ltx_Math" display="inline" id="S5.SS9.p2.m1"><semantics><mi>κ</mi><annotation encoding="application/x-tex">\kappa</annotation></semantics></math> = 0.87 following Landis and Koch <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="2508.12461v1.html#bib.bib101" title="">101</a>]</cite>, indicating strong agreement. Sensitivity analysis following Ulmer et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="2508.12461v1.html#bib.bib102" title="">102</a>]</cite> shows minimal impact of hyperparameter variations on relative rankings.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS10">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">V-J </span><span class="ltx_text ltx_font_italic">Efficiency Evaluation</span>
</h3>
<div class="ltx_para" id="S5.SS10.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Memory footprint:</span> On A100/H100-class hardware, peak GPU memory stabilised around <span class="ltx_text ltx_font_bold">80 GB</span> per device for gpt-oss-120B (MoE; 8 experts active per token) and <span class="ltx_text ltx_font_bold">16 GB</span> for gpt-oss-20B under our batch/sequence settings (including 4-bit KV cache and activation checkpointing). The 5<math alttext="\times" class="ltx_Math" display="inline" id="S5.SS10.p1.m1"><semantics><mo>×</mo><annotation encoding="application/x-tex">\times</annotation></semantics></math> reduction for 20B enables higher-batch, lower-latency serving tiers and easier horizontal scaling.
<span class="ltx_text ltx_font_bold">Throughput and latency:</span> With identical decoding settings, end-to-end throughput measured <span class="ltx_text ltx_font_bold">128 tokens/s</span> (120B) vs <span class="ltx_text ltx_font_bold">178 tokens/s</span> (20B) on matched H100 clusters, with 20B achieving lower time-to-first-token and better steady-state throughput under moderate concurrency.
<span class="ltx_text ltx_font_bold">Latency under multi-turn dialogue:</span> Consistent with engineering analyses by Pope et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="2508.12461v1.html#bib.bib103" title="">103</a>]</cite>, incremental latency growth is observed in extended conversations for gpt-oss-120B, increasing from <span class="ltx_text ltx_font_bold">0.8</span> to <span class="ltx_text ltx_font_bold">2.9</span> seconds over ten turns, reflecting attention-scaling costs that compound with conversational depth. The gpt-oss-20B model hit <span class="ltx_text ltx_font_bold">2.6<math alttext="\times" class="ltx_Math" display="inline" id="S5.SS10.p1.m2"><semantics><mo>×</mo><annotation encoding="application/x-tex">\times</annotation></semantics></math> less energy per completed response</span> at target accuracy than 120B, which can be attributed to the reduced active parameters per token and a smaller KV cache.
<span class="ltx_text ltx_font_bold">Active parameters per token:</span> MoE routing yields small active parameter counts per token: <span class="ltx_text ltx_font_bold">5.1B</span> for gpt-oss-120B and <span class="ltx_text ltx_font_bold">3.6B</span> for gpt-oss-20B, respectively, providing an attractive balance between capability and computational efficiency.
<span class="ltx_text ltx_font_bold">Output economy:</span> As shown by the token in <span class="ltx_text ltx_font_bold">Fig <a class="ltx_ref" href="2508.12461v1.html#S5.F7" title="Figure 7 ‣ V-E2 Quality Dimensions Analysis ‣ V-E Case Study: Logic Reasoning Task with Response Quality Analysis ‣ V Results and Analysis ‣ Is GPT-OSS Good? A Comprehensive Evaluation of OpenAI’s Latest Open Source Models ∗: Equal contribution. †: Corresponding author: Junhao Song (junhao.song23@imperial.ac.uk)"><span class="ltx_text ltx_ref_tag">7</span></a></span>, GPT-OSS models tend to produce comparatively concise outputs for correct solutions, which can materially affect cost at scale.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS11">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">V-K </span><span class="ltx_text ltx_font_italic">Mutlilingual Capability </span>
</h3>
<div class="ltx_para" id="S5.SS11.p1">
<p class="ltx_p">The multilingual evaluation exposes significant limitations, with both GPT-OSS models achieving below 45% accuracy on Chinese-language tasks. Strong performance from models like Qwen 3, which incorporate language-specific optimisation, suggests that general-purpose training may be insufficient for true multilingual capability.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS12">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">V-L </span><span class="ltx_text ltx_font_italic">Synthesis and Implications</span>
</h3>
<div class="ltx_para" id="S5.SS12.p1">
<p class="ltx_p">This benchmark study provides critical insight into OpenAI’s GPT-OSS models within the current open source landscape. Our evaluation across five benchmark categories: general understanding, mathematical reasoning, code generation, Chinese comprehension and conversational abilities. Also can reveals a complex picture that challenges simplistic narratives about scale and performance.</p>
</div>
<div class="ltx_para" id="S5.SS12.p2">
<p class="ltx_p">The most striking finding is the inconsistent scaling between gpt-oss-120B and gpt-oss-20B models. Contrary to established scaling laws, the smaller 20B variant outperforms its larger counterpart on multiple benchmarks, including MMLU (69% vs 66%) and SCIQ (87% vs 82%). This inverse scaling suggests potential inefficiencies in the MoE routing mechanism or suboptimal training configuration for the larger model. The consistency across diverse benchmarks and task types, combined with rigorous statistical validation (<math alttext="p&lt;0.01" class="ltx_Math" display="inline" id="S5.SS12.p2.m1"><semantics><mrow><mi>p</mi><mo>&lt;</mo><mn>0.01</mn></mrow><annotation encoding="application/x-tex">p&lt;0.01</annotation></semantics></math>), confirms this is a genuine architectural phenomenon worth investigating.</p>
</div>
<div class="ltx_para" id="S5.SS12.p3">
<p class="ltx_p">GPT-OSS models occupy a middle tier in the current open source ecosystem. While they demonstrate competence across various tasks, they are consistently outperformed by newer architectures. Llama 4 Scout’s 85% accuracy on MMLU and DeepSeek-R1’s strong reasoning capability highlight the rapid pace of advancement. However, GPT-OSS models show particular strength in code generation, where efficiency and concise outputs provide practical advantages over several larger models of similar scale.</p>
</div>
<div class="ltx_para" id="S5.SS12.p4">
<p class="ltx_p">For practitioners, gpt-oss-20B offers superior cost-performance for most applications. It matches or exceeds the 120B variant on several tasks while requiring 5<math alttext="\times" class="ltx_Math" display="inline" id="S5.SS12.p4.m1"><semantics><mo>×</mo><annotation encoding="application/x-tex">\times</annotation></semantics></math> less GPU memory, 2.6<math alttext="\times" class="ltx_Math" display="inline" id="S5.SS12.p4.m2"><semantics><mo>×</mo><annotation encoding="application/x-tex">\times</annotation></semantics></math> lower energy per response, and delivering higher throughput. These models are best suited for code generation, structured reasoning, and English-language processing, and are less suitable for multilingual or specialised professional domains.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span><span class="ltx_text ltx_font_smallcaps">Conclusion</span>
</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p">We presented a comprehensive, multi-domain evaluation of OpenAI’s GPT-OSS models and analysed their behaviour within the broader open source landscape. Our findings clarify theoretical and practical aspects of modern language models: most notably, the observed inverse scaling between GPT-OSS variants challenges the assumption that parameter count alone predicts capability. Comparative evidence across dense, MoE, and reasoning-optimised architectures underscores the importance of architectural and training innovations beyond scale.</p>
</div>
<div class="ltx_para" id="S6.p2">
<p class="ltx_p">This study relies on established benchmarks that, while widely adopted, do not fully capture emerging capabilities or real-world robustness. We also did not exhaustively optimise prompting or decoding strategies for any single model. As the ecosystem evolves rapidly, results represent a snapshot in time. Future work should incorporate dynamic and longitudinal evaluations, task-specific assessments tailored to MoE architectures, and systematic studies of prompting and inference time trade-offs.</p>
</div>
<div class="ltx_para" id="S6.p3">
<p class="ltx_p">GPT-OSS models add architectural diversity and provide evidence to refine our understanding of MoE scaling dynamics. Insights from this evaluation can inform the design and deployment of sparse models and guide prioritisation of efficiency-aware research directions. The results offer a grounded basis for model selection and deployment decisions while motivating rigorous, transparent evaluation practices and more comprehensive benchmarks that capture the full spectrum of capabilities.</p>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_font_smallcaps ltx_title_section">Acknowledgments</h2>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p">We thank the open source community for providing access to the evaluated models via standardised interfaces. We acknowledge the computational resources provided by our institution, which made this evaluation possible. Special thanks to the maintainers of the benchmark datasets and evaluation frameworks used in this study.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
N. Shazeer, A. Mirhoseini, K. Maziarz, A. Davis, Q. Le, G. Hinton, and J. Dean, “Outrageously large neural networks: The sparsely-gated mixture-of-experts layer,” in <em class="ltx_emph ltx_font_italic">International Conference on Learning Representations</em>, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
W. Fedus, B. Zoph, and N. Shazeer, “Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity,” <em class="ltx_emph ltx_font_italic">The Journal of Machine Learning Research</em>, vol. 23, no. 1, pp. 5232–5270, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child, S. Gray, A. Radford, J. Wu, and D. Amodei, “Scaling laws for neural language models,” <em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2001.08361</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
T. Henighan, J. Kaplan, M. Katz, M. Chen, C. Hesse, J. Jackson, H. Jun, T. B. Brown, P. Dhariwal, S. Gray <em class="ltx_emph ltx_font_italic">et al.</em>, “Scaling laws for autoregressive generative modeling,” <em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2010.14701</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. d. L. Casas, L. A. Hendricks, J. Welbl, A. Clark <em class="ltx_emph ltx_font_italic">et al.</em>, “Training compute-optimal large language models,” <em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2203.15556</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever <em class="ltx_emph ltx_font_italic">et al.</em>, “Language models are unsupervised multitask learners,” <em class="ltx_emph ltx_font_italic">OpenAI blog</em>, vol. 1, no. 8, p. 9, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozière, N. Goyal, E. Hambro, F. Azhar <em class="ltx_emph ltx_font_italic">et al.</em>, “Llama: Open and efficient foundation language models,” <em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2302.13971</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
G. Team, T. Mesnard, C. Hardin, R. Dadashi, S. Bhupatiraju, S. Pathak, L. Sifre, M. Rivière, M. S. Kale, J. Love <em class="ltx_emph ltx_font_italic">et al.</em>, “Gemma: Open models based on gemini research and technology,” <em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2403.08295</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
D.-A. and:, X. Bi, D. Chen, G. Chen, S. Chen, D. Dai, C. Deng, H. Ding, K. Dong <em class="ltx_emph ltx_font_italic">et al.</em>, “Deepseek llm: Scaling open-source language models with longtermism,” <em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2401.02954</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
J. Bai, S. Bai, Y. Chu, Z. Cui, K. Dang, X. Deng, Y. Fan, W. Ge, Y. Han, F. Huang <em class="ltx_emph ltx_font_italic">et al.</em>, “Qwen technical report,” <em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2309.16609</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Qwen, :, A. Yang, B. Yang, B. Zhang, B. Hui, B. Zheng, B. Yu, C. Li, D. Liu, F. Huang, H. Wei, H. Lin, J. Yang, J. Tu, J. Zhang, J. Yang, J. Yang, J. Zhou, J. Lin, K. Dang, K. Lu, K. Bao, K. Yang, L. Yu, M. Li, M. Xue, P. Zhang, Q. Zhu, R. Men, R. Lin, T. Li, T. Tang, T. Xia, X. Ren, X. Ren, Y. Fan, Y. Su, Y. Zhang, Y. Wan, Y. Liu, Z. Cui, Z. Zhang, and Z. Qiu, “Qwen2.5 technical report,” <em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2409.12186</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
M. Abdin, S. A. Jacobs, A. A. Awan, J. Aneja, A. Awadallah, H. Awadalla, N. Bach, A. Bahree, A. Bakhtiari, H. Behl <em class="ltx_emph ltx_font_italic">et al.</em>, “Phi-3 technical report: A highly capable language model locally on your phone,” <em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2404.14219</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
J. Wei, Y. Tay, R. Bommasani, C. Raffel, B. Zoph, S. Borgeaud, D. Yogatama, M. Bosma, D. Zhou, D. Metzler <em class="ltx_emph ltx_font_italic">et al.</em>, “Emergent abilities of large language models,” <em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2206.07682</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
R. Schaeffer, B. Miranda, and S. Koyejo, “Are emergent abilities of large language models a mirage?” <em class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, vol. 36, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt, “Measuring massive multitask language understanding,” <em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2009.03300</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek, J. Hilton, R. Nakano <em class="ltx_emph ltx_font_italic">et al.</em>, “Training verifiers to solve math word problems,” <em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2110.14168</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
A. Lewkowycz, A. Andreassen, D. Dohan, E. Dyer, H. Michalewski, V. Ramasesh, A. Slone, C. Anil, I. Schlag, T. Gutman-Solo <em class="ltx_emph ltx_font_italic">et al.</em>, “Solving quantitative reasoning problems with language models,” <em class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, vol. 35, pp. 3843–3857, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. d. O. Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman <em class="ltx_emph ltx_font_italic">et al.</em>, “Evaluating large language models trained on code,” <em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2107.03374</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
J. Austin, A. Odena, M. Nye, M. Bosma, H. Michalewski, D. Dohan, E. Jiang, C. Cai, M. Terry, Q. Le <em class="ltx_emph ltx_font_italic">et al.</em>, “Program synthesis with large language models,” <em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2108.07732</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
R. Li, L. B. Allal, Y. Zi, N. Muennighoff, D. Kocetkov, C. Mou, M. Marone, C. Akiki, J. Li, J. Chim <em class="ltx_emph ltx_font_italic">et al.</em>, “Starcoder: may the source be with you!” <em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.06161</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
E. Nijkamp, B. Pang, H. Hayashi, L. Tu, H. Wang, Y. Zhou, S. Savarese, and C. Xiong, “Codegen: An open large language model for code with multi-turn program synthesis,” <em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2203.13474</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
D. Fried, A. Aghajanyan, J. Lin, S. Wang, E. Wallace, F. Shi, R. Zhong, W.-t. Yih, L. Zettlemoyer, and M. Lewis, “Incoder: A generative model for code infilling and synthesis,” <em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2204.05999</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Y. Huang, Y. Bai, Z. Zhu, J. Zhang, J. Zhang, T. Su, J. Liu, C. Lv, Y. Zhang, J. Lei <em class="ltx_emph ltx_font_italic">et al.</em>, “C-eval: A multi-level multi-discipline chinese evaluation suite for foundation models,” <em class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, vol. 36, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
G. I. Winata, A. Madotto, Z. Lin, R. Liu, J. Yosinski, and P. Fung, “Language models are few-shot multilingual learners,” <em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2109.07684</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
N. Du, Y. Huang, A. M. Dai, S. Tong, D. Lepikhin, Y. Xu, M. Krikun, Y. Zhou, A. W. Yu, O. Firat <em class="ltx_emph ltx_font_italic">et al.</em>, “Glam: Efficient scaling of language models with mixture-of-experts,” <em class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>, pp. 5547–5569, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
P. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin, N. Goyal, H. Küttler, M. Lewis, W.-t. Yih, T. Rocktäschel <em class="ltx_emph ltx_font_italic">et al.</em>, “Retrieval-augmented generation for knowledge-intensive nlp tasks,” <em class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, vol. 33, pp. 9459–9474, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
M. Artetxe, S. Bhosale, N. Goyal, T. Mihaylov, M. Ott, S. Shleifer, X. V. Lin, J. Du, S. Iyer, R. Pasunuru <em class="ltx_emph ltx_font_italic">et al.</em>, “Efficient large scale language modeling with mixtures of experts,” <em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2112.10684</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Y. Zhou, T. Lei, H. Liu, N. Du, Y. Huang, V. Zhao, A. M. Dai <em class="ltx_emph ltx_font_italic">et al.</em>, “Mixture-of-experts with expert choice routing,” <em class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, vol. 35, pp. 7103–7114, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
S. R. Bowman, “Eight things to know about large language models,” <em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2304.00612</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
I. D. Raji, E. Denton, E. M. Bender, A. Hanna, and A. Paullada, “Ai and the everything in the whole wide world benchmark,” <em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2111.15366</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale <em class="ltx_emph ltx_font_italic">et al.</em>, “Llama 2: Open foundation and fine-tuned chat models,” <em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2307.09288</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
E. Almazrouei, H. Alobeidli, A. Alshamsi, A. Cappelli, R. Cojocaru, M. Debbah, É. Goffinet, D. Hesslow, J. Launay, Q. Malartic <em class="ltx_emph ltx_font_italic">et al.</em>, “The falcon series of open language models,” <em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2311.16867</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
T. Dettmers, M. Lewis, Y. Belkada, and L. Zettlemoyer, “Gpt3. int8 (): 8-bit matrix multiplication for transformers at scale,” <em class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, vol. 35, pp. 30 318–30 332, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
R. Bommasani, D. A. Hudson, E. Adeli, R. Altman, S. Arora, S. von Arx, M. S. Bernstein, J. Bohg, A. Bosselut, E. Brunskill <em class="ltx_emph ltx_font_italic">et al.</em>, “On the opportunities and risks of foundation models,” <em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2108.07258</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
P. Liang, R. Bommasani, T. Lee, D. Tsipras, D. Soylu, M. Yasunaga, Y. Zhang, D. Narayanan, Y. Wu, A. Kumar <em class="ltx_emph ltx_font_italic">et al.</em>, “Holistic evaluation of language models,” in <em class="ltx_emph ltx_font_italic">Transactions on Machine Learning Research</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
D. Card, P. Henderson, U. Khandelwal, R. Jia, K. Mahowald, and D. Jurafsky, “With little power comes great responsibility,” in <em class="ltx_emph ltx_font_italic">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</em>, 2020, pp. 9263–9274.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
O. Sainz, J. A. Campos, I. García-Ferrero, J. Etxaniz, O. L. de Lacalle, and E. Agirre, “Nlp evaluation in trouble: On the need to measure llm data contamination for each benchmark,” <em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2310.18018</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
L. Zheng, W.-L. Chiang, Y. Sheng, S. Zhuang, Z. Wu, Y. Zhuang, Z. Lin, Z. Li, D. Li, E. Xing <em class="ltx_emph ltx_font_italic">et al.</em>, “Judging llm-as-a-judge with mt-bench and chatbot arena,” <em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2306.05685</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
R. Thoppilan, D. De Freitas, J. Hall, N. Shazeer, A. Kulshreshtha, H.-T. Cheng, A. Jin, T. Bos, L. Baker, Y. Du <em class="ltx_emph ltx_font_italic">et al.</em>, “Lamda: Language models for dialog applications,” <em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2201.08239</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
S. Roller, E. Dinan, N. Goyal, D. Ju, M. Williamson, Y. Liu, J. Xu, M. Ott, E. M. Smith, Y.-L. Boureau <em class="ltx_emph ltx_font_italic">et al.</em>, “Recipes for building an open-domain chatbot,” <em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2004.13637</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
W. X. Zhao, K. Zhou, J. Li, T. Tang, X. Wang, Y. Hou, Y. Min, B. Zhang, J. Zhang, Z. Dong <em class="ltx_emph ltx_font_italic">et al.</em>, “A survey of large language models,” <em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2303.18223</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
S. Minaee, T. Mikolov, N. Nikzad, M. Chenaghlu, R. Socher, X. Amatriain, and J. Gao, “Large language models: A survey,” <em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2402.06196</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
G. Gerganov, “llama.cpp: Inference of llama model in pure c/c++,” <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/ggerganov/llama.cpp" title="">https://github.com/ggerganov/llama.cpp</a>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
D. Lepikhin, H. Lee, Y. Xu, D. Chen, O. Firat, Y. Huang, M. Krikun, N. Shazeer, and Z. Chen, “Gshard: Scaling giant models with conditional computation and automatic sharding,” <em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2006.16668</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin, “Attention is all you need,” <em class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, vol. 30, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
J. W. Rae, S. Borgeaud, T. Cai, K. Millican, J. Hoffmann, F. Song, J. Aslanides, S. Henderson, R. Ring, S. Young <em class="ltx_emph ltx_font_italic">et al.</em>, “Scaling language models: Methods, analysis &amp; insights from training gopher,” <em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2112.11446</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W. Chung, C. Sutton, S. Gehrmann <em class="ltx_emph ltx_font_italic">et al.</em>, “Palm: Scaling language modeling with pathways,” <em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2204.02311</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
Z. Chen, W. Chen, C. Smiley, S. Shah, I. Borova, D. Langdon, R. Moussa, M. Beane, T.-H. Huang, B. Routledge, and W. Y. Wang, “Finqa: A dataset of numerical reasoning over financial data,” 2022. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2109.00122" title="">https://arxiv.org/abs/2109.00122</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
X. Zhang, J. Wu, Z. He, X. Liu, and Y. Su, “Medical exam question answering with large-scale reading comprehension,” 2018. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/1802.10279" title="">https://arxiv.org/abs/1802.10279</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
J. Li, R. Bhambhoria, S. Dahan, and X. Zhu, “Experimenting with legal ai solutions: The case of question-answering for access to justice,” <em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2409.07713</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">
J. Welbl, N. F. Liu, and M. Gardner, “Crowdsourcing multiple choice science questions,” 2017. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/1707.06209" title="">https://arxiv.org/abs/1707.06209</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock">
Y. Bisk, R. Zellers, R. L. Bras, J. Gao, and Y. Choi, “Piqa: Reasoning about physical commonsense in natural language,” 2019. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/1911.11641" title="">https://arxiv.org/abs/1911.11641</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock">
Y. Chen, Y. Liu, L. Chen, and Y. Zhang, “Dialogsum: A real-life scenario dialogue summarization dataset,” 2021. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2105.06762" title="">https://arxiv.org/abs/2105.06762</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock">
J. Hu, S. Ruder, A. Siddhant, G. Neubig, O. Firat, and M. Johnson, “Xtreme: A massively multilingual multi-task benchmark for evaluating cross-lingual generalisation,” <em class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>, pp. 4411–4421, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock">
L. Xue, N. Constant, A. Roberts, M. Kale, R. Al-Rfou, A. Siddhant, A. Barua, and C. Raffel, “mt5: A massively multilingual pre-trained text-to-text transformer,” <em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2010.11934</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock">
A. Srivastava, A. Rastogi, A. Rao, A. A. M. Shoeb, A. Abid, A. Fisch, A. R. Brown, A. Santoro, A. Gupta, A. Garriga-Alonso <em class="ltx_emph ltx_font_italic">et al.</em>, “Beyond the imitation game: Quantifying and extrapolating the capabilities of language models,” <em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2206.04615</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock">
T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell <em class="ltx_emph ltx_font_italic">et al.</em>, “Language models are few-shot learners,” <em class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, vol. 33, pp. 1877–1901, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock">
J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V. Le, D. Zhou <em class="ltx_emph ltx_font_italic">et al.</em>, “Chain-of-thought prompting elicits reasoning in large language models,” <em class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, vol. 35, pp. 24 824–24 837, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock">
T. Kojima, S. S. Gu, M. Reid, Y. Matsuo, and Y. Iwasawa, “Large language models are zero-shot reasoners,” <em class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, vol. 35, pp. 22 199–22 213, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_tag_bibitem">[60]</span>
<span class="ltx_bibblock">
A. Holtzman, J. Buys, L. Du, M. Forbes, and Y. Choi, “The curious case of neural text degeneration,” <em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1904.09751</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_tag_bibitem">[61]</span>
<span class="ltx_bibblock">
S. Welleck, I. Kulikov, S. Roller, E. Dinan, K. Cho, and J. Weston, “Neural text generation with unlikelihood training,” <em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1908.04319</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_tag_bibitem">[62]</span>
<span class="ltx_bibblock">
L. Ruis, A. K. Kemfert, D. Hupkes, L. Kaiser, and B. M. Lake, “The goldilocks of pragmatic understanding: Fine-tuning strategy matters for implicature resolution by llms,” <em class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, vol. 37, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_tag_bibitem">[63]</span>
<span class="ltx_bibblock">
C. Meister, T. Pimentel, G. Wiher, and R. Cotterell, “Locally typical sampling,” <em class="ltx_emph ltx_font_italic">Transactions of the Association for Computational Linguistics</em>, vol. 11, pp. 102–121, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_tag_bibitem">[64]</span>
<span class="ltx_bibblock">
R. Dror, G. Baumer, S. Shlomov, and R. Reichart, “The hitchhiker’s guide to testing statistical significance in natural language processing,” <em class="ltx_emph ltx_font_italic">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</em>, vol. 1, pp. 1383–1392, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_tag_bibitem">[65]</span>
<span class="ltx_bibblock">
A. Benavoli, G. Corani, J. Demšar, and M. Zaffalon, “Time for a change: a tutorial for comparing multiple classifiers through bayesian analysis,” <em class="ltx_emph ltx_font_italic">The Journal of Machine Learning Research</em>, vol. 18, no. 1, pp. 2653–2688, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib66">
<span class="ltx_tag ltx_tag_bibitem">[66]</span>
<span class="ltx_bibblock">
Q. McNemar, “Note on the sampling error of the difference between correlated proportions or percentages,” <em class="ltx_emph ltx_font_italic">Psychometrika</em>, vol. 12, no. 2, pp. 153–157, 1947.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib67">
<span class="ltx_tag ltx_tag_bibitem">[67]</span>
<span class="ltx_bibblock">
C. Bonferroni, “Teoria statistica delle classi e calcolo delle probabilita,” <em class="ltx_emph ltx_font_italic">Pubblicazioni del R Istituto Superiore di Scienze Economiche e Commericiali di Firenze</em>, vol. 8, pp. 3–62, 1936.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib68">
<span class="ltx_tag ltx_tag_bibitem">[68]</span>
<span class="ltx_bibblock">
J. Cohen, <em class="ltx_emph ltx_font_italic">Statistical power analysis for the behavioral sciences</em>. Lawrence Erlbaum Associates, 1988.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib69">
<span class="ltx_tag ltx_tag_bibitem">[69]</span>
<span class="ltx_bibblock">
B. Efron and R. J. Tibshirani, <em class="ltx_emph ltx_font_italic">An introduction to the bootstrap</em>. CRC press, 1994.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib70">
<span class="ltx_tag ltx_tag_bibitem">[70]</span>
<span class="ltx_bibblock">
T. Pimentel, J. Valvoda, R. Hall Maudslay, R. Zmigrod, A. Williams, and R. Cotterell, “Information-theoretic probing for linguistic structure,” <em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2004.03061</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib71">
<span class="ltx_tag ltx_tag_bibitem">[71]</span>
<span class="ltx_bibblock">
D. Deutsch, R. Dror, and D. Roth, “A statistical analysis of summarization evaluation metrics using resampling methods,” <em class="ltx_emph ltx_font_italic">Transactions of the Association for Computational Linguistics</em>, vol. 9, pp. 1132–1146, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib72">
<span class="ltx_tag ltx_tag_bibitem">[72]</span>
<span class="ltx_bibblock">
P. Koehn, “Statistical significance tests for machine translation evaluation,” in <em class="ltx_emph ltx_font_italic">Proceedings of the 2004 conference on empirical methods in natural language processing</em>, 2004, pp. 388–395.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib73">
<span class="ltx_tag ltx_tag_bibitem">[73]</span>
<span class="ltx_bibblock">
Y. Benjamini and Y. Hochberg, “Controlling the false discovery rate: a practical and powerful approach to multiple testing,” <em class="ltx_emph ltx_font_italic">Journal of the Royal statistical society: series B (Methodological)</em>, vol. 57, no. 1, pp. 289–300, 1995.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib74">
<span class="ltx_tag ltx_tag_bibitem">[74]</span>
<span class="ltx_bibblock">
J. Pineau, P. Vincent-Lamarre, K. Sinha, V. Larivière, A. Beygelzimer, F. d’Alché Buc, E. Fox, and H. Larochelle, “Improving reproducibility in machine learning research: a report from the neurips 2019 reproducibility program,” <em class="ltx_emph ltx_font_italic">The Journal of Machine Learning Research</em>, vol. 22, no. 1, pp. 7459–7478, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib75">
<span class="ltx_tag ltx_tag_bibitem">[75]</span>
<span class="ltx_bibblock">
J. Dodge, S. Gururangan, D. Card, R. Schwartz, and N. A. Smith, “Show your work: Improved reporting of experimental results,” in <em class="ltx_emph ltx_font_italic">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</em>, 2019, pp. 2185–2194.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib76">
<span class="ltx_tag ltx_tag_bibitem">[76]</span>
<span class="ltx_bibblock">
I. Magar and R. Schwartz, “Data contamination: From memorization to exploitation,” <em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2203.08242</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib77">
<span class="ltx_tag ltx_tag_bibitem">[77]</span>
<span class="ltx_bibblock">
J. Dekoninck, M. Whitehouse, M. Suppanen, S. Cullen, M. Kuchnik, and J. Mitchell, “Evading data contamination detection: Exploring test-time preprocessing methods,” <em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2411.02643</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib78">
<span class="ltx_tag ltx_tag_bibitem">[78]</span>
<span class="ltx_bibblock">
M. Dehghani, Y. Tay, A. A. Gritsenko, Z. Zhao, N. Houlsby, F. Diaz, D. Metzler, and O. Vinyals, “The benchmark lottery,” <em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2107.07002</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib79">
<span class="ltx_tag ltx_tag_bibitem">[79]</span>
<span class="ltx_bibblock">
E. M. Bender, T. Gebru, A. McMillan-Major, and S. Shmitchell, “On the dangers of stochastic parrots: Can language models be too big?” <em class="ltx_emph ltx_font_italic">Proceedings of the 2021 ACM conference on fairness, accountability, and transparency</em>, pp. 610–623, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib80">
<span class="ltx_tag ltx_tag_bibitem">[80]</span>
<span class="ltx_bibblock">
A. Birhane, V. U. Prabhu, and E. Kahembwe, “Multimodal datasets: misogyny, pornography, and malignant stereotypes,” <em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2110.01963</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib81">
<span class="ltx_tag ltx_tag_bibitem">[81]</span>
<span class="ltx_bibblock">
E. Strubell, A. Ganesh, and A. McCallum, “Energy and policy considerations for deep learning in nlp,” <em class="ltx_emph ltx_font_italic">Proceedings of the 57th annual meeting of the association for computational linguistics</em>, pp. 3645–3650, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib82">
<span class="ltx_tag ltx_tag_bibitem">[82]</span>
<span class="ltx_bibblock">
D. Patterson, J. Gonzalez, Q. Le, C. Liang, L.-M. Munguia, D. Rothchild, D. So, M. Texier, and J. Dean, “Carbon emissions and large neural network training,” <em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2104.10350</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib83">
<span class="ltx_tag ltx_tag_bibitem">[83]</span>
<span class="ltx_bibblock">
P. Wang, L. Li, L. Chen, Z. Cai, D. Zhu, B. Lin, Y. Cao, Q. Liu, T. Liu, and Z. Sui, “Large language models are not fair evaluators,” <em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.17926</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib84">
<span class="ltx_tag ltx_tag_bibitem">[84]</span>
<span class="ltx_bibblock">
J. Wei, N. Hou, A. Lampinen, X. Chen, D. Huang, Y. Tay, X. Chen, Y. Lu, D. Zhou, T. Ma <em class="ltx_emph ltx_font_italic">et al.</em>, “Inverse scaling can become u-shaped,” <em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2211.02011</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib85">
<span class="ltx_tag ltx_tag_bibitem">[85]</span>
<span class="ltx_bibblock">
I. R. McKenzie, A. Lyzhov, M. Pieler, A. Parrish, A. Mueller, A. Prabhu, E. McLean, A. Kirtland, A. Ross, A. Liu <em class="ltx_emph ltx_font_italic">et al.</em>, “Inverse scaling: When bigger isn’t better,” <em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2306.09479</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib86">
<span class="ltx_tag ltx_tag_bibitem">[86]</span>
<span class="ltx_bibblock">
D. Ganguli, D. Hernandez, L. Lovitt, N. DasSarma, T. Henighan, A. Jones, N. Joseph, J. Kernion, B. Mann, A. Askell <em class="ltx_emph ltx_font_italic">et al.</em>, “Predictability and surprise in large generative models,” <em class="ltx_emph ltx_font_italic">Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency</em>, pp. 1747–1764, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib87">
<span class="ltx_tag ltx_tag_bibitem">[87]</span>
<span class="ltx_bibblock">
A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot, D. d. l. Casas, F. Bressand, G. Lengyel, G. Lample, L. Saulnier <em class="ltx_emph ltx_font_italic">et al.</em>, “Mistral 7b,” <em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2310.06825</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib88">
<span class="ltx_tag ltx_tag_bibitem">[88]</span>
<span class="ltx_bibblock">
A. Roberts, C. Raffel, and N. Shazeer, “How much knowledge can you pack into the parameters of a language model?” <em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2002.08910</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib89">
<span class="ltx_tag ltx_tag_bibitem">[89]</span>
<span class="ltx_bibblock">
N. Kandpal, H. Deng, A. Roberts, E. Wallace, and C. Raffel, “Large language models struggle to learn long-tail knowledge,” <em class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>, pp. 15 696–15 707, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib90">
<span class="ltx_tag ltx_tag_bibitem">[90]</span>
<span class="ltx_bibblock">
X. Wang, J. Wei, D. Schuurmans, Q. Le, E. Chi, S. Narang, A. Chowdhery, and D. Zhou, “Self-consistency improves chain of thought reasoning in language models,” <em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2203.11171</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib91">
<span class="ltx_tag ltx_tag_bibitem">[91]</span>
<span class="ltx_bibblock">
R. Burnell, W. Schellaert, J. Burden, T. D. Ullman, F. Martinez-Plumed, J. B. Tenenbaum, D. Rutar, L. G. Cheke, J. Sohl-Dickstein, M. Mitchell <em class="ltx_emph ltx_font_italic">et al.</em>, “Rethink reporting of evaluation results in ai,” <em class="ltx_emph ltx_font_italic">Science</em>, vol. 380, no. 6641, pp. 136–138, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib92">
<span class="ltx_tag ltx_tag_bibitem">[92]</span>
<span class="ltx_bibblock">
N. Dziri, X. Lu, M. Sclar, X. L. Li, L. Jian, B. Y. Lin, P. West, C. Bhagavatula, R. Le Bras, J. D. Hwang <em class="ltx_emph ltx_font_italic">et al.</em>, “Faith and fate: Limits of transformers on compositionality,” <em class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, vol. 36, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib93">
<span class="ltx_tag ltx_tag_bibitem">[93]</span>
<span class="ltx_bibblock">
Y. Razeghi, R. L. Logan IV, M. Gardner, and S. Singh, “Impact of pretraining term frequencies on few-shot numerical reasoning,” <em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2202.07206</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib94">
<span class="ltx_tag ltx_tag_bibitem">[94]</span>
<span class="ltx_bibblock">
F. F. Xu, U. Alon, G. Neubig, and V. J. Hellendoorn, “A systematic evaluation of large language models of code,” <em class="ltx_emph ltx_font_italic">Proceedings of the 6th ACM SIGPLAN International Symposium on Machine Programming</em>, pp. 1–10, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib95">
<span class="ltx_tag ltx_tag_bibitem">[95]</span>
<span class="ltx_bibblock">
Z. Zhang, C. Shen, H. Zhu, and Z. Li, “Unifying the perspectives of nlp and software engineering: A survey on language models for code,” <em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2311.07989</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib96">
<span class="ltx_tag ltx_tag_bibitem">[96]</span>
<span class="ltx_bibblock">
A. See, S. Roller, D. Kiela, and J. Weston, “What makes a good conversation? how controllable attributes affect human judgments,” <em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1902.08654</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib97">
<span class="ltx_tag ltx_tag_bibitem">[97]</span>
<span class="ltx_bibblock">
Y. Tay, M. Dehghani, J. Rao, W. Fedus, S. Abnar, H. W. Chung, S. Narang, D. Yogatama, A. Vaswani, and D. Metzler, “Scale efficiently: Insights from pretraining and finetuning transformers,” <em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2109.10686</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib98">
<span class="ltx_tag ltx_tag_bibitem">[98]</span>
<span class="ltx_bibblock">
P. Henderson, J. Hu, J. Romoff, E. Brunskill, D. Jurafsky, and J. Pineau, “Towards the systematic reporting of the energy and carbon footprints of machine learning,” <em class="ltx_emph ltx_font_italic">The Journal of Machine Learning Research</em>, vol. 21, no. 1, pp. 10 039–10 081, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib99">
<span class="ltx_tag ltx_tag_bibitem">[99]</span>
<span class="ltx_bibblock">
R. Schwartz, J. Dodge, N. A. Smith, and O. Etzioni, “Green ai,” <em class="ltx_emph ltx_font_italic">Communications of the ACM</em>, vol. 63, no. 12, pp. 54–63, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib100">
<span class="ltx_tag ltx_tag_bibitem">[100]</span>
<span class="ltx_bibblock">
S. S. Sawilowsky, “New effect size rules of thumb,” <em class="ltx_emph ltx_font_italic">Journal of modern applied statistical methods</em>, vol. 8, no. 2, p. 26, 2009.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib101">
<span class="ltx_tag ltx_tag_bibitem">[101]</span>
<span class="ltx_bibblock">
J. R. Landis and G. G. Koch, “The measurement of observer agreement for categorical data,” <em class="ltx_emph ltx_font_italic">biometrics</em>, pp. 159–174, 1977.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib102">
<span class="ltx_tag ltx_tag_bibitem">[102]</span>
<span class="ltx_bibblock">
D. Ulmer, C. Hardmeier, and J. Frellsen, “Prior and posterior networks: A survey on evidential deep learning methods for uncertainty estimation,” <em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2110.03051</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib103">
<span class="ltx_tag ltx_tag_bibitem">[103]</span>
<span class="ltx_bibblock">
R. Pope, S. Douglas, A. Chowdhery, J. Devlin, J. Bradbury, J. Heek, K. Xiao, S. Agrawal, and J. Dean, “Efficiently scaling transformer inference,” <em class="ltx_emph ltx_font_italic">Proceedings of Machine Learning and Systems</em>, vol. 5, 2023.

</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Sun Aug 17 18:24:02 2025 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
