<!DOCTYPE html><!--MYXrWUjg0HELsl6zLYF_6--><html lang="en-us" class="__variable_dd5b2f scroll-smooth"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" href="/_next/static/media/36966cca54120369-s.p.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="stylesheet" href="/_next/static/css/4ac7c0f872b74ee7.css" data-precedence="next"/><link rel="stylesheet" href="/_next/static/css/a06d7e24bf9a7d93.css" data-precedence="next"/><link rel="stylesheet" href="/_next/static/css/7246298b30c42979.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/webpack-91cf1b3b084c6ba2.js"/><script src="/_next/static/chunks/4bd1b696-cf72ae8a39fa05aa.js" async=""></script><script src="/_next/static/chunks/964-dd2321b4b492b566.js" async=""></script><script src="/_next/static/chunks/main-app-01d32ba99375ba1b.js" async=""></script><script src="/_next/static/chunks/874-8edb22cc7428423c.js" async=""></script><script src="/_next/static/chunks/650-5bb235cd9bfca45d.js" async=""></script><script src="/_next/static/chunks/app/layout-66df3ac28fcfbced.js" async=""></script><script src="/_next/static/chunks/63-d245e42a784ca56d.js" async=""></script><script src="/_next/static/chunks/app/blog/%5B...slug%5D/page-1ed2702378ba9d6b.js" async=""></script><link rel="preload" href="https://analytics.umami.is/script.js" as="script"/><link rel="apple-touch-icon" sizes="76x76" href="/static/favicons/apple-touch-icon.png"/><link rel="icon" type="image/svg+xml" href="/static/favicons/favicon.svg"/><link rel="icon" type="image/png" sizes="32x32" href="/static/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="16x16" href="/static/favicons/favicon-16x16.png"/><link rel="manifest" href="/static/favicons/site.webmanifest"/><link rel="mask-icon" href="/static/favicons/safari-pinned-tab.svg" color="#5bbad5"/><meta name="msapplication-TileColor" content="#000000"/><meta name="theme-color" media="(prefers-color-scheme: light)" content="#fff"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#000"/><link rel="alternate" type="application/rss+xml" href="/feed.xml"/><meta name="next-size-adjust" content=""/><title>The Accelerator Toolkit: A Review of Profiling and Tracing for GPUs and other co-processor | 云微的胡思乱想</title><meta name="robots" content="index, follow"/><meta name="googlebot" content="index, follow, max-video-preview:-1, max-image-preview:large, max-snippet:-1"/><link rel="canonical" href="https://www.yunwei37.com/blog/gpu-profile-tools-analysis"/><link rel="alternate" type="application/rss+xml" href="https://www.yunwei37.com/feed.xml"/><meta property="og:title" content="The Accelerator Toolkit: A Review of Profiling and Tracing for GPUs and other co-processor"/><meta property="og:url" content="https://www.yunwei37.com/blog/gpu-profile-tools-analysis"/><meta property="og:site_name" content="云微的胡思乱想"/><meta property="og:locale" content="en_US"/><meta property="og:image" content="https://www.yunwei37.com/static/images/twitter-card.png"/><meta property="og:type" content="article"/><meta property="article:published_time" content="2025-04-11T16:00:00.000Z"/><meta property="article:modified_time" content="2025-04-11T16:00:00.000Z"/><meta property="article:author" content="Yusheng Zheng (云微)"/><meta name="twitter:card" content="summary_large_image"/><meta name="twitter:title" content="The Accelerator Toolkit: A Review of Profiling and Tracing for GPUs and other co-processor"/><meta name="twitter:image" content="https://www.yunwei37.com/static/images/twitter-card.png"/><script src="/_next/static/chunks/polyfills-42372ed130431b0a.js" noModule=""></script></head><body class="pl-[calc(100vw-100%)] text-black antialiased dark:text-white min-h-screen"><div hidden=""><!--$--><!--/$--></div><script>((a,b,c,d,e,f,g,h)=>{let i=document.documentElement,j=["light","dark"];function k(b){var c;(Array.isArray(a)?a:[a]).forEach(a=>{let c="class"===a,d=c&&f?e.map(a=>f[a]||a):e;c?(i.classList.remove(...d),i.classList.add(f&&f[b]?f[b]:b)):i.setAttribute(a,b)}),c=b,h&&j.includes(c)&&(i.style.colorScheme=c)}if(d)k(d);else try{let a=localStorage.getItem(b)||c,d=g&&"system"===a?window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light":a;k(d)}catch(a){}})("class","theme","system",null,["light","dark"],null,true,true)</script><div class="min-h-screen flex flex-col justify-center py-8"><div class="content-glass mx-auto w-full max-w-5xl px-4 sm:px-6 xl:px-8 py-8"><header class="flex items-center w-full justify-between py-6 border-b border-gray-200/30 dark:border-gray-700/30"><a class="break-words" aria-label="yunwei37" href="/"><div class="flex items-center"><div class="mr-3"><svg xmlns="http://www.w3.org/2000/svg" width="40" height="40" fill="none"><defs><linearGradient id="logo_svg__a" x1="0%" x2="100%" y1="0%" y2="100%"><stop offset="0%" style="stop-color:#3b82f6;stop-opacity:1"></stop><stop offset="100%" style="stop-color:#06b6d4;stop-opacity:1"></stop></linearGradient><linearGradient id="logo_svg__b" x1="0%" x2="100%" y1="0%" y2="100%"><stop offset="0%" style="stop-color:#8b5cf6;stop-opacity:1"></stop><stop offset="100%" style="stop-color:#3b82f6;stop-opacity:1"></stop></linearGradient></defs><circle cx="20" cy="20" r="18" fill="url(#logo_svg__a)" opacity="0.1"></circle><path fill="url(#logo_svg__a)" d="m12 8 6 10v10h4V18l6-10h-4l-4 6-4-6Z"></path><path fill="url(#logo_svg__b)" d="m8 24 4 8 4-8 4 8 4-8h8v4h-6l-4 8-4-8-4 8-4-8H8Z" opacity="0.8"></path><circle cx="32" cy="12" r="2" fill="url(#logo_svg__b)"></circle><circle cx="8" cy="12" r="1.5" fill="url(#logo_svg__a)" opacity="0.6"></circle></svg></div><div class="hidden text-2xl font-bold sm:block hover:text-primary-600 dark:hover:text-primary-400 transition-colors">yunwei37</div></div></a><div class="flex items-center space-x-4"><div class="no-scrollbar hidden items-center space-x-2 sm:flex"><a class="px-3 py-2 rounded-lg transition-all duration-200 font-medium hover:text-primary-600 dark:hover:text-primary-400" href="/blog">Blog</a><a class="px-3 py-2 rounded-lg transition-all duration-200 font-medium hover:text-primary-600 dark:hover:text-primary-400" href="/docs">Docs</a><a class="px-3 py-2 rounded-lg transition-all duration-200 font-medium hover:text-primary-600 dark:hover:text-primary-400" href="/tags">Tags</a><a class="px-3 py-2 rounded-lg transition-all duration-200 font-medium hover:text-primary-600 dark:hover:text-primary-400" href="/about">About</a></div><div class="flex items-center space-x-2"><button aria-label="Search" class="p-2 rounded-lg transition-all duration-200"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" class="h-6 w-6 hover:text-primary-600 dark:hover:text-primary-400 transition-colors"><path stroke-linecap="round" stroke-linejoin="round" d="M21 21l-5.197-5.197m0 0A7.5 7.5 0 105.196 5.196a7.5 7.5 0 0010.607 10.607z"></path></svg></button><button aria-label="Toggle Menu" class="sm:hidden p-2 rounded-lg hover:glass transition-all duration-200 hover:scale-105"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" fill="currentColor" class="h-6 w-6 hover:text-primary-600 dark:hover:text-primary-400 transition-colors"><path fill-rule="evenodd" d="M3 5a1 1 0 011-1h12a1 1 0 110 2H4a1 1 0 01-1-1zM3 10a1 1 0 011-1h12a1 1 0 110 2H4a1 1 0 01-1-1zM3 15a1 1 0 011-1h12a1 1 0 110 2H4a1 1 0 01-1-1z" clip-rule="evenodd"></path></svg></button><span hidden="" style="position:fixed;top:1px;left:1px;width:1px;height:0;padding:0;margin:-1px;overflow:hidden;clip:rect(0, 0, 0, 0);white-space:nowrap;border-width:0;display:none"></span></div></div></header><main class="py-6"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","headline":"The Accelerator Toolkit: A Review of Profiling and Tracing for GPUs and other co-processor","datePublished":"2025-04-11T16:00:00.000Z","dateModified":"2025-04-11T16:00:00.000Z","image":"/static/images/twitter-card.png","url":"https://www.yunwei37.com/blog/gpu-profile-tools-analysis","author":[{"@type":"Person","name":"Yusheng Zheng (云微)"}]}</script><section class="mx-auto max-w-3xl px-4 sm:px-6 xl:max-w-5xl xl:px-0"><div class="fixed right-8 bottom-8 hidden flex-col gap-3 md:hidden"><button aria-label="Scroll To Comment" class="rounded-full bg-white/20 dark:bg-black/20 p-2 transition-all hover:bg-white/30 dark:hover:bg-black/30"><svg class="h-5 w-5" viewBox="0 0 20 20" fill="currentColor"><path fill-rule="evenodd" d="M18 10c0 3.866-3.582 7-8 7a8.841 8.841 0 01-4.083-.98L2 17l1.338-3.123C2.493 12.767 2 11.434 2 10c0-3.866 3.582-7 8-7s8 3.134 8 7zM7 9H5v2h2V9zm8 0h-2v2h2V9zM9 9h2v2H9V9z" clip-rule="evenodd"></path></svg></button><button aria-label="Scroll To Top" class="rounded-full bg-white/20 dark:bg-black/20 p-2 transition-all hover:bg-white/30 dark:hover:bg-black/30"><svg class="h-5 w-5" viewBox="0 0 20 20" fill="currentColor"><path fill-rule="evenodd" d="M3.293 9.707a1 1 0 010-1.414l6-6a1 1 0 011.414 0l6 6a1 1 0 01-1.414 1.414L11 5.414V17a1 1 0 11-2 0V5.414L4.707 9.707a1 1 0 01-1.414 0z" clip-rule="evenodd"></path></svg></button></div><article class="content-glass p-8"><div class="xl:divide-y xl:divide-gray-200/30 xl:dark:divide-gray-700/30"><header class="pt-6 xl:pb-6"><div class="space-y-4 text-center"><dl class="space-y-4"><div><dt class="sr-only">Published on</dt><dd class="text-base leading-6 font-medium text-gray-600 dark:text-gray-300"><time dateTime="2025-04-11T16:00:00.000Z">Friday, April 11, 2025</time></dd></div></dl><div><h1 class="text-3xl leading-9 font-extrabold tracking-tight sm:text-4xl sm:leading-10 md:text-5xl md:leading-14">The Accelerator Toolkit: A Review of Profiling and Tracing for GPUs and other co-processor</h1></div></div></header><div class="grid-rows-[auto_1fr] divide-y divide-gray-200/30 pb-8 xl:grid xl:grid-cols-4 xl:gap-x-8 xl:divide-y-0 dark:divide-gray-700/30"><dl class="pt-6 pb-10 xl:border-b xl:border-gray-200/30 xl:pt-11 xl:dark:border-gray-700/30"><dt class="sr-only">Authors</dt><dd><ul class="flex flex-wrap justify-center gap-4 sm:space-x-12 xl:block xl:space-y-6 xl:space-x-0"><li class="flex items-center space-x-3 glass p-4 rounded-xl"><img alt="avatar" loading="lazy" width="38" height="38" decoding="async" data-nimg="1" class="h-10 w-10 rounded-full" style="color:transparent" src="/static/images/avatar.png"/><dl class="text-sm leading-5 font-medium whitespace-nowrap"><dt class="sr-only">Name</dt><dd class="text-gray-900 dark:text-gray-100">Yusheng Zheng (云微)</dd><dt class="sr-only">Twitter</dt><dd><a class="text-primary-600 hover:text-primary-700 dark:text-primary-400 dark:hover:text-primary-300 transition-colors" target="_blank" rel="noopener noreferrer" href="https://twitter.com/yunwei37">@yunwei37</a></dd></dl></li></ul></dd></dl><div class="divide-y divide-gray-200/30 xl:col-span-3 xl:row-span-2 xl:pb-0 dark:divide-gray-700/30"><div class="prose dark:prose-invert max-w-none pt-10 pb-8 text-gray-700 dark:text-gray-200"><h1 class="content-header" id="the-accelerator-toolkit-a-review-of-profiling-and-tracing-for-gpus-and-other-co-processor"><a class="break-words" href="#the-accelerator-toolkit-a-review-of-profiling-and-tracing-for-gpus-and-other-co-processor" aria-hidden="true" tabindex="-1"><span class="content-header-link"><svg class="h-5 linkicon w-5" fill="currentColor" viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg"><path d="M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z"></path><path d="M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z"></path></svg></span></a>The Accelerator Toolkit: A Review of Profiling and Tracing for GPUs and other co-processor</h1><p>Modern computing increasingly relies on specialized accelerators – notably <strong>GPUs</strong>, <strong>DPUs</strong>, and <strong>APUs</strong> – to handle diverse workloads. A <em>graphics processing unit (GPU)</em> is a massively parallel processor originally for graphics, now essential in general-purpose computing (HPC) and AI. A <em>data processing unit (DPU)</em> is a newer class of programmable processor combining CPU cores with high-performance network/storage engines. DPUs offload networking, security, and storage tasks from the CPU, and are considered the &quot;third pillar&quot; of computing alongside CPUs and GPUs. Meanwhile, <em>accelerated processing units (APUs)</em> integrate CPU and GPU components on one chip – an approach pioneered by AMD&#x27;s Fusion architecture – enabling unified memory and high throughput for HPC and AI workloads. These accelerators run a range of workloads: GPUs excel in parallel math (HPC simulation, deep learning training/inference, data analytics) and rendering graphics; DPUs focus on data-centric tasks (network packet processing, encryption, storage offload, virtualization); and APUs target heterogeneous workloads needing tight CPU-GPU coupling (e.g. sharing memory for AI or multimedia applications).</p><p><strong>Profiling and tracing</strong> tools are crucial for optimizing performance on these accelerators. Such tools collect <strong>low-level hardware telemetry</strong> (e.g. counters for utilization, memory throughput, SM occupancy, cache misses) and can perform <strong>instruction or event-level tracing</strong> (capturing timelines of kernel executions, memory copies, network packet flows, etc.). The goal is to identify bottlenecks and inefficiencies in both <em>general-purpose</em> code and <em>domain-specific</em> pipelines (like ML model training, network function processing, or graphics rendering). However, profiling highly parallel, heterogeneous systems presents challenges of overhead, data volume, and cross-platform compatibility. This review categorizes current tracing/profiling tools by hardware type and domain, compares open-source and commercial solutions, and highlights major projects and recent research. We also discuss specialized toolsets (including eBPF-based approaches akin to Linux&#x27;s BCC) adapted for GPUs/DPUs/APUs, typical workloads and matching toolchains, and the limitations and emerging directions in this landscape.</p><h2 class="content-header" id="profiling-and-tracing-tools-for-gpus"><a class="break-words" href="#profiling-and-tracing-tools-for-gpus" aria-hidden="true" tabindex="-1"><span class="content-header-link"><svg class="h-5 linkicon w-5" fill="currentColor" viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg"><path d="M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z"></path><path d="M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z"></path></svg></span></a>Profiling and Tracing Tools for GPUs</h2><p>GPU profiling has matured over years of graphics and GPGPU development, yielding a rich ecosystem of vendor tools and open-source frameworks. Broadly, GPU profilers fall into two categories: <strong>development-time profilers</strong> that provide fine-grained insight for optimization (often with high overhead and GUI analysis), and <strong>lightweight monitors</strong> suitable for runtime or production use (low overhead, focusing on high-level metrics).</p><ul><li><p><strong>NVIDIA&#x27;s Profiling Suite:</strong> NVIDIA&#x27;s GPUs dominate in HPC/AI, and their tools are considered industry-standard. <em>Nsight Systems</em> provides system-wide timeline tracing – capturing CPU threads, CUDA kernel launches, memory copies, etc. – to pinpoint bottlenecks across the CPU-GPU boundary. It uses the CUDA Profiling Tools Interface (CUPTI) to gather detailed metrics and events. Nsight Systems is extremely powerful for deep analysis of GPU-accelerated applications, but it must explicitly start a profiling session and can incur significant overhead (often slowing programs by <strong>2×–10×</strong> during profiling). It&#x27;s intended for development and tuning rather than continuous use. <em>Nsight Compute</em>, on the other hand, focuses on per-kernel deep dives: it profiles individual GPU kernels, providing instruction-level stats (e.g. instruction mix, memory transactions, warp occupancy, execution stalls) and can even associate performance metrics with source lines or PTX/SASS assembly. NVIDIA also offers <em>Visual Profiler</em> (an older GUI, now largely replaced by Nsight) and command-line profiling via <code>nvprof</code>/<code>nsys</code>. For graphics workloads (DirectX, OpenGL, Vulkan), NVIDIA&#x27;s <strong>Nsight Graphics</strong> captures frame renderer pipelines, shader timings, and GPU state to assist game developers. All these NVIDIA tools are free but proprietary (closed-source). They are tightly optimized for NVIDIA hardware and support domain-specific analysis modes (graphics vs. compute vs. AI).</p></li><li><p><strong>AMD&#x27;s GPU Profiling Tools:</strong> AMD provides both open-source and proprietary tools as part of the ROCm and GPUOpen ecosystems. Historically, <em>CodeXL</em> was AMD&#x27;s all-in-one profiler and debugger for CPUs, GPUs, and APUs. CodeXL (now discontinued) could profile OpenCL, HIP, and HSA applications on AMD APUs/GPUs, collecting kernel execution times and hardware counters on integrated devices. In recent years, AMD shifted to ROCm-based tooling: <strong>rocProfiler</strong> and <strong>rocTracer</strong> libraries (analogous to NVIDIA&#x27;s CUPTI) enable profiling and tracing of HIP and OpenCL applications. In 2024, AMD introduced new tools in ROCm 6.2: <em>Omniperf</em> and <em>Omnitrace</em>. <strong>Omniperf</strong> is a kernel-level profiler for machine learning and HPC workloads on AMD Instinct GPUs, offering detailed performance counters analysis via CLI or a GUI dashboard. <strong>Omnitrace</strong> is a <em>multi-purpose profiling/tracing tool</em> for CPU <strong>and</strong> GPU, supporting dynamic binary instrumentation, call-stack sampling, and even causal profiling to pinpoint which functions consume time on heterogeneous CPU-GPU executions. These tools are open-source or part of AMD&#x27;s open ROCm stack. For graphics and game development, AMD provides <strong>Radeon GPU Profiler (RGP)</strong> and <strong>Radeon Developer Panel</strong> under GPUOpen. RGP offers low-level timeline and wavefront occupancy data for Vulkan/DX12 applications on Radeon GPUs, while Radeon Memory Visualizer helps track memory usage. AMD&#x27;s tools can be used on their APUs as well, benefiting from unified memory (e.g., profiling an APU means GPU kernels can be traced without PCIe transfer overhead). Overall, AMD&#x27;s approach emphasizes open interfaces (e.g., the <em>GPUPerfAPI</em> library gives developers direct access to GPU performance counters) and integration with generic profilers.</p></li><li><p><strong>Intel and Other GPU Tools:</strong> Intel&#x27;s GPUs (integrated Iris Xe and data-center GPUs like Ponte Vecchio) can be profiled by Intel&#x27;s oneAPI toolset. <strong>Intel VTune Profiler</strong> supports GPU offload analysis, providing kernel timelines, EU (execution unit) occupancy, and memory bandwidth for Intel GPUs. Intel also offers <strong>Graphics Performance Analyzers (GPA)</strong> for game/graphics profiling on Intel hardware. Notably, Intel has open-sourced a suite called <em>Profiling Tools Interface (PTI) for GPU</em>, which includes lightweight tracing tools for oneAPI Level Zero and OpenCL applications. These command-line tools (available on GitHub) can trace GPU kernel submissions, memory operations, etc., on Intel GPUs, reflecting Intel&#x27;s push for an open profiling ecosystem. Beyond the big three vendors, there are domain-specific GPU profilers: e.g., ARM&#x27;s <em>Mali</em> GPUs (for mobile) have <strong>ARM Mobile Studio</strong> with tools like Streamline for profiling mobile GPU workloads; Qualcomm Adreno GPUs can be analyzed with Qualcomm&#x27;s Snapdragon Profiler. These are more specialized but underscore that across vendors, profiling often requires <em>proprietary SDKs or tools</em> unique to each architecture, with little standardization – a pain point if one needs to support multiple GPU vendors.</p></li><li><p><strong>HPC and Cross-Platform Profiling:</strong> Outside vendor-specific utilities, the HPC community has developed powerful <strong>open-source profiling frameworks</strong> that work across CPUs and accelerators. <strong>HPCToolkit</strong> is a prominent example: it uses statistical sampling to profile both CPU and GPU execution with minimal overhead (often <code>&lt;5%</code>). HPCToolkit can trace GPU operations (kernels, memcopies, sync) on NVIDIA, AMD, and Intel GPUs, and on NVIDIA it even leverages hardware PC sampling to measure instruction-level execution and stall cycles. The tool correlates GPU activity back to CPU call stacks, allowing a <em>unified profile</em> attributing GPU costs to the calling CPU code context. This is invaluable for <em>heterogeneous applications</em>, e.g. identifying which CPU-side function launched a slow GPU kernel. Other cross-platform tools include <strong>TAU</strong> and <strong>Score-P/Extrae</strong>, which can instrument MPI+GPU programs and produce integrated traces. For instance, the Extrae tracer (from BSC) records CPU events and CUDA runtime events, enabling visualization in Paraver of both CPU timeline and GPU kernel timelines. These OSS tools typically support multiple accelerators via plugins (CUPTI for CUDA, ROCm tools for AMD, etc.), providing vendor-neutral analysis. They may not always expose the full depth of vendor tools&#x27; metrics, but they excel in <em>coordinating multi-node, multi-accelerator traces</em>. Academic efforts like <strong>Paraver/Extrae, Vampir, and Allinea MAP (Arm Forge)</strong> have evolved to handle GPU-accelerated HPC codes, indicating a trend toward unified performance analysis for heterogeneous systems.</p></li><li><p><strong>Graphics and Game Profiling:</strong> In graphics domains, <strong>tracing GPU workloads</strong> often involves capturing API calls and GPU command streams. Open-source projects like <strong>RenderDoc</strong> allow frame-by-frame capture of Vulkan/OpenGL/Direct3D calls, with introspection of GPU draw call timing (helpful for graphics debugging/performance). While RenderDoc is more of a debugger, it can give insights on whether the GPU is bound by certain draw calls or shaders. Platform-specific tools exist too: Microsoft&#x27;s <strong>PIX</strong> (for DirectX on Windows/Xbox) provides detailed GPU timing for each render pass, and even Linux has tools like <strong>GPUView</strong> that trace kernel-level GPU scheduling events for graphics workloads. These are highly domain-specific (targeting graphics pipelines rather than general compute). They complement general GPU profilers by focusing on frame rendering latency, vs. kernel throughput.</p></li></ul><p><strong>Table 1</strong> below summarizes a sample of representative GPU profiling/tracing tools across vendors and domains, highlighting their availability and focus:</p><div class="w-full overflow-x-auto"><table><thead><tr><th><strong>Tool</strong></th><th><strong>Vendor / Origin</strong></th><th><strong>Open-Source?</strong></th><th><strong>Primary Focus</strong></th><th><strong>Supported Domain</strong></th></tr></thead><tbody><tr><td><strong>Nsight Systems</strong></td><td>NVIDIA</td><td>No (free)</td><td>Timeline tracing (CPU &amp; GPU), deep metrics</td><td>CUDA Compute, AI, some Graphics</td></tr><tr><td><strong>Nsight Compute</strong></td><td>NVIDIA</td><td>No (free)</td><td>Kernel microarchitecture profiling</td><td>CUDA/HPC/AI (per-kernel)</td></tr><tr><td><strong>NVIDIA PerfKit/DCGM</strong></td><td>NVIDIA</td><td>No (free)</td><td>Low-level HW counters (PerfKit); Datacenter GPU monitoring (DCGM)</td><td>System GPU Telemetry</td></tr><tr><td><strong>Radeon GPU Profiler</strong></td><td>AMD (GPUOpen)</td><td><strong>Yes</strong> (free)</td><td>Low-level GPU trace (wavefront, ISA)</td><td>Graphics (Vulkan/DX12), GPGPU</td></tr><tr><td><strong>ROCm Omniperf</strong></td><td>AMD ROCm (Instinct MI GPUs)</td><td><strong>Yes</strong></td><td>Kernel profiling (counters &amp; analysis)</td><td>HPC/AI Compute</td></tr><tr><td><strong>ROCm Omnitrace</strong></td><td>AMD ROCm</td><td><strong>Yes</strong></td><td>CPU-GPU tracing, call-stack profiling</td><td>HPC/AI, Heterogeneous apps</td></tr><tr><td><strong>Intel VTune &amp; GPA</strong></td><td>Intel oneAPI</td><td>Partial (VTune closed, PTI open)</td><td>VTune: GPU offload analysis; GPA: frame analysis</td><td>Compute (oneAPI) &amp; Graphics</td></tr><tr><td><strong>HPCToolkit</strong></td><td>Rice Univ. (HPC tool)</td><td><strong>Yes</strong></td><td>Sampling-based profiling (CPU &amp; GPU)</td><td>HPC/AI (CPU+GPU)</td></tr><tr><td><strong>RenderDoc</strong></td><td>Community / Baldur Karlsson</td><td><strong>Yes</strong></td><td>Frame capture &amp; API trace</td><td>Graphics debugging</td></tr><tr><td><strong>PyTorch Kineto</strong></td><td>FB/Intel (via PyTorch)</td><td><strong>Yes</strong></td><td>In-framework profiler (CPU+GPU)</td><td>AI/ML (Deep Learning)</td></tr></tbody></table></div><p><strong>Sources:</strong> GPU vendor documentation and tool websites.</p><p>As the table suggests, <strong>open-source solutions</strong> are prominent in research and HPC (e.g. HPCToolkit, Omniperf/Omnitrace, RenderDoc), while <strong>commercial/vendor tools</strong> (Nsight, VTune, etc.) often provide the most optimized access to proprietary hardware features (like NVIDIA&#x27;s profilers using privileged CUPTI APIs). Open tools may trade some low-level detail for broader applicability – for example, HPCToolkit can profile across NVIDIA, AMD, and Intel GPUs in a uniform way, but for deepest NVIDIA-specific metrics (e.g. SM warp stall reasons), developers still rely on Nsight Compute. Conversely, vendor tools are typically free-of-cost but <strong>closed-source</strong>, and each vendor&#x27;s toolchain is distinct, leading to fragmentation. A developer targeting multiple GPU platforms might need to juggle multiple profilers (one for CUDA, one for ROCm, one for Intel) since there is <em>no single standard interface</em> for GPU performance counters across vendors. The lack of standardization has led to projects like ARM&#x27;s <strong>HWCPipe</strong> library that attempt to abstract GPU counters for multiple architectures in one API, but such efforts are still evolving.</p><h3 class="content-header" id="gpu-tracing-with-low-overhead-and-continuous-monitoring"><a class="break-words" href="#gpu-tracing-with-low-overhead-and-continuous-monitoring" aria-hidden="true" tabindex="-1"><span class="content-header-link"><svg class="h-5 linkicon w-5" fill="currentColor" viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg"><path d="M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z"></path><path d="M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z"></path></svg></span></a>GPU Tracing with Low-Overhead and Continuous Monitoring</h3><p>Classic GPU profilers (as described above) are extremely useful during development, but their overhead and intrusive workflows make them unsuitable for always-on monitoring in production (for instance, you can&#x27;t afford a 5× slowdown on a live AI inference server just to collect traces). To fill this gap, recent tools leverage <em>low-overhead tracing techniques</em> inspired by systems like Linux&#x27;s eBPF. One notable approach is Meta&#x27;s deployment of <strong>eBPF</strong> for fleet-wide GPU profiling, as presented by Selim (2023) at the eBPF Summit. Instead of instrumenting GPU code, Meta&#x27;s tool attaches to GPU driver events via eBPF, enabling continuous collection of GPU metrics across thousands of machines with negligible overhead.</p><p>An example of an open project in this vein is <strong>GPUprobe</strong>, a Linux eBPF-based GPU observability tool. GPUprobe uses <em>uprobes</em> (user-level probes) to hook into NVIDIA&#x27;s CUDA runtime library functions at the kernel level. In doing so, it can monitor events like GPU memory allocations (<code>cudaMalloc/free</code>) and kernel launches in real time – <strong>without</strong> requiring any modifications or instrumentation in the target application code. The overhead is very low (measured under 4% in benchmarks), so it&#x27;s feasible to run in production continuously. GPUprobe fills a <em>middle ground</em> between heavy profilers and coarse monitoring: it provides richer, per-application insights than NVIDIA&#x27;s Data Center GPU Manager (DCGM) – such as tracking memory leaks per process and kernel launch frequencies – but with far less overhead than Nsight&#x27;s full profiling. As the GPUprobe authors note, <strong>Nsight Systems</strong> is like a &quot;GPU-specific debugger&quot; that&#x27;s great for deep dives but not for continuous use, while <strong>DCGM</strong> gives high-level stats (utilization, temps, health) and misses app-specific details. Tools like GPUprobe bridge this gap, exporting metrics to standard observability systems (e.g. Prometheus/Grafana) for integration into data center dashboards. In fact, GPUprobe&#x27;s design allows scraping of its metrics (memory usage maps, kernel launch counts, bandwidth usage) in <strong>OpenMetrics</strong> format, so operators can visualize GPU behavior over time in Grafana alongside CPU, network, and other metrics.</p><p>This <strong>BCC/eBPF-inspired approach</strong> is an emerging trend for GPU profiling. It aims to bring the powerful methodology of kernel tracing (pioneered on CPUs by tools like <code>perf</code>, <code>bcc</code>, and eBPF) into the GPU realm. Research prototypes have even explored running eBPF programs <em>on the GPU</em> itself (for instance, an academic project &quot;eGPU&quot; offloaded BPF bytecode to GPUs via PTX injection), though such techniques are not yet mainstream. At present, the more practical uses involve hooking GPU driver or runtime events from the CPU side. The result is a non-intrusive peek into GPU operations: for example, detecting if a GPU job is launch-bound (many small kernel launches) or memory-leak-prone, <em>without</em> recompiling the application. This is particularly valuable for cloud providers or large-scale AI deployments, where continuous profiling can catch performance regressions or resource leaks in long-running GPU services.</p><h2 class="content-header" id="profiling-and-tracing-tools-for-dpus-smartnics"><a class="break-words" href="#profiling-and-tracing-tools-for-dpus-smartnics" aria-hidden="true" tabindex="-1"><span class="content-header-link"><svg class="h-5 linkicon w-5" fill="currentColor" viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg"><path d="M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z"></path><path d="M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z"></path></svg></span></a>Profiling and Tracing Tools for DPUs (SmartNICs)</h2><p><strong>DPUs</strong> (data processing units), often manifested as <strong>SmartNICs</strong>, combine general-purpose cores with specialized packet-processing hardware. They are used to offload networking (packet switching, virtualization), storage (NVMe-oF, encryption), and security tasks from the main CPU. Profiling DPUs presents a distinct challenge: one must consider both the on-board CPU (usually an Arm SoC running Linux) and the networking data plane which may involve FPGA logic or ASIC accelerators on the NIC.</p><p>In general, profiling on a DPU can leverage many of the <em>standard Linux tools</em> for the embedded CPU portion. For example, NVIDIA&#x27;s BlueField DPUs run Ubuntu, so one can use <strong>Linux perf</strong>, standard CPU profilers, or even eBPF-based monitors <em>on the DPU&#x27;s Arm cores</em> to profile software running locally (e.g., an offloaded software switch). If a user application or agent runs on the DPU&#x27;s OS, it&#x27;s profiled much like on any Linux server – albeit with an awareness of the limited cores and unique tasks (often packet-handling threads). In fact, one could run BCC tools on a BlueField to measure syscalls, or use <code>perf</code> to sample cache misses in the DPU&#x27;s code. This is the <em>general-purpose workload</em> profiling on DPUs (similar to any Linux host, but constrained resources).</p><p>However, much of a DPU&#x27;s workload is <em>domain-specific (networking and storage)</em> and handled by specialized hardware blocks. For instance, a DPU may accelerate an Open vSwitch (OVS) datapath in hardware, or perform RDMA and NVMe operations via dedicated engines. Profiling these aspects often relies on <strong>vendor-provided telemetry and counters</strong>. Vendors like NVIDIA (Mellanox) and Broadcom supply tools to monitor packet throughput, latency, and offload engine stats on their SmartNICs. NVIDIA&#x27;s <strong>DOCA</strong> SDK for BlueField includes profiling APIs and performance monitors for accelerated functions (e.g., crypto, RDMA). The BlueField DPU exposes metrics such as packets per second, drops, and queue depths via standard interfaces (perhaps through DPDK or /sys counters). In the case of DPDK (a common user-space packet I/O library used with SmartNICs), developers can profile their packet processing pipeline on CPU using Intel VTune or perf, and measure NIC throughput using DPDK&#x27;s built-in event counters. Intel&#x27;s documentation even covers using VTune to analyze DPDK event scheduling on their infrastructure processing units (IPUs).</p><p>Commercial SmartNIC vendors offer their own monitoring suites. For example, <strong>Napatech</strong> (a SmartNIC manufacturer) distributes profiling tools that report port throughput, packet counters (RMON statistics), and even host application interaction metrics. These tools often come as command-line monitors or GUI dashboards. Napatech&#x27;s monitoring CLI (shown in their docs) can live-update line rate (e.g., ~48 Gbps Rx/Tx on a 100G NIC) and various packet size counters. Such vendor tools are usually proprietary (bundled with the NIC), highlighting a similarity with GPU space: to get full visibility into hardware accelerators on the DPU, you typically use the vendor&#x27;s API or utility. Another example: Broadcom/Pensando DPUs (now part of AMD) have an SDK that likely includes telemetry for their packet processors, though details are often behind NDAs. <strong>Cisco</strong> and <strong>Marvell</strong> likewise provide manageability interfaces for their SmartNICs (often as part of network OS or NIC firmware), focusing on throughput and latency metrics rather than instruction-level traces.</p><p>That said, open-source efforts are emerging for SmartNIC performance analysis. The P4 language, used to program some NICs, has debugging tools which can simulate or log packet flow through the pipeline (though not exactly a profiler in the traditional sense). Academic research has produced tools like <em>Clara</em> and <em>Pipefuse</em> to analyze or even <em>predict</em> network function performance on SmartNICs. These aim to answer questions like &quot;if I offload this function to a SmartNIC, what throughput can I expect?&quot; by modeling the NIC&#x27;s resources. While not runtime profilers, they address the broader performance tuning of DPU workloads. Another research example is <em>LogNIC</em>, which provides a performance model for SmartNIC pipelines. Such tools are largely experimental but point toward future <strong>high-level profilers</strong> for networking tasks.</p><p>In summary, <strong>DPU profiling today is a patchwork</strong> of general CPU profiling on one hand, and specialized network telemetry on the other. One might profile the <strong>software control plane</strong> on the DPU using familiar tools (to ensure the DPU&#x27;s CPU isn&#x27;t a bottleneck) while simultaneously using NIC counters or synthetic traffic tests to profile the <strong>data plane throughput</strong>. Coordinating these is often manual. For instance, to profile an Open vSwitch offloaded to a DPU, you&#x27;d measure the DPU&#x27;s CPU usage (for control tasks, flow setup) and gather NIC stats for packet rate and latency, possibly by generating test traffic and measuring end-to-end latency. Standard performance profilers for <em>network workloads</em> (like how to trace a P4 program on hardware) are still nascent. We expect that as DPUs become more common, vendor-agnostic profiling standards may emerge – perhaps an extension of eBPF/XDP to trace through a SmartNIC, or an open telemetry schema for NICs – but currently much is vendor-specific.</p><h2 class="content-header" id="profiling-and-tracing-tools-for-apus-cpugpu-integrated-platforms"><a class="break-words" href="#profiling-and-tracing-tools-for-apus-cpugpu-integrated-platforms" aria-hidden="true" tabindex="-1"><span class="content-header-link"><svg class="h-5 linkicon w-5" fill="currentColor" viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg"><path d="M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z"></path><path d="M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z"></path></svg></span></a>Profiling and Tracing Tools for APUs (CPU–GPU Integrated Platforms)</h2><p>Accelerated Processing Units (APUs) blend CPU and GPU on a single die, sharing memory and interconnect. AMD&#x27;s latest Instinct MI300A is a prime example: a data-center APU combining 24 Zen4 CPU cores with 128 GB of HBM memory and a CDNA3 GPU in one package. Profiling APUs involves understanding the <strong>interaction between the on-chip CPU and GPU</strong>, which can be both a blessing and a challenge. On one hand, unified memory means developers don&#x27;t need to profile PCIe transfer bottlenecks – CPU and GPU can access the same HBM pool, and data movement is via pointers rather than explicit copies. On the other hand, an APU&#x27;s GPU shares power/thermal budgets with CPUs, which can introduce contention that profiling tools should reveal (e.g., if the GPU is throttling when CPU is maxed out).</p><p>Tools for APUs largely overlap with the CPU and GPU tools discussed, with added emphasis on <strong>integrated analysis</strong>. AMD&#x27;s toolchain, for example, is APU-aware: <strong>AMD uProf</strong> is a profiling suite that covers CPU performance (PMU events, cache misses, etc.) and also can correlate with GPU activity on supported APUs. AMD uProf and CodeXL historically allowed profiling OpenCL kernels on an APU&#x27;s iGPU, reporting each kernel&#x27;s performance counters. The new ROCm Omnitrace (mentioned earlier) explicitly supports profiling both CPU and GPU in one timeline, which is ideal for APUs where CPU threads launch GPU work frequently. Omnitrace&#x27;s ability to use binary instrumentation and call-stack sampling on the CPU side, while tracing GPU kernels, helps map performance &quot;holographically&quot; across the APU. This means if a CPU function on the APU calls a GPU kernel, the tool can show the time in the CPU function and the nested time in the GPU kernel as part of the same call tree – a critical capability for optimizing heterogeneous code.</p><p>For consumer APUs (like AMD Ryzen processors with Radeon graphics), developers commonly use <strong>graphics profilers</strong> (for gaming use-cases) or <strong>OpenCL/Vulkan profilers</strong> for compute. AMD&#x27;s Radeon GPU Profiler, for instance, works on integrated GPUs the same as discrete. The unified memory also allows use of standard OS performance counters: on Linux, AMD&#x27;s GPU drivers expose certain GPU utilization metrics via <code>drm/sysfs</code>, so one could even use system monitors or custom scripts to log GPU activity alongside CPU. Windows developers with APUs might use Microsoft&#x27;s <strong>PIX</strong> or AMD&#x27;s Radeon Developer tools to profile DirectX12 games running on the integrated GPU – these tools show CPU and GPU timelines and could highlight if the CPU is starving the GPU or vice versa. Essentially, APU profiling doesn&#x27;t require an entirely new class of tools, but it <strong>benefits from tools that can correlate CPU and GPU performance</strong> tightly. This is similar to profiling on a discrete GPU system, except the latency between CPU-GPU is lower and memory is shared, which tools need to account for (e.g., a cache coherency effect might appear where CPU and GPU contend on memory).</p><p>It&#x27;s worth noting that integrated architectures spurred the development of HSA (Heterogeneous System Architecture) in the mid-2010s, and AMD&#x27;s tools had HSA-specific profiling modes. For example, CodeXL included an HSA profiler for AMD APUs which could trace HSA kernel dispatches and HSAIL instructions. Much of that functionality has been absorbed into ROCm tools now. The essence remains: <strong>APUs require profiling of the <em>whole system</em> rather than just &quot;CPU vs GPU&quot; in isolation</strong>. Tools like Omnitrace, VTune, or HPCToolkit (with its heterogeneous call path analysis) are particularly apt for APU-based workloads because they naturally mix CPU and GPU metrics.</p><h2 class="content-header" id="open-source-vs-commercial-solutions--a-comparison"><a class="break-words" href="#open-source-vs-commercial-solutions--a-comparison" aria-hidden="true" tabindex="-1"><span class="content-header-link"><svg class="h-5 linkicon w-5" fill="currentColor" viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg"><path d="M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z"></path><path d="M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z"></path></svg></span></a>Open-Source vs Commercial Solutions – A Comparison</h2><p>There is a healthy mix of open-source (OSS) and commercial/proprietary solutions in accelerator profiling, each with pros and cons. Here we compare key aspects:</p><ul><li><p><strong>Feature Depth and Hardware Access:</strong> Vendor-provided tools (usually closed-source, but often free of charge) tend to have the most <strong>comprehensive access to hardware performance counters and features</strong>. For example, NVIDIA&#x27;s Nsight can report SM warp stall reasons and texture cache hit rates – metrics exposed by NVIDIA&#x27;s secret sauce interfaces that open tools generally don&#x27;t get. Similarly, AMD&#x27;s proprietary driver might expose GPU wavefront occupancy details to RGP that generic tools can&#x27;t obtain. OSS tools rely on published or reverse-engineered interfaces; for instance, AMD&#x27;s GPUPerfAPI (open library) provides cross-platform counter access, which is why AMD&#x27;s own tools could be open-sourced. <strong>Open-source projects</strong> sometimes lack the very latest hardware support until vendors release documentation, whereas commercial tools are ready on Day 1 for new GPUs (since the vendor builds them). On the other hand, open tools like HPCToolkit have innovated features like fully automated call-stack unwinding and statistical GPU instruction sampling that are not available in vendor GUIs, showing that OSS can lead in certain capabilities (particularly around integration and low overhead).</p></li><li><p><strong>Extensibility and Customization:</strong> Open-source profilers (HPCToolkit, TAU, etc.) allow users to modify or script them, enabling <strong>custom analyses</strong> or integration into automated pipelines. For instance, you can instrument code with Score-P and emit traces in Open Trace Format (OTF2), then post-process with custom analytics – all because the formats and code are open. In contrast, commercial tools often lock data in proprietary formats (e.g., Nsight&#x27;s <code>.nsys-rep</code> trace files) that require the vendor&#x27;s viewer, though some export options exist (like CSV exports). The OSS approach also fosters community contributions – e.g., support for new programming models (OpenMP offload, Kokkos, etc.) often appears first in tools like TAU or Score-P via community patches.</p></li><li><p><strong>Cross-Vendor Support:</strong> As noted, open-source tools are generally more <strong>vendor-neutral</strong>. A single tool like Vampir or HPCToolkit can handle <em>multiple</em> accelerator types in one run, whereas vendor tools are siloed (Nsight won&#x27;t profile an AMD GPU, and AMD&#x27;s rocprof won&#x27;t work on NVIDIA). For a heterogeneous environment (say, an Intel CPU, an NVIDIA GPU, and maybe a Xilinx FPGA in one system), your only hope for a unified trace might be an open tool that supports all via plugins or standard APIs (OpenCL, oneAPI, etc.). This is a strong point in favor of OSS solutions in research or multi-vendor shops. The downside is that <strong>vendor tools are often better optimized</strong> for their own hardware – they may offer a more polished UI, or more stable data collection on that platform. For example, an open tool using unofficial GPU counters might be brittle or less accurate if drivers change.</p></li><li><p><strong>Cost and Support:</strong> Most vendor tools for GPUs/DPUs are free (as in beer) but closed. There are a few truly commercial (for-purchase) performance tools in HPC, such as the Arm Forge suite (which includes the MAP profiler) – these come with professional support. Open-source tools are free (as in speech/beer) but support comes from community or self-expertise. Companies with mission-critical needs sometimes prefer tools backed by vendor support (to help interpret results or get bug fixes). That said, big vendors (NVIDIA, Intel) do provide support forums even for their free tools. In niche domains like networking, some commercial analyzers (e.g., deep packet inspection performance profilers) might come from specialized firms and require licenses – but these are relatively rare.</p></li></ul><p>In practice, environments often use a <strong>combination</strong>: e.g., an HPC center might use vendor profilers to optimize code on a specific GPU, but then integrate HPCToolkit or IPM (an MPI profiler) for regression testing and cross-system comparisons. Notably, open and closed tools can complement each other. A developer might run an OSS tracing tool for a high-level overview and cross-check specific kernels with the vendor&#x27;s detailed profiler. An example from the GPU domain: a user could run a Score-P instrumented program to get an overall MPI+GPU timeline, then zoom into a particular GPU kernel of interest with Nsight Compute to inspect its memory throughput. This layered approach plays to each tool&#x27;s strengths.</p><h2 class="content-header" id="workload-specific-tool-mappings"><a class="break-words" href="#workload-specific-tool-mappings" aria-hidden="true" tabindex="-1"><span class="content-header-link"><svg class="h-5 linkicon w-5" fill="currentColor" viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg"><path d="M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z"></path><path d="M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z"></path></svg></span></a>Workload-Specific Tool Mappings</h2><p>Different workloads stress accelerators in different ways, and accordingly, certain tools are favored in each domain:</p><ul><li><p><strong>General-Purpose Computing / HPC:</strong> These workloads (scientific simulations, linear algebra, data analytics) use GPUs for throughput. Profiling focuses on kernel efficiency and GPU utilization. Tools like Nsight Compute (for compute kernel analysis) or HPCToolkit/Omniperf are well-suited. HPC codes also run on thousands of GPUs in parallel; tracing each in detail is impractical, so tools like HPCToolkit that add only ~1-5% overhead via sampling are invaluable for profiling large-scale runs. HPC workloads often use MPI + GPU, so tools that can correlate communication and computation (e.g., timeline traces via Extrae, or MPI profiles via mpiP combined with GPU profiles) map well. On DPUs in HPC (e.g., using SmartNICs for RDMA), the &quot;workload&quot; is typically just networking – here one cares about throughput and overlap (profiling ensures that the DPU handles data transfers while GPUs compute, for example). Tools: network benchmarks (like IB Profiler for InfiniBand) plus GPU profilers to see if communication overlaps with computation.</p></li><li><p><strong>Machine Learning / AI:</strong> ML training combines heavy GPU compute with data pipeline overhead. Profiling an ML workload might involve <strong>framework-level profilers</strong>: e.g., <em>PyTorch Profiler</em> (built on Kineto) which internally uses CUPTI to record each op&#x27;s GPU time, or <em>TensorFlow Profiler</em> which similarly captures timelines of ops and streams. These produce high-level views (which model layer took time) as well as low-level kernel details. NVIDIA has a <em>DLProf</em> tool that integrates with TensorBoard to show GPU kernel metrics in the context of neural network operations. For multi-GPU training, Nsight Systems can trace activity across GPUs (especially if using NCCL for communication – Nsight can show NCCL calls timeline). An emerging challenge is profiling <strong>distributed training</strong>: tools like PyTorch Profiler now have distributed traces, but it&#x27;s still an area of active development to seamlessly profile 100s of GPUs training one model. On the DPU side, AI may use DPUs for preprocessing or moving data – profiling the DPU&#x27;s effect (say using it to do data filtering) would involve monitoring how much the DPU speeds up data ingestion (tracked via throughput) and ensuring the GPU is not starved. In the future, <em>AI accelerators on DPUs</em> (some DPUs might include tiny ML cores) could require new profilers, but currently most AI work is GPU-centric.</p></li><li><p><strong>Networking and I/O Workloads:</strong> For pure networking tasks on DPUs or GPUs (yes, GPUs can also do packet processing in some cases using CUDA or OpenCL), the profiling is about latency and throughput. Tools here include packet generators (to measure how many Mpps a pipeline can handle) and tracing tools for code paths. For instance, if using a GPU to accelerate packet encryption, one might use Nsight to ensure kernel launches overlap with data transfers. If using a DPU to run, say, an IDS (intrusion detection) in software, one might profile it with perf to see if it&#x27;s CPU-bound and use NIC counters for drops. Networking workloads often demand real-time tracing (to catch jitter spikes), so eBPF-based monitors or even hardware telemetry (like P4 runtime logs) could be employed. There&#x27;s also interest in using GPU for networking (GPU-accelerated NIC offloads via CUDA pipelines), which would involve both GPU and network profiling – but that&#x27;s fairly niche and experimental.</p></li><li><p><strong>Graphics and Visualization:</strong> For game engines or VR apps on GPUs (especially APUs in consoles or laptops), tools like RenderDoc, Nsight Graphics, and platform profilers (e.g., Apple&#x27;s Xcode Instruments for Metal) are tailored to measure <em>frame times</em>, <em>GPU pipeline stages</em>, and <em>CPU-GPU synchronization</em>. A graphics workload is typically limited by either the GPU shader throughput or the CPU draw-call submission rate. Profiling maps to checking if the GPU&#x27;s frame time budget is being exceeded and why (which stage – vertex shading? fragment? memory?). These tools often provide specialized visualizations (HUD overlays, frame scrubbers) that general compute profilers don&#x27;t. For APUs handling graphics, one must also consider that the CPU and GPU share memory bandwidth – graphics debuggers can show if CPU memory traffic (e.g., updating textures) is affecting the GPU. Additionally, in professional visualization (CAD, etc.), GPU memory usage can be limiting; tools like NVIDIA&#x27;s Nsight or AMD&#x27;s RGPA can profile VRAM usage and cache behavior to optimize large models.</p></li></ul><p>In essence, each processing unit type sees use in particular domains, and the profiling solutions have evolved accordingly – but there is also convergence. A modern AI application may involve <strong>GPUs for compute, CPUs for orchestration, DPUs for data loading</strong>, all in one pipeline. This raises the need for <strong>multi-accelerator profiling</strong> – the ability to trace an operation as it moves through CPU, DPU, and GPU. Today this often means running multiple tools and correlating timestamps manually. For instance, one might use Nsight to trace the GPU and <em>simultaneously</em> run <code>tcpdump</code> or NIC counters on the network side, then align the logs. Such multi-component workflows are cumbersome, pointing to an opportunity for more integrated profiling of heterogeneous workflows.</p><h2 class="content-header" id="current-research-directions-and-emerging-trends"><a class="break-words" href="#current-research-directions-and-emerging-trends" aria-hidden="true" tabindex="-1"><span class="content-header-link"><svg class="h-5 linkicon w-5" fill="currentColor" viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg"><path d="M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z"></path><path d="M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z"></path></svg></span></a>Current Research Directions and Emerging Trends</h2><p>The field of accelerator tracing/profiling is actively evolving. Several key research and development directions are apparent:</p><ul><li><p><strong>Low-Overhead, Always-On Profiling:</strong> As discussed, techniques borrowing from OS telemetry (eBPF, hardware performance monitors) are being adapted for accelerators. Meta&#x27;s continuous GPU profiler and tools like GPUprobe demonstrate that one can collect useful data <em>in production</em> with minimal overhead. We anticipate more work in this area: for example, <em>continuous DPU monitoring</em> integrated into data center observability stacks (similar to DCGM for GPUs) – perhaps using eBPF to monitor DPU NIC drivers or using in-hardware telemetry (many NICs have telemetry streams for packet pacing, queue occupancy, etc., which could be tapped into). For GPUs, researchers are exploring <strong>sampling-based profiling</strong> to reduce overhead further. NVIDIA&#x27;s latest architectures support <em>PC sampling</em>, where the GPU periodically samples its program counter and reports which instructions are executing and if they stalled. This can be done in the background with little interference. HPCToolkit already leverages this on NVIDIA GPUs to sample instructions and derive stall breakdowns. Future tools might extend such sampling to gather a statistical trace of GPU activity without instrumenting each kernel launch.</p></li><li><p><strong>Unified and Standardized Interfaces:</strong> A recurring theme is the lack of standardization across vendors. There are calls for a <strong>vendor-neutral GPU profiling API</strong> – for instance, a past suggestion in the OpenGL community proposed a unified API for GPU performance counters across vendors. In the compute realm, something analogous to the CPU&#x27;s PAPI (Performance API) for GPUs would be valuable. We see early steps: the Khronos Group&#x27;s OpenCL and Vulkan APIs have performance query extensions (like Vulkan&#x27;s <code>VK_KHR_performance_query</code>) that let applications gather some counters in a standardized way. Also, Intel&#x27;s oneAPI aims to provide a uniform interface (Level Zero) for accelerators, including tools support. While oneAPI is Intel-centric, it sets a precedent for abstracting profiling: an application could, in theory, use oneAPI to profile code on CPUs, GPUs, and FPGAs with a single tool – but only if other vendors adopt or adapt to it. Another push is in <strong>OpenTelemetry</strong> for hardware – currently OpenTelemetry (popular in cloud for tracing requests) doesn&#x27;t cover internal hardware events, but conceivably it could be extended to span spans across CPU, accelerator, and network events for distributed tracing of heterogeneous workloads.</p></li><li><p><strong>Integration of AI in Performance Analysis:</strong> With the complexity of performance data (especially from fine-grained traces), there&#x27;s interest in using machine learning to assist performance analysis. Research projects have looked at learning models to predict performance from partial traces or to automatically classify bottlenecks from counter signatures. For example, an ML model might learn the patterns in counters that indicate memory bandwidth bottleneck vs computation-bound, automating what human experts do manually with roofline models. While not mainstream in tools yet, some profilers (like Intel Advisor&#x27;s automated roofline analysis, or NVIDIA&#x27;s Guided Analysis in Nsight Compute) incorporate heuristic guidance that could evolve into ML-driven suggestions. The goal is to help developers interpret the deluge of profiling data more easily.</p></li><li><p><strong>Causal Tracing and Causal Profiling:</strong> Traditional profiling observes performance passively. A newer research direction is <em>causal profiling</em>, where the profiler experiments by perturbing execution to gauge impact on performance (e.g., artificially slow down one component to see if others idle, determining causality of bottlenecks). Omnitrace&#x27;s mention of causal profiling support hints that such techniques are being implemented for GPU/CPU combos. Causal tracing could identify, for instance, that GPU kernel X finishing late is what delays CPU task Y, by seeing how timings change if X is made faster or slower. This is an advanced capability with potential to untangle complex dependencies in asynchronous pipelines.</p></li><li><p><strong>Multi-Accelerator and Distributed Coordination:</strong> As systems integrate CPUs + various accelerators (GPUs, DPUs, FPGAs, TPUs, etc.), one research challenge is coordinating profiling across them. How do we get a coherent timeline or profile when parts of the work happen on different chips with their own clocks and trace buffers? Tools like <strong>Extrae/Paraver</strong> in HPC can merge traces from CPU and GPU by aligning timestamps (assuming synchronized clocks) and allow analyzing them together. We expect further development here, possibly with standard timestamping (PTP – precision time protocol – could be used to sync time between host and DPU, for example). Also, orchestrating triggers across tools – e.g., start a GPU profiler when a network event happens – is being explored. Some current tools allow triggers (Nsight can start/stop based on CUDA API calls or markers); extending this across devices (start GPU trace when DPU&#x27;s packet queue overflows) could be extremely useful for diagnosing cross-stack performance issues (like a slow GPU causing packet backlog on a SmartNIC).</p></li><li><p><strong>Better Profiling for New Accelerator Types:</strong> While this report focuses on GPUs, DPUs, and APUs, the universe of accelerators includes FPGAs, AI ASICs (e.g., Google&#x27;s TPUs), and more. Each is spawning its own tooling – Xilinx (AMD) FPGAs have the Vivado and Vitis analyzer for hardware kernels, Google TPUs have a profiler in TensorBoard, etc. A clear direction is to bring these together. If a cloud has CPUs, GPUs, DPUs, TPUs all working in tandem, the dream is a <em>single pane of glass</em> to observe them. Industry consortia may eventually collaborate on open standards for accelerator telemetry (analogous to how OpenCL was a standard for compute). Until then, research often steps in: for example, academic work on monitoring FPGAs in datacenters via in-fabric monitors, or using eBPF-like techniques on other devices.</p></li></ul><p>In summary, <strong>profiling/tracing research is moving toward making these tools more <em>pervasive</em>, <em>intelligent</em>, and <em>unified</em></strong>. The aim is to reduce the burden on developers to manually instrument and correlate performance data from disparate sources, and instead provide smarter tools that work across the complex heterogeneous systems of today&#x27;s data centers.</p><h2 class="content-header" id="limitations-and-challenges-of-existing-toolchains"><a class="break-words" href="#limitations-and-challenges-of-existing-toolchains" aria-hidden="true" tabindex="-1"><span class="content-header-link"><svg class="h-5 linkicon w-5" fill="currentColor" viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg"><path d="M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z"></path><path d="M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z"></path></svg></span></a>Limitations and Challenges of Existing Toolchains</h2><p>Despite the plethora of tools discussed, practitioners face several persistent challenges when profiling GPUs, DPUs, and APUs:</p><ul><li><p><strong>Fragmentation and Vendor Lock-In:</strong> As noted, each vendor&#x27;s accelerators often come with a siloed toolchain. This means expertise in one doesn&#x27;t translate easily to another, and mixing hardware leads to multiple tools. It also risks <em>lock-in</em>: optimizations done with a proprietary tool might rely on vendor-specific features. There is no universal standard like &quot;perf&quot; that universally covers all accelerator types (though on CPUs, perf itself is limited to Linux). The lack of common performance counter interfaces across GPU vendors is a prime example – developers must use CUDA-specific or ROCm-specific APIs, making portable performance analysis difficult. For DPUs, which are relatively new, there isn&#x27;t even a widely adopted third-party profiler – you use whatever the DPU vendor provides. This fragmentation not only complicates life for developers, but also for researchers trying to compare platforms fairly.</p></li><li><p><strong>Limited Visibility (&quot;Black Box&quot; issues):</strong> Some aspects of accelerator performance are effectively black boxes to current profilers. For instance, GPU internal scheduling (how warps are scheduled, how L2 cache lines are evicted) might not be fully exposed by counters. Similarly, if a DPU&#x27;s packet accelerator is an FPGA or ASIC, external tools might only see a throughput number, not <em>why</em> it saturates at X Gbps (e.g., a microarchitecture detail inside the NIC). Even with something like NVIDIA&#x27;s counters, there are often <em>undocumented metrics</em> or ones that are hard to interpret. This limited visibility is often by design (IP protection), but hampers optimization. Another visibility issue arises in <em>closed-source workloads</em>: if you are profiling a third-party library on the GPU, you might see a kernel name and time but not what it did internally. Tools like HPCToolkit help by attributing costs to instructions even without source, but generally it&#x27;s hard to optimize what you can&#x27;t inspect.</p></li><li><p><strong>Overhead vs. Intrusiveness:</strong> Many precise tracing tools perturb the very performance they measure. Instrumentation-based tracing (inserting hooks on each kernel launch or each packet) can cause Heisenberg effects where the act of measuring changes timing. While sampling-based methods alleviate this, they trade detail for lower overhead. There is always a challenge to find the right balance: <em>how much data to collect</em> and <em>at what frequency</em> to get a representative profile without drowning in overhead or data volume. High-overhead tools must be limited to test environments, meaning you might not catch issues that only manifest at scale or in production. Conversely, ultra-light monitors might only flag &quot;GPU utilization low&quot; but not explain the cause. Bridging this gap remains an ongoing challenge.</p></li><li><p><strong>Concurrency and Ordering Issues:</strong> Profiling multi-threaded, multi-stream workloads can run into issues aligning events. For example, timeline traces from different GPUs or different devices may have clock skew, making it tricky to know the true sequence of events. Even on one GPU, the hardware can execute many kernels concurrently (on different SMs or using async streams), and visualizing or understanding overlapping activities is complex. Tools try (Nsight Systems, for instance, shows overlapping kernels on a timeline), but as systems scale, one confronts what we might call the &quot;explosion of events&quot; problem: too many events to reason about. Filtering and focusing on the right subset is difficult. Tools are only beginning to introduce smarter filtering (like only trace kernels longer than X microseconds, or only trace GPU activity when GPU utilization drops, etc.).</p></li><li><p><strong>Lack of Multi-Accelerator Coordination:</strong> Today&#x27;s tools largely operate in isolation per device. If you have a heterogeneous node with CPU, GPU, DPU, FPGA, each might be profiled separately, yielding separate reports that the user must correlate. Suppose a performance issue is due to a mismatch between GPU throughput and NIC throughput – a GPU profiler might just show the GPU is idle 20% of time (waiting for data), and a NIC monitor shows 100% utilization on a queue – it&#x27;s up to the engineer to correlate those and deduce the cause (network-bound). Ideally, a profiler would capture such cross-device dependency automatically (e.g., a visual cue that GPU idleness correlates with NIC saturation). Achieving that requires a holistic view and perhaps standardized trace events that can link across devices (like an event on NIC &quot;frame delivered&quot; could be tied to an event &quot;frame processed on GPU&quot;). Without common standards, such correlation is mostly manual or via ad-hoc instrumentation (inserting timestamps in app code).</p></li><li><p><strong>Scaling and Big Data Problems:</strong> When profiling large-scale workloads (think 1000 GPUs or a DPU handling millions of packets per second), the volume of profiling data can be enormous. Storing and analyzing trace logs from even a few seconds of operation may be non-trivial. There is a challenge in <strong>data reduction</strong> – how to summarize performance data meaningfully. Current tools offer some summaries (like average kernel time, top 10 memory consumers, etc.), but more automated summarization is needed, possibly with hierarchical or statistical techniques to condense traces. HPC centers have dealt with this by selective tracing (capturing only on a few ranks, etc.) or using sampling. Future tools might incorporate on-line analysis, where the tool itself does some processing of data as it&#x27;s collected (for example, computing distributions instead of logging every event).</p></li><li><p><strong>Education and Usability:</strong> Finally, it&#x27;s worth noting a practical challenge: the learning curve. Each tool often comes with its own GUI or output format, and understanding metrics like &quot;warp serialize&quot; or &quot;bank conflict&quot; or &quot;DPU cache hit&quot; requires some architecture knowledge. Performance analysis on these accelerators is somewhat a dark art, and although tools provide data, making sense of it is not always straightforward. Efforts like roofline models are attempts to simplify interpretation, but users still struggle to go from profiler output to concrete optimizations. This is partially an educational challenge – documentation and training need to accompany tools. It&#x27;s also a design challenge for tool builders to present data in intuitive ways (e.g., Nvidia now often integrates AI performance metrics like &quot;utilization of Tensor Cores&quot; directly, to tell ML engineers how well they used the GPU).</p></li></ul><p>In summary, while current tracing and profiling tools for GPUs, DPUs, and APUs are powerful and essential, they operate in a landscape that is <strong>highly specialized and fragmented</strong>, with visibility gaps and integration shortcomings. Overcoming these limitations will require collaborative efforts – between hardware vendors (to open up interfaces), tool developers (to create smarter, standard tools), and the research community (to pioneer new techniques for low-overhead and combined tracing).</p><h2 class="content-header" id="conclusion"><a class="break-words" href="#conclusion" aria-hidden="true" tabindex="-1"><span class="content-header-link"><svg class="h-5 linkicon w-5" fill="currentColor" viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg"><path d="M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z"></path><path d="M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z"></path></svg></span></a>Conclusion</h2><p>Tracing and profiling accelerators has become as critical as profiling CPUs was in earlier eras. We now have a broad arsenal of tools tailored to different hardware and use cases: from NVIDIA&#x27;s Nsight suite and AMD&#x27;s ROCm tools for deep GPU analysis, to open-source frameworks like HPCToolkit for holistic profiling, to emerging eBPF-based approaches enabling continuous monitoring. <strong>GPUs</strong> enjoy the most mature tool support (reflecting their longer history in computing), while <strong>DPUs</strong> and other domain-specific accelerators are catching up with their own nascent profilers and telemetry systems. <strong>APUs</strong> illustrate the need for tools that can seamlessly profile across traditional processor boundaries, as computing moves toward tightly integrated heterogeneous designs.</p><p>This review has highlighted that both open-source and commercial solutions play important roles: open tools foster cross-platform agility and innovation, whereas vendor tools leverage proprietary knowledge for maximum insight on their hardware. The best outcomes often arise from using them in combination. In specialized domains like ML, networking, and graphics, domain-specific profiling capabilities augment general tools to provide the needed perspective (e.g., viewing a GPU timeline in terms of neural network layers, or network throughput in context of CPU cycles).</p><p>Looking ahead, the trends point to more <strong>integration (unified timelines across accelerators), automation (intelligent analysis), and low-overhead observability</strong> becoming standard. Research is actively addressing many current gaps, from standardizing performance metrics to using novel techniques like causal profiling and ML-driven analysis to interpret performance data. At the same time, challenges like vendor lock-in and black-box hardware will require industry collaboration and perhaps a push for more open hardware telemetry interfaces.</p><p>In conclusion, developers and engineers aiming to optimize accelerator-powered systems should take a <strong>hybrid approach</strong>: leverage the rich features of vendor-specific profilers for detailed analysis, use open-source and cross-platform tools to get the &quot;big picture&quot; across diverse hardware, and keep an eye on emerging tools that can be adopted to improve continuous performance monitoring. By combining these tools and techniques, one can obtain a comprehensive understanding of performance for GPUs, DPUs, and APUs across any workload – from a single GPU kernel&#x27;s instruction stalls up to the end-to-end behavior of an entire heterogeneous pipeline. Such deep and broad profiling capability will be essential to fully exploit the computational power of modern accelerators in general-purpose and domain-specific applications alike.</p><h2 class="content-header" id="references"><a class="break-words" href="#references" aria-hidden="true" tabindex="-1"><span class="content-header-link"><svg class="h-5 linkicon w-5" fill="currentColor" viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg"><path d="M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z"></path><path d="M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z"></path></svg></span></a><strong>References</strong></h2><ul><li><a class="break-words" target="_blank" rel="noopener noreferrer" href="https://blogs.nvidia.com/blog/whats-a-dpu-data-processing-unit/">NVIDIA Blog - &quot;What Is a DPU?&quot;</a></li><li><a class="break-words" target="_blank" rel="noopener noreferrer" href="https://www.amd.com/en/products/accelerators/instinct/mi300/mi300a.html">AMD - &quot;AMD Instinct™ MI300A Accelerators&quot;</a></li><li><a class="break-words" target="_blank" rel="noopener noreferrer" href="https://www.admin-magazine.com/HPC/Articles/Profiling-Is-the-Key-to-Survival">ADMIN Magazine - &quot;Profiling Is the Key to Survival&quot;</a></li><li><a class="break-words" target="_blank" rel="noopener noreferrer" href="https://dev.to/ethgraham/snooping-on-your-gpu-using-ebpf-to-build-zero-instrumentation-cuda-monitoring-2hh1">DEV Community - &quot;Snooping on your GPU: Using eBPF to Build Zero-instrumentation CUDA Monitoring&quot;</a></li><li><a class="break-words" target="_blank" rel="noopener noreferrer" href="https://documentation.help/codexl/using-the-gpu-profiler.htm">CodeXL Documentation - &quot;Using the GPU Profiler&quot;</a></li><li><a class="break-words" target="_blank" rel="noopener noreferrer" href="https://community.amd.com/sdtpp67534/attachments/sdtpp67534/codexl-discussions/62/1/CodeXL_Release_Notes.pdf">AMD Community - &quot;CodeXL 2.6 GA Release Notes&quot;</a></li><li><a class="break-words" target="_blank" rel="noopener noreferrer" href="https://rocm.docs.amd.com/en/docs-6.2.0/about/release-notes.html">ROCm Documentation - &quot;ROCm 6.2.0 release notes&quot;</a></li><li><a class="break-words" target="_blank" rel="noopener noreferrer" href="https://www.performance-intensive-computing.com/objectives/developing-ai-and-hpc-solutions-check-out-the-new-amd-rocm-62-release">Performance Intensive Computing - &quot;Developing AI and HPC solutions? Check out the new AMD ROCm&quot;</a></li><li><a class="break-words" target="_blank" rel="noopener noreferrer" href="https://www.lei.chat/posts/sampling-performance-counters-from-gpu-drivers/">Lei.Chat - &quot;Sampling Performance Counters from Mobile GPU Drivers&quot;</a></li><li><a class="break-words" target="_blank" rel="noopener noreferrer" href="https://gpuopen.com/gpuperfapi/">AMD GPUOpen - &quot;GPUPerfAPI&quot;</a></li><li><a class="break-words" target="_blank" rel="noopener noreferrer" href="https://biztechmagazine.com/article/2024/09/what-data-processing-unit-and-how-does-it-help-computing-perfcon">BizTech Magazine - &quot;What Is a DPU (Data Processing Unit)? How Does It Work?&quot;</a></li><li><a class="break-words" target="_blank" rel="noopener noreferrer" href="https://www.linkedin.com/posts/ebpf-summit_gpu-profiling-with-bpf-at-meta-riham-selim-activity-7138119919023419392-prW4">LinkedIn - eBPF Summit - &quot;GPU Profiling with BPF at Meta - Riham Selim&quot;</a></li><li><a class="break-words" target="_blank" rel="noopener noreferrer" href="https://github.com/GPUprobe/gpuprobe-daemon">GitHub - &quot;GPUprobe/gpuprobe-daemon&quot;</a></li></ul></div><div class="pt-6 pb-6 text-sm text-gray-700 dark:text-gray-300 glass p-4 rounded-xl"><a class="hover:text-primary-600 dark:hover:text-primary-400 transition-colors" target="_blank" rel="nofollow" href="https://mobile.twitter.com/search?q=https%3A%2F%2Fwww.yunwei37.com%2Fblog%2Fgpu-profile-tools-analysis">Discuss on Twitter</a> • <a class="hover:text-primary-600 dark:hover:text-primary-400 transition-colors" target="_blank" rel="noopener noreferrer" href="https://github.com/yunwei37/yunwei37-blog/blob/main/data/blog/gpu-profile-tools-analysis.mdx">View on GitHub</a></div><div class="pt-6 pb-6 text-center text-gray-700 dark:text-gray-300 glass p-6 rounded-xl" id="comment"><button>Load Comments</button></div></div><footer><div class="divide-gray-200/30 text-sm leading-5 font-medium xl:col-start-1 xl:row-start-2 xl:divide-y dark:divide-gray-700/30"><div class="py-4 xl:py-8"><h2 class="text-xs tracking-wide text-gray-600 uppercase dark:text-gray-300 mb-4">Tags</h2><div class="flex flex-wrap gap-2"><a class="inline-block px-3 py-1 text-xs font-medium uppercase tracking-wide bg-primary-100 hover:bg-primary-200 dark:bg-primary-900/30 dark:hover:bg-primary-800/40 rounded-full transition-all duration-200 text-primary-800 hover:text-primary-900 dark:text-primary-300 dark:hover:text-primary-200 border border-primary-200/50 dark:border-primary-700/50" href="/tags/ebpf">ebpf</a><a class="inline-block px-3 py-1 text-xs font-medium uppercase tracking-wide bg-primary-100 hover:bg-primary-200 dark:bg-primary-900/30 dark:hover:bg-primary-800/40 rounded-full transition-all duration-200 text-primary-800 hover:text-primary-900 dark:text-primary-300 dark:hover:text-primary-200 border border-primary-200/50 dark:border-primary-700/50" href="/tags/systems">systems</a></div></div><div class="flex justify-between py-4 xl:block xl:space-y-6 xl:py-8"><div class="glass p-4 rounded-xl"><h2 class="text-xs tracking-wide text-gray-600 uppercase dark:text-gray-300 mb-2">Previous Article</h2><div class="text-primary-600 hover:text-primary-700 dark:text-primary-400 dark:hover:text-primary-300 transition-colors"><a class="break-words" href="/blog/aios">OS-Level Challenges in LLM Inference and Optimizations</a></div></div><div class="glass p-4 rounded-xl"><h2 class="text-xs tracking-wide text-gray-600 uppercase dark:text-gray-300 mb-2">Next Article</h2><div class="text-primary-600 hover:text-primary-700 dark:text-primary-400 dark:hover:text-primary-300 transition-colors"><a class="break-words" href="/blog/gpu-profile-tool-impl">GPU Profiling Under the Hood: An Implementation-Focused Survey of Modern Accelerator Tracing Tools</a></div></div></div></div><div class="pt-4 xl:pt-8"><a class="inline-flex items-center glass px-4 py-2 rounded-xl text-primary-600 hover:text-primary-700 dark:text-primary-400 dark:hover:text-primary-300 transition-all duration-200 hover:scale-105" aria-label="Back to the blog" href="/blog">← Back to the blog</a></div></footer></div></div></article></section><!--$--><!--/$--></main><footer class="border-t border-gray-200/30 dark:border-gray-700/30 pt-8"><div class="flex flex-col items-center space-y-6"><div class="flex flex-wrap justify-center gap-4"><a class="text-sm text-gray-500 transition hover:text-gray-600" target="_blank" rel="noopener noreferrer" href="mailto:yunwei356@gmail.com"><span class="sr-only">mail</span><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" class="hover:text-primary-500 dark:hover:text-primary-400 fill-current text-gray-700 dark:text-gray-200 h-6 w-6"><title>Mail</title><path d="M2.003 5.884L10 9.882l7.997-3.998A2 2 0 0016 4H4a2 2 0 00-1.997 1.884z"></path><path d="M18 8.118l-8 4-8-4V14a2 2 0 002 2h12a2 2 0 002-2V8.118z"></path></svg></a><a class="text-sm text-gray-500 transition hover:text-gray-600" target="_blank" rel="noopener noreferrer" href="https://github.com/yunwei37"><span class="sr-only">github</span><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" class="hover:text-primary-500 dark:hover:text-primary-400 fill-current text-gray-700 dark:text-gray-200 h-6 w-6"><title>GitHub</title><path d="M12 .297c-6.63 0-12 5.373-12 12 0 5.303 3.438 9.8 8.205 11.385.6.113.82-.258.82-.577 0-.285-.01-1.04-.015-2.04-3.338.724-4.042-1.61-4.042-1.61C4.422 18.07 3.633 17.7 3.633 17.7c-1.087-.744.084-.729.084-.729 1.205.084 1.838 1.236 1.838 1.236 1.07 1.835 2.809 1.305 3.495.998.108-.776.417-1.305.76-1.605-2.665-.3-5.466-1.332-5.466-5.93 0-1.31.465-2.38 1.235-3.22-.135-.303-.54-1.523.105-3.176 0 0 1.005-.322 3.3 1.23.96-.267 1.98-.399 3-.405 1.02.006 2.04.138 3 .405 2.28-1.552 3.285-1.23 3.285-1.23.645 1.653.24 2.873.12 3.176.765.84 1.23 1.91 1.23 3.22 0 4.61-2.805 5.625-5.475 5.92.42.36.81 1.096.81 2.22 0 1.606-.015 2.896-.015 3.286 0 .315.21.69.825.57C20.565 22.092 24 17.592 24 12.297c0-6.627-5.373-12-12-12"></path></svg></a><a class="text-sm text-gray-500 transition hover:text-gray-600" target="_blank" rel="noopener noreferrer" href="https://facebook.com"><span class="sr-only">facebook</span><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" class="hover:text-primary-500 dark:hover:text-primary-400 fill-current text-gray-700 dark:text-gray-200 h-6 w-6"><title>Facebook</title><path d="M24 12.073c0-6.627-5.373-12-12-12s-12 5.373-12 12c0 5.99 4.388 10.954 10.125 11.854v-8.385H7.078v-3.47h3.047V9.43c0-3.007 1.792-4.669 4.533-4.669 1.312 0 2.686.235 2.686.235v2.953H15.83c-1.491 0-1.956.925-1.956 1.874v2.25h3.328l-.532 3.47h-2.796v8.385C19.612 23.027 24 18.062 24 12.073z"></path></svg></a><a class="text-sm text-gray-500 transition hover:text-gray-600" target="_blank" rel="noopener noreferrer" href="https://youtube.com"><span class="sr-only">youtube</span><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" class="hover:text-primary-500 dark:hover:text-primary-400 fill-current text-gray-700 dark:text-gray-200 h-6 w-6"><title>Youtube</title><path d="M23.499 6.203a3.008 3.008 0 00-2.089-2.089c-1.87-.501-9.4-.501-9.4-.501s-7.509-.01-9.399.501a3.008 3.008 0 00-2.088 2.09A31.258 31.26 0 000 12.01a31.258 31.26 0 00.523 5.785 3.008 3.008 0 002.088 2.089c1.869.502 9.4.502 9.4.502s7.508 0 9.399-.502a3.008 3.008 0 002.089-2.09 31.258 31.26 0 00.5-5.784 31.258 31.26 0 00-.5-5.808zm-13.891 9.4V8.407l6.266 3.604z"></path></svg></a><a class="text-sm text-gray-500 transition hover:text-gray-600" target="_blank" rel="noopener noreferrer" href="https://www.linkedin.com/in/yusheng-zheng-611920280"><span class="sr-only">linkedin</span><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" class="hover:text-primary-500 dark:hover:text-primary-400 fill-current text-gray-700 dark:text-gray-200 h-6 w-6"><title>Linkedin</title><path d="M20.447 20.452h-3.554v-5.569c0-1.328-.027-3.037-1.852-3.037-1.853 0-2.136 1.445-2.136 2.939v5.667H9.351V9h3.414v1.561h.046c.477-.9 1.637-1.85 3.37-1.85 3.601 0 4.267 2.37 4.267 5.455v6.286zM5.337 7.433a2.062 2.062 0 01-2.063-2.065 2.064 2.064 0 112.063 2.065zm1.782 13.019H3.555V9h3.564v11.452zM22.225 0H1.771C.792 0 0 .774 0 1.729v20.542C0 23.227.792 24 1.771 24h20.451C23.2 24 24 23.227 24 22.271V1.729C24 .774 23.2 0 22.222 0h.003z"></path></svg></a><a class="text-sm text-gray-500 transition hover:text-gray-600" target="_blank" rel="noopener noreferrer" href="https://bsky.app/"><span class="sr-only">bluesky</span><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" class="hover:text-primary-500 dark:hover:text-primary-400 fill-current text-gray-700 dark:text-gray-200 h-6 w-6"><title>Bluesky</title><path d="M12 10.8c-1.087-2.114-4.046-6.053-6.798-7.995C2.566.944 1.561 1.266.902 1.565C.139 1.908 0 3.08 0 3.768c0 .69.378 5.65.624 6.479c.815 2.736 3.713 3.66 6.383 3.364q.204-.03.415-.056q-.207.033-.415.056c-3.912.58-7.387 2.005-2.83 7.078c5.013 5.19 6.87-1.113 7.823-4.308c.953 3.195 2.05 9.271 7.733 4.308c4.267-4.308 1.172-6.498-2.74-7.078a9 9 0 0 1-.415-.056q.21.026.415.056c2.67.297 5.568-.628 6.383-3.364c.246-.828.624-5.79.624-6.478c0-.69-.139-1.861-.902-2.206c-.659-.298-1.664-.62-4.3 1.24C16.046 4.748 13.087 8.687 12 10.8"></path></svg></a><a class="text-sm text-gray-500 transition hover:text-gray-600" target="_blank" rel="noopener noreferrer" href="https://x.com/yunwei37"><span class="sr-only">x</span><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" class="hover:text-primary-500 dark:hover:text-primary-400 fill-current text-gray-700 dark:text-gray-200 h-6 w-6"><title>X</title><path d="M18.901 1.153h3.68l-8.04 9.19L24 22.846h-7.406l-5.8-7.584-6.638 7.584H.474l8.6-9.83L0 1.154h7.594l5.243 6.932ZM17.61 20.644h2.039L6.486 3.24H4.298Z"></path></svg></a><a class="text-sm text-gray-500 transition hover:text-gray-600" target="_blank" rel="noopener noreferrer" href="https://www.instagram.com"><span class="sr-only">instagram</span><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" class="hover:text-primary-500 dark:hover:text-primary-400 fill-current text-gray-700 dark:text-gray-200 h-6 w-6"><title>Instagram</title><path d="M12 0C8.74 0 8.333.015 7.053.072 5.775.132 4.905.333 4.14.63c-.789.306-1.459.717-2.126 1.384S.935 3.35.63 4.14C.333 4.905.131 5.775.072 7.053.012 8.333 0 8.74 0 12s.015 3.667.072 4.947c.06 1.277.261 2.148.558 2.913.306.788.717 1.459 1.384 2.126.667.666 1.336 1.079 2.126 1.384.766.296 1.636.499 2.913.558C8.333 23.988 8.74 24 12 24s3.667-.015 4.947-.072c1.277-.06 2.148-.262 2.913-.558.788-.306 1.459-.718 2.126-1.384.666-.667 1.079-1.335 1.384-2.126.296-.765.499-1.636.558-2.913.06-1.28.072-1.687.072-4.947s-.015-3.667-.072-4.947c-.06-1.277-.262-2.149-.558-2.913-.306-.789-.718-1.459-1.384-2.126C21.319 1.347 20.651.935 19.86.63c-.765-.297-1.636-.499-2.913-.558C15.667.012 15.26 0 12 0zm0 2.16c3.203 0 3.585.016 4.85.071 1.17.055 1.805.249 2.227.415.562.217.96.477 1.382.896.419.42.679.819.896 1.381.164.422.36 1.057.413 2.227.057 1.266.07 1.646.07 4.85s-.015 3.585-.074 4.85c-.061 1.17-.256 1.805-.421 2.227-.224.562-.479.96-.899 1.382-.419.419-.824.679-1.38.896-.42.164-1.065.36-2.235.413-1.274.057-1.649.07-4.859.07-3.211 0-3.586-.015-4.859-.074-1.171-.061-1.816-.256-2.236-.421-.569-.224-.96-.479-1.379-.899-.421-.419-.69-.824-.9-1.38-.165-.42-.359-1.065-.42-2.235-.045-1.26-.061-1.649-.061-4.844 0-3.196.016-3.586.061-4.861.061-1.17.255-1.814.42-2.234.21-.57.479-.96.9-1.381.419-.419.81-.689 1.379-.898.42-.166 1.051-.361 2.221-.421 1.275-.045 1.65-.06 4.859-.06l.045.03zm0 3.678c-3.405 0-6.162 2.76-6.162 6.162 0 3.405 2.76 6.162 6.162 6.162 3.405 0 6.162-2.76 6.162-6.162 0-3.405-2.76-6.162-6.162-6.162zM12 16c-2.21 0-4-1.79-4-4s1.79-4 4-4 4 1.79 4 4-1.79 4-4 4zm7.846-10.405c0 .795-.646 1.44-1.44 1.44-.795 0-1.44-.646-1.44-1.44 0-.794.646-1.439 1.44-1.439.793-.001 1.44.645 1.44 1.439z"></path></svg></a><a class="text-sm text-gray-500 transition hover:text-gray-600" target="_blank" rel="noopener noreferrer" href="https://www.threads.net"><span class="sr-only">threads</span><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" class="hover:text-primary-500 dark:hover:text-primary-400 fill-current text-gray-700 dark:text-gray-200 h-6 w-6"><title>Threads</title><path d="M12.186 24h-.007c-3.581-.024-6.334-1.205-8.184-3.509C2.35 18.44 1.5 15.586 1.472 12.01v-.017c.03-3.579.879-6.43 2.525-8.482C5.845 1.205 8.6.024 12.18 0h.014c2.746.02 5.043.725 6.826 2.098 1.677 1.29 2.858 3.13 3.509 5.467l-2.04.569c-1.104-3.96-3.898-5.984-8.304-6.015-2.91.022-5.11.936-6.54 2.717C4.307 6.504 3.616 8.914 3.589 12c.027 3.086.718 5.496 2.057 7.164 1.43 1.783 3.631 2.698 6.54 2.717 2.623-.02 4.358-.631 5.8-2.045 1.647-1.613 1.618-3.593 1.09-4.798-.31-.71-.873-1.3-1.634-1.75-.192 1.352-.622 2.446-1.284 3.272-.886 1.102-2.14 1.704-3.73 1.79-1.202.065-2.361-.218-3.259-.801-1.063-.689-1.685-1.74-1.752-2.964-.065-1.19.408-2.285 1.33-3.082.88-.76 2.119-1.207 3.583-1.291a13.853 13.853 0 0 1 3.02.142c-.126-.742-.375-1.332-.75-1.757-.513-.586-1.308-.883-2.359-.89h-.029c-.844 0-1.992.232-2.721 1.32L7.734 7.847c.98-1.454 2.568-2.256 4.478-2.256h.044c3.194.02 5.097 1.975 5.287 5.388.108.046.216.094.321.142 1.49.7 2.58 1.761 3.154 3.07.797 1.82.871 4.79-1.548 7.158-1.85 1.81-4.094 2.628-7.277 2.65Zm1.003-11.69c-.242 0-.487.007-.739.021-1.836.103-2.98.946-2.916 2.143.067 1.256 1.452 1.839 2.784 1.767 1.224-.065 2.818-.543 3.086-3.71a10.5 10.5 0 0 0-2.215-.221z"></path></svg></a><a class="text-sm text-gray-500 transition hover:text-gray-600" target="_blank" rel="noopener noreferrer" href="https://medium.com"><span class="sr-only">medium</span><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" class="hover:text-primary-500 dark:hover:text-primary-400 fill-current text-gray-700 dark:text-gray-200 h-6 w-6"><title>Medium</title><path d="M13.54 12a6.8 6.8 0 01-6.77 6.82A6.8 6.8 0 010 12a6.8 6.8 0 016.77-6.82A6.8 6.8 0 0113.54 12zM20.96 12c0 3.54-1.51 6.42-3.38 6.42-1.87 0-3.39-2.88-3.39-6.42s1.52-6.42 3.39-6.42 3.38 2.88 3.38 6.42M24 12c0 3.17-.53 5.75-1.19 5.75-.66 0-1.19-2.58-1.19-5.75s.53-5.75 1.19-5.75C23.47 6.25 24 8.83 24 12z"></path></svg></a></div><div class="flex space-x-2 text-sm opacity-80"><div>Yusheng Zheng</div><div> • </div><div>© 2026</div><div> • </div><a class="hover:text-primary-500 transition-colors" href="/">云微的胡思乱想</a></div><div class="text-sm opacity-60"><a class="hover:text-primary-500 transition-colors" target="_blank" rel="noopener noreferrer" href="https://github.com/timlrx/tailwind-nextjs-starter-blog">Tailwind Nextjs Theme</a></div></div></footer></div></div><script src="/_next/static/chunks/webpack-91cf1b3b084c6ba2.js" id="_R_" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:\"$Sreact.fragment\"\n2:I[1478,[\"874\",\"static/chunks/874-8edb22cc7428423c.js\",\"650\",\"static/chunks/650-5bb235cd9bfca45d.js\",\"177\",\"static/chunks/app/layout-66df3ac28fcfbced.js\"],\"ThemeProviders\"]\n3:I[4091,[\"874\",\"static/chunks/874-8edb22cc7428423c.js\",\"650\",\"static/chunks/650-5bb235cd9bfca45d.js\",\"177\",\"static/chunks/app/layout-66df3ac28fcfbced.js\"],\"default\"]\n4:I[9243,[\"874\",\"static/chunks/874-8edb22cc7428423c.js\",\"650\",\"static/chunks/650-5bb235cd9bfca45d.js\",\"177\",\"static/chunks/app/layout-66df3ac28fcfbced.js\"],\"\"]\n5:I[7392,[\"874\",\"static/chunks/874-8edb22cc7428423c.js\",\"650\",\"static/chunks/650-5bb235cd9bfca45d.js\",\"177\",\"static/chunks/app/layout-66df3ac28fcfbced.js\"],\"KBarSearchProvider\"]\n6:I[6874,[\"874\",\"static/chunks/874-8edb22cc7428423c.js\",\"63\",\"static/chunks/63-d245e42a784ca56d.js\",\"909\",\"static/chunks/app/blog/%5B...slug%5D/page-1ed2702378ba9d6b.js\"],\"\"]\ne:I[8393,[],\"\"]\n:HL[\"/_next/static/media/36966cca54120369-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n:HL[\"/_next/static/css/4ac7c0f872b74ee7.css\",\"style\"]\n:HL[\"/_next/static/css/a06d7e24bf9a7d93.css\",\"style\"]\n:HL[\"/_next/static/css/7246298b30c42979.css\",\"style\"]\n"])</script><script>self.__next_f.push([1,"0:{\"P\":null,\"b\":\"MYXrWUjg0HELsl6zLYF_6\",\"p\":\"\",\"c\":[\"\",\"blog\",\"gpu-profile-tools-analysis\"],\"i\":false,\"f\":[[[\"\",{\"children\":[\"blog\",{\"children\":[[\"slug\",\"gpu-profile-tools-analysis\",\"c\"],{\"children\":[\"__PAGE__\",{}]}]}]},\"$undefined\",\"$undefined\",true],[\"\",[\"$\",\"$1\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/4ac7c0f872b74ee7.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}],[\"$\",\"link\",\"1\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/a06d7e24bf9a7d93.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"en-us\",\"className\":\"__variable_dd5b2f scroll-smooth\",\"suppressHydrationWarning\":true,\"children\":[[\"$\",\"link\",null,{\"rel\":\"apple-touch-icon\",\"sizes\":\"76x76\",\"href\":\"/static/favicons/apple-touch-icon.png\"}],[\"$\",\"link\",null,{\"rel\":\"icon\",\"type\":\"image/svg+xml\",\"href\":\"/static/favicons/favicon.svg\"}],[\"$\",\"link\",null,{\"rel\":\"icon\",\"type\":\"image/png\",\"sizes\":\"32x32\",\"href\":\"/static/favicons/favicon-32x32.png\"}],[\"$\",\"link\",null,{\"rel\":\"icon\",\"type\":\"image/png\",\"sizes\":\"16x16\",\"href\":\"/static/favicons/favicon-16x16.png\"}],[\"$\",\"link\",null,{\"rel\":\"manifest\",\"href\":\"/static/favicons/site.webmanifest\"}],[\"$\",\"link\",null,{\"rel\":\"mask-icon\",\"href\":\"/static/favicons/safari-pinned-tab.svg\",\"color\":\"#5bbad5\"}],[\"$\",\"meta\",null,{\"name\":\"msapplication-TileColor\",\"content\":\"#000000\"}],[\"$\",\"meta\",null,{\"name\":\"theme-color\",\"media\":\"(prefers-color-scheme: light)\",\"content\":\"#fff\"}],[\"$\",\"meta\",null,{\"name\":\"theme-color\",\"media\":\"(prefers-color-scheme: dark)\",\"content\":\"#000\"}],[\"$\",\"link\",null,{\"rel\":\"alternate\",\"type\":\"application/rss+xml\",\"href\":\"/feed.xml\"}],[\"$\",\"body\",null,{\"className\":\"pl-[calc(100vw-100%)] text-black antialiased dark:text-white min-h-screen\",\"children\":[\"$\",\"$L2\",null,{\"children\":[[\"$\",\"$L3\",null,{}],[\"$undefined\",\"$undefined\",\"$undefined\",[\"$\",\"$L4\",null,{\"async\":true,\"defer\":true,\"src\":\"https://analytics.umami.is/script.js\"}],\"$undefined\",\"$undefined\"],[\"$\",\"div\",null,{\"className\":\"min-h-screen flex flex-col justify-center py-8\",\"children\":[\"$\",\"div\",null,{\"className\":\"content-glass mx-auto w-full max-w-5xl px-4 sm:px-6 xl:px-8 py-8\",\"children\":[\"$\",\"$L5\",null,{\"kbarConfig\":{\"searchDocumentsPath\":\"/search.json\"},\"children\":[[\"$\",\"header\",null,{\"className\":\"flex items-center w-full justify-between py-6 border-b border-gray-200/30 dark:border-gray-700/30\",\"children\":[[\"$\",\"$L6\",null,{\"className\":\"break-words\",\"href\":\"/\",\"aria-label\":\"yunwei37\",\"children\":[\"$\",\"div\",null,{\"className\":\"flex items-center\",\"children\":[[\"$\",\"div\",null,{\"className\":\"mr-3\",\"children\":[\"$\",\"svg\",null,{\"xmlns\":\"http://www.w3.org/2000/svg\",\"width\":40,\"height\":40,\"fill\":\"none\",\"children\":[[\"$\",\"defs\",null,{\"children\":[[\"$\",\"linearGradient\",null,{\"id\":\"logo_svg__a\",\"x1\":\"0%\",\"x2\":\"100%\",\"y1\":\"0%\",\"y2\":\"100%\",\"children\":[[\"$\",\"stop\",null,{\"offset\":\"0%\",\"style\":{\"stopColor\":\"#3b82f6\",\"stopOpacity\":1}}],[\"$\",\"stop\",null,{\"offset\":\"100%\",\"style\":{\"stopColor\":\"#06b6d4\",\"stopOpacity\":1}}]]}],[\"$\",\"linearGradient\",null,{\"id\":\"logo_svg__b\",\"x1\":\"0%\",\"x2\":\"100%\",\"y1\":\"0%\",\"y2\":\"100%\",\"children\":[[\"$\",\"stop\",null,{\"offset\":\"0%\",\"style\":{\"stopColor\":\"#8b5cf6\",\"stopOpacity\":1}}],[\"$\",\"stop\",null,{\"offset\":\"100%\",\"style\":{\"stopColor\":\"#3b82f6\",\"stopOpacity\":1}}]]}]]}],[\"$\",\"circle\",null,{\"cx\":20,\"cy\":20,\"r\":18,\"fill\":\"url(#logo_svg__a)\",\"opacity\":0.1}],[\"$\",\"path\",null,{\"fill\":\"url(#logo_svg__a)\",\"d\":\"m12 8 6 10v10h4V18l6-10h-4l-4 6-4-6Z\"}],[\"$\",\"path\",null,{\"fill\":\"url(#logo_svg__b)\",\"d\":\"m8 24 4 8 4-8 4 8 4-8h8v4h-6l-4 8-4-8-4 8-4-8H8Z\",\"opacity\":0.8}],[\"$\",\"circle\",null,{\"cx\":32,\"cy\":12,\"r\":2,\"fill\":\"url(#logo_svg__b)\"}],[\"$\",\"circle\",null,{\"cx\":8,\"cy\":12,\"r\":1.5,\"fill\":\"url(#logo_svg__a)\",\"opacity\":0.6}]]}]}],[\"$\",\"div\",null,{\"className\":\"hidden text-2xl font-bold sm:block hover:text-primary-600 dark:hover:text-primary-400 transition-colors\",\"children\":\"yunwei37\"}]]}]}],[\"$\",\"div\",null,{\"className\":\"flex items-center space-x-4\",\"children\":[[\"$\",\"div\",null,{\"className\":\"no-scrollbar hidden items-center space-x-2 sm:flex\",\"children\":[[\"$\",\"$L6\",\"Blog\",{\"className\":\"px-3 py-2 rounded-lg transition-all duration-200 font-medium hover:text-primary-600 dark:hover:text-primary-400\",\"href\":\"/blog\",\"children\":\"Blog\"}],[\"$\",\"$L6\",\"Docs\",{\"className\":\"px-3 py-2 rounded-lg transition-all duration-200 font-medium hover:text-primary-600 dark:hover:text-primary-400\",\"href\":\"/docs\",\"children\":\"Docs\"}],[\"$\",\"$L6\",\"Tags\",{\"className\":\"px-3 py-2 rounded-lg transition-all duration-200 font-medium hover:text-primary-600 dark:hover:text-primary-400\",\"href\":\"/tags\",\"children\":\"Tags\"}],[\"$\",\"$L6\",\"About\",{\"className\":\"px-3 py-2 rounded-lg transition-all duration-200 font-medium hover:text-primary-600 dark:hover:text-primary-400\",\"href\":\"/about\",\"children\":\"About\"}]]}],\"$L7\"]}]]}],\"$L8\",\"$L9\"]}]}]}]]}]}]]}]]}],{\"children\":[\"blog\",\"$La\",{\"children\":[[\"slug\",\"gpu-profile-tools-analysis\",\"c\"],\"$Lb\",{\"children\":[\"__PAGE__\",\"$Lc\",{},null,false]},null,false]},null,false]},null,false],\"$Ld\",false]],\"m\":\"$undefined\",\"G\":[\"$e\",[]],\"s\":false,\"S\":true}\n"])</script><script>self.__next_f.push([1,"f:I[4159,[\"874\",\"static/chunks/874-8edb22cc7428423c.js\",\"650\",\"static/chunks/650-5bb235cd9bfca45d.js\",\"177\",\"static/chunks/app/layout-66df3ac28fcfbced.js\"],\"KBarButton\"]\n10:I[1762,[\"874\",\"static/chunks/874-8edb22cc7428423c.js\",\"650\",\"static/chunks/650-5bb235cd9bfca45d.js\",\"177\",\"static/chunks/app/layout-66df3ac28fcfbced.js\"],\"default\"]\n11:I[7298,[\"874\",\"static/chunks/874-8edb22cc7428423c.js\",\"650\",\"static/chunks/650-5bb235cd9bfca45d.js\",\"177\",\"static/chunks/app/layout-66df3ac28fcfbced.js\"],\"default\"]\n12:I[7555,[],\"\"]\n13:I[1295,[],\"\"]\n1e:I[9665,[],\"OutletBoundary\"]\n20:I[4911,[],\"AsyncMetadataOutlet\"]\n22:I[9665,[],\"ViewportBoundary\"]\n24:I[9665,[],\"MetadataBoundary\"]\n25:\"$Sreact.suspense\"\n7:[\"$\",\"div\",null,{\"className\":\"flex items-center space-x-2\",\"children\":[[\"$\",\"$Lf\",null,{\"aria-label\":\"Search\",\"className\":\"p-2 rounded-lg transition-all duration-200\",\"children\":[\"$\",\"svg\",null,{\"xmlns\":\"http://www.w3.org/2000/svg\",\"fill\":\"none\",\"viewBox\":\"0 0 24 24\",\"strokeWidth\":1.5,\"stroke\":\"currentColor\",\"className\":\"h-6 w-6 hover:text-primary-600 dark:hover:text-primary-400 transition-colors\",\"children\":[\"$\",\"path\",null,{\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"d\":\"M21 21l-5.197-5.197m0 0A7.5 7.5 0 105.196 5.196a7.5 7.5 0 0010.607 10.607z\"}]}]}],[\"$\",\"$L10\",null,{}],[\"$\",\"$L11\",null,{}]]}]\n"])</script><script>self.__next_f.push([1,"8:[\"$\",\"main\",null,{\"className\":\"py-6\",\"children\":[\"$\",\"$L12\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L13\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[\"$\",\"div\",null,{\"className\":\"flex flex-col items-start justify-start md:mt-24 md:flex-row md:items-center md:justify-center md:space-x-6\",\"children\":[[\"$\",\"div\",null,{\"className\":\"space-x-2 pt-6 pb-8 md:space-y-5\",\"children\":[\"$\",\"h1\",null,{\"className\":\"text-6xl leading-9 font-extrabold tracking-tight md:border-r-2 md:px-6 md:text-8xl md:leading-14\",\"children\":\"404\"}]}],[\"$\",\"div\",null,{\"className\":\"max-w-md\",\"children\":[[\"$\",\"p\",null,{\"className\":\"mb-4 text-xl leading-normal font-bold md:text-2xl\",\"children\":\"Sorry we couldn't find this page.\"}],[\"$\",\"p\",null,{\"className\":\"mb-8\",\"children\":\"But dont worry, you can find plenty of other things on our homepage.\"}],[\"$\",\"$L6\",null,{\"className\":\"focus:shadow-outline-blue inline rounded-lg border border-transparent bg-blue-600 px-4 py-2 text-sm leading-5 font-medium text-white shadow-xs transition-colors duration-150 hover:bg-blue-700 focus:outline-hidden dark:hover:bg-blue-500\",\"href\":\"/\",\"children\":\"Back to homepage\"}]]}]]}],[]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]}]\n"])</script><script>self.__next_f.push([1,"9:[\"$\",\"footer\",null,{\"className\":\"border-t border-gray-200/30 dark:border-gray-700/30 pt-8\",\"children\":[\"$\",\"div\",null,{\"className\":\"flex flex-col items-center space-y-6\",\"children\":[[\"$\",\"div\",null,{\"className\":\"flex flex-wrap justify-center gap-4\",\"children\":[[\"$\",\"a\",null,{\"className\":\"text-sm text-gray-500 transition hover:text-gray-600\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"href\":\"mailto:yunwei356@gmail.com\",\"children\":[[\"$\",\"span\",null,{\"className\":\"sr-only\",\"children\":\"mail\"}],[\"$\",\"svg\",null,{\"xmlns\":\"http://www.w3.org/2000/svg\",\"viewBox\":\"0 0 20 20\",\"className\":\"hover:text-primary-500 dark:hover:text-primary-400 fill-current text-gray-700 dark:text-gray-200 h-6 w-6\",\"children\":[[\"$\",\"title\",null,{\"children\":\"Mail\"}],[\"$\",\"path\",null,{\"d\":\"M2.003 5.884L10 9.882l7.997-3.998A2 2 0 0016 4H4a2 2 0 00-1.997 1.884z\"}],[\"$\",\"path\",null,{\"d\":\"M18 8.118l-8 4-8-4V14a2 2 0 002 2h12a2 2 0 002-2V8.118z\"}]]}]]}],[\"$\",\"a\",null,{\"className\":\"text-sm text-gray-500 transition hover:text-gray-600\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"href\":\"https://github.com/yunwei37\",\"children\":[[\"$\",\"span\",null,{\"className\":\"sr-only\",\"children\":\"github\"}],[\"$\",\"svg\",null,{\"xmlns\":\"http://www.w3.org/2000/svg\",\"viewBox\":\"0 0 24 24\",\"className\":\"hover:text-primary-500 dark:hover:text-primary-400 fill-current text-gray-700 dark:text-gray-200 h-6 w-6\",\"children\":[[\"$\",\"title\",null,{\"children\":\"GitHub\"}],[\"$\",\"path\",null,{\"d\":\"M12 .297c-6.63 0-12 5.373-12 12 0 5.303 3.438 9.8 8.205 11.385.6.113.82-.258.82-.577 0-.285-.01-1.04-.015-2.04-3.338.724-4.042-1.61-4.042-1.61C4.422 18.07 3.633 17.7 3.633 17.7c-1.087-.744.084-.729.084-.729 1.205.084 1.838 1.236 1.838 1.236 1.07 1.835 2.809 1.305 3.495.998.108-.776.417-1.305.76-1.605-2.665-.3-5.466-1.332-5.466-5.93 0-1.31.465-2.38 1.235-3.22-.135-.303-.54-1.523.105-3.176 0 0 1.005-.322 3.3 1.23.96-.267 1.98-.399 3-.405 1.02.006 2.04.138 3 .405 2.28-1.552 3.285-1.23 3.285-1.23.645 1.653.24 2.873.12 3.176.765.84 1.23 1.91 1.23 3.22 0 4.61-2.805 5.625-5.475 5.92.42.36.81 1.096.81 2.22 0 1.606-.015 2.896-.015 3.286 0 .315.21.69.825.57C20.565 22.092 24 17.592 24 12.297c0-6.627-5.373-12-12-12\"}]]}]]}],[\"$\",\"a\",null,{\"className\":\"text-sm text-gray-500 transition hover:text-gray-600\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"href\":\"https://facebook.com\",\"children\":[[\"$\",\"span\",null,{\"className\":\"sr-only\",\"children\":\"facebook\"}],[\"$\",\"svg\",null,{\"xmlns\":\"http://www.w3.org/2000/svg\",\"viewBox\":\"0 0 24 24\",\"className\":\"hover:text-primary-500 dark:hover:text-primary-400 fill-current text-gray-700 dark:text-gray-200 h-6 w-6\",\"children\":[[\"$\",\"title\",null,{\"children\":\"Facebook\"}],[\"$\",\"path\",null,{\"d\":\"M24 12.073c0-6.627-5.373-12-12-12s-12 5.373-12 12c0 5.99 4.388 10.954 10.125 11.854v-8.385H7.078v-3.47h3.047V9.43c0-3.007 1.792-4.669 4.533-4.669 1.312 0 2.686.235 2.686.235v2.953H15.83c-1.491 0-1.956.925-1.956 1.874v2.25h3.328l-.532 3.47h-2.796v8.385C19.612 23.027 24 18.062 24 12.073z\"}]]}]]}],[\"$\",\"a\",null,{\"className\":\"text-sm text-gray-500 transition hover:text-gray-600\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"href\":\"https://youtube.com\",\"children\":[[\"$\",\"span\",null,{\"className\":\"sr-only\",\"children\":\"youtube\"}],[\"$\",\"svg\",null,{\"xmlns\":\"http://www.w3.org/2000/svg\",\"viewBox\":\"0 0 24 24\",\"className\":\"hover:text-primary-500 dark:hover:text-primary-400 fill-current text-gray-700 dark:text-gray-200 h-6 w-6\",\"children\":[[\"$\",\"title\",null,{\"children\":\"Youtube\"}],[\"$\",\"path\",null,{\"d\":\"M23.499 6.203a3.008 3.008 0 00-2.089-2.089c-1.87-.501-9.4-.501-9.4-.501s-7.509-.01-9.399.501a3.008 3.008 0 00-2.088 2.09A31.258 31.26 0 000 12.01a31.258 31.26 0 00.523 5.785 3.008 3.008 0 002.088 2.089c1.869.502 9.4.502 9.4.502s7.508 0 9.399-.502a3.008 3.008 0 002.089-2.09 31.258 31.26 0 00.5-5.784 31.258 31.26 0 00-.5-5.808zm-13.891 9.4V8.407l6.266 3.604z\"}]]}]]}],\"$L14\",\"$L15\",\"$L16\",\"$L17\",\"$L18\",\"$L19\",\"$L1a\"]}],\"$L1b\",\"$L1c\"]}]}]\n"])</script><script>self.__next_f.push([1,"a:[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L12\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L13\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}]\nb:[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L12\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L13\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}]\nc:[\"$\",\"$1\",\"c\",{\"children\":[\"$L1d\",[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/7246298b30c42979.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]],[\"$\",\"$L1e\",null,{\"children\":[\"$L1f\",[\"$\",\"$L20\",null,{\"promise\":\"$@21\"}]]}]]}]\nd:[\"$\",\"$1\",\"h\",{\"children\":[null,[[\"$\",\"$L22\",null,{\"children\":\"$L23\"}],[\"$\",\"meta\",null,{\"name\":\"next-size-adjust\",\"content\":\"\"}]],[\"$\",\"$L24\",null,{\"children\":[\"$\",\"div\",null,{\"hidden\":true,\"children\":[\"$\",\"$25\",null,{\"fallback\":null,\"children\":\"$L26\"}]}]}]]}]\n"])</script><script>self.__next_f.push([1,"14:[\"$\",\"a\",null,{\"className\":\"text-sm text-gray-500 transition hover:text-gray-600\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"href\":\"https://www.linkedin.com/in/yusheng-zheng-611920280\",\"children\":[[\"$\",\"span\",null,{\"className\":\"sr-only\",\"children\":\"linkedin\"}],[\"$\",\"svg\",null,{\"xmlns\":\"http://www.w3.org/2000/svg\",\"viewBox\":\"0 0 24 24\",\"className\":\"hover:text-primary-500 dark:hover:text-primary-400 fill-current text-gray-700 dark:text-gray-200 h-6 w-6\",\"children\":[[\"$\",\"title\",null,{\"children\":\"Linkedin\"}],[\"$\",\"path\",null,{\"d\":\"M20.447 20.452h-3.554v-5.569c0-1.328-.027-3.037-1.852-3.037-1.853 0-2.136 1.445-2.136 2.939v5.667H9.351V9h3.414v1.561h.046c.477-.9 1.637-1.85 3.37-1.85 3.601 0 4.267 2.37 4.267 5.455v6.286zM5.337 7.433a2.062 2.062 0 01-2.063-2.065 2.064 2.064 0 112.063 2.065zm1.782 13.019H3.555V9h3.564v11.452zM22.225 0H1.771C.792 0 0 .774 0 1.729v20.542C0 23.227.792 24 1.771 24h20.451C23.2 24 24 23.227 24 22.271V1.729C24 .774 23.2 0 22.222 0h.003z\"}]]}]]}]\n"])</script><script>self.__next_f.push([1,"15:null\n"])</script><script>self.__next_f.push([1,"16:[\"$\",\"a\",null,{\"className\":\"text-sm text-gray-500 transition hover:text-gray-600\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"href\":\"https://bsky.app/\",\"children\":[[\"$\",\"span\",null,{\"className\":\"sr-only\",\"children\":\"bluesky\"}],[\"$\",\"svg\",null,{\"xmlns\":\"http://www.w3.org/2000/svg\",\"viewBox\":\"0 0 24 24\",\"className\":\"hover:text-primary-500 dark:hover:text-primary-400 fill-current text-gray-700 dark:text-gray-200 h-6 w-6\",\"children\":[[\"$\",\"title\",null,{\"children\":\"Bluesky\"}],[\"$\",\"path\",null,{\"d\":\"M12 10.8c-1.087-2.114-4.046-6.053-6.798-7.995C2.566.944 1.561 1.266.902 1.565C.139 1.908 0 3.08 0 3.768c0 .69.378 5.65.624 6.479c.815 2.736 3.713 3.66 6.383 3.364q.204-.03.415-.056q-.207.033-.415.056c-3.912.58-7.387 2.005-2.83 7.078c5.013 5.19 6.87-1.113 7.823-4.308c.953 3.195 2.05 9.271 7.733 4.308c4.267-4.308 1.172-6.498-2.74-7.078a9 9 0 0 1-.415-.056q.21.026.415.056c2.67.297 5.568-.628 6.383-3.364c.246-.828.624-5.79.624-6.478c0-.69-.139-1.861-.902-2.206c-.659-.298-1.664-.62-4.3 1.24C16.046 4.748 13.087 8.687 12 10.8\"}]]}]]}]\n"])</script><script>self.__next_f.push([1,"17:[\"$\",\"a\",null,{\"className\":\"text-sm text-gray-500 transition hover:text-gray-600\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"href\":\"https://x.com/yunwei37\",\"children\":[[\"$\",\"span\",null,{\"className\":\"sr-only\",\"children\":\"x\"}],[\"$\",\"svg\",null,{\"xmlns\":\"http://www.w3.org/2000/svg\",\"viewBox\":\"0 0 24 24\",\"className\":\"hover:text-primary-500 dark:hover:text-primary-400 fill-current text-gray-700 dark:text-gray-200 h-6 w-6\",\"children\":[[\"$\",\"title\",null,{\"children\":\"X\"}],[\"$\",\"path\",null,{\"d\":\"M18.901 1.153h3.68l-8.04 9.19L24 22.846h-7.406l-5.8-7.584-6.638 7.584H.474l8.6-9.83L0 1.154h7.594l5.243 6.932ZM17.61 20.644h2.039L6.486 3.24H4.298Z\"}]]}]]}]\n27:T69f,"])</script><script>self.__next_f.push([1,"M12 0C8.74 0 8.333.015 7.053.072 5.775.132 4.905.333 4.14.63c-.789.306-1.459.717-2.126 1.384S.935 3.35.63 4.14C.333 4.905.131 5.775.072 7.053.012 8.333 0 8.74 0 12s.015 3.667.072 4.947c.06 1.277.261 2.148.558 2.913.306.788.717 1.459 1.384 2.126.667.666 1.336 1.079 2.126 1.384.766.296 1.636.499 2.913.558C8.333 23.988 8.74 24 12 24s3.667-.015 4.947-.072c1.277-.06 2.148-.262 2.913-.558.788-.306 1.459-.718 2.126-1.384.666-.667 1.079-1.335 1.384-2.126.296-.765.499-1.636.558-2.913.06-1.28.072-1.687.072-4.947s-.015-3.667-.072-4.947c-.06-1.277-.262-2.149-.558-2.913-.306-.789-.718-1.459-1.384-2.126C21.319 1.347 20.651.935 19.86.63c-.765-.297-1.636-.499-2.913-.558C15.667.012 15.26 0 12 0zm0 2.16c3.203 0 3.585.016 4.85.071 1.17.055 1.805.249 2.227.415.562.217.96.477 1.382.896.419.42.679.819.896 1.381.164.422.36 1.057.413 2.227.057 1.266.07 1.646.07 4.85s-.015 3.585-.074 4.85c-.061 1.17-.256 1.805-.421 2.227-.224.562-.479.96-.899 1.382-.419.419-.824.679-1.38.896-.42.164-1.065.36-2.235.413-1.274.057-1.649.07-4.859.07-3.211 0-3.586-.015-4.859-.074-1.171-.061-1.816-.256-2.236-.421-.569-.224-.96-.479-1.379-.899-.421-.419-.69-.824-.9-1.38-.165-.42-.359-1.065-.42-2.235-.045-1.26-.061-1.649-.061-4.844 0-3.196.016-3.586.061-4.861.061-1.17.255-1.814.42-2.234.21-.57.479-.96.9-1.381.419-.419.81-.689 1.379-.898.42-.166 1.051-.361 2.221-.421 1.275-.045 1.65-.06 4.859-.06l.045.03zm0 3.678c-3.405 0-6.162 2.76-6.162 6.162 0 3.405 2.76 6.162 6.162 6.162 3.405 0 6.162-2.76 6.162-6.162 0-3.405-2.76-6.162-6.162-6.162zM12 16c-2.21 0-4-1.79-4-4s1.79-4 4-4 4 1.79 4 4-1.79 4-4 4zm7.846-10.405c0 .795-.646 1.44-1.44 1.44-.795 0-1.44-.646-1.44-1.44 0-.794.646-1.439 1.44-1.439.793-.001 1.44.645 1.44 1.439z"])</script><script>self.__next_f.push([1,"18:[\"$\",\"a\",null,{\"className\":\"text-sm text-gray-500 transition hover:text-gray-600\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"href\":\"https://www.instagram.com\",\"children\":[[\"$\",\"span\",null,{\"className\":\"sr-only\",\"children\":\"instagram\"}],[\"$\",\"svg\",null,{\"xmlns\":\"http://www.w3.org/2000/svg\",\"viewBox\":\"0 0 24 24\",\"className\":\"hover:text-primary-500 dark:hover:text-primary-400 fill-current text-gray-700 dark:text-gray-200 h-6 w-6\",\"children\":[[\"$\",\"title\",null,{\"children\":\"Instagram\"}],[\"$\",\"path\",null,{\"d\":\"$27\"}]]}]]}]\n28:T498,"])</script><script>self.__next_f.push([1,"M12.186 24h-.007c-3.581-.024-6.334-1.205-8.184-3.509C2.35 18.44 1.5 15.586 1.472 12.01v-.017c.03-3.579.879-6.43 2.525-8.482C5.845 1.205 8.6.024 12.18 0h.014c2.746.02 5.043.725 6.826 2.098 1.677 1.29 2.858 3.13 3.509 5.467l-2.04.569c-1.104-3.96-3.898-5.984-8.304-6.015-2.91.022-5.11.936-6.54 2.717C4.307 6.504 3.616 8.914 3.589 12c.027 3.086.718 5.496 2.057 7.164 1.43 1.783 3.631 2.698 6.54 2.717 2.623-.02 4.358-.631 5.8-2.045 1.647-1.613 1.618-3.593 1.09-4.798-.31-.71-.873-1.3-1.634-1.75-.192 1.352-.622 2.446-1.284 3.272-.886 1.102-2.14 1.704-3.73 1.79-1.202.065-2.361-.218-3.259-.801-1.063-.689-1.685-1.74-1.752-2.964-.065-1.19.408-2.285 1.33-3.082.88-.76 2.119-1.207 3.583-1.291a13.853 13.853 0 0 1 3.02.142c-.126-.742-.375-1.332-.75-1.757-.513-.586-1.308-.883-2.359-.89h-.029c-.844 0-1.992.232-2.721 1.32L7.734 7.847c.98-1.454 2.568-2.256 4.478-2.256h.044c3.194.02 5.097 1.975 5.287 5.388.108.046.216.094.321.142 1.49.7 2.58 1.761 3.154 3.07.797 1.82.871 4.79-1.548 7.158-1.85 1.81-4.094 2.628-7.277 2.65Zm1.003-11.69c-.242 0-.487.007-.739.021-1.836.103-2.98.946-2.916 2.143.067 1.256 1.452 1.839 2.784 1.767 1.224-.065 2.818-.543 3.086-3.71a10.5 10.5 0 0 0-2.215-.221z"])</script><script>self.__next_f.push([1,"19:[\"$\",\"a\",null,{\"className\":\"text-sm text-gray-500 transition hover:text-gray-600\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"href\":\"https://www.threads.net\",\"children\":[[\"$\",\"span\",null,{\"className\":\"sr-only\",\"children\":\"threads\"}],[\"$\",\"svg\",null,{\"xmlns\":\"http://www.w3.org/2000/svg\",\"viewBox\":\"0 0 24 24\",\"className\":\"hover:text-primary-500 dark:hover:text-primary-400 fill-current text-gray-700 dark:text-gray-200 h-6 w-6\",\"children\":[[\"$\",\"title\",null,{\"children\":\"Threads\"}],[\"$\",\"path\",null,{\"d\":\"$28\"}]]}]]}]\n"])</script><script>self.__next_f.push([1,"1a:[\"$\",\"a\",null,{\"className\":\"text-sm text-gray-500 transition hover:text-gray-600\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"href\":\"https://medium.com\",\"children\":[[\"$\",\"span\",null,{\"className\":\"sr-only\",\"children\":\"medium\"}],[\"$\",\"svg\",null,{\"xmlns\":\"http://www.w3.org/2000/svg\",\"viewBox\":\"0 0 24 24\",\"className\":\"hover:text-primary-500 dark:hover:text-primary-400 fill-current text-gray-700 dark:text-gray-200 h-6 w-6\",\"children\":[[\"$\",\"title\",null,{\"children\":\"Medium\"}],[\"$\",\"path\",null,{\"d\":\"M13.54 12a6.8 6.8 0 01-6.77 6.82A6.8 6.8 0 010 12a6.8 6.8 0 016.77-6.82A6.8 6.8 0 0113.54 12zM20.96 12c0 3.54-1.51 6.42-3.38 6.42-1.87 0-3.39-2.88-3.39-6.42s1.52-6.42 3.39-6.42 3.38 2.88 3.38 6.42M24 12c0 3.17-.53 5.75-1.19 5.75-.66 0-1.19-2.58-1.19-5.75s.53-5.75 1.19-5.75C23.47 6.25 24 8.83 24 12z\"}]]}]]}]\n"])</script><script>self.__next_f.push([1,"1b:[\"$\",\"div\",null,{\"className\":\"flex space-x-2 text-sm opacity-80\",\"children\":[[\"$\",\"div\",null,{\"children\":\"Yusheng Zheng\"}],[\"$\",\"div\",null,{\"children\":\" • \"}],[\"$\",\"div\",null,{\"children\":\"© 2026\"}],[\"$\",\"div\",null,{\"children\":\" • \"}],[\"$\",\"$L6\",null,{\"className\":\"hover:text-primary-500 transition-colors\",\"href\":\"/\",\"children\":\"云微的胡思乱想\"}]]}]\n1c:[\"$\",\"div\",null,{\"className\":\"text-sm opacity-60\",\"children\":[\"$\",\"a\",null,{\"className\":\"hover:text-primary-500 transition-colors\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"href\":\"https://github.com/timlrx/tailwind-nextjs-starter-blog\",\"children\":\"Tailwind Nextjs Theme\"}]}]\n"])</script><script>self.__next_f.push([1,"29:I[1839,[\"874\",\"static/chunks/874-8edb22cc7428423c.js\",\"63\",\"static/chunks/63-d245e42a784ca56d.js\",\"909\",\"static/chunks/app/blog/%5B...slug%5D/page-1ed2702378ba9d6b.js\"],\"default\"]\n2a:I[3063,[\"874\",\"static/chunks/874-8edb22cc7428423c.js\",\"63\",\"static/chunks/63-d245e42a784ca56d.js\",\"909\",\"static/chunks/app/blog/%5B...slug%5D/page-1ed2702378ba9d6b.js\"],\"Image\"]\n"])</script><script>self.__next_f.push([1,"1d:[[\"$\",\"script\",null,{\"type\":\"application/ld+json\",\"dangerouslySetInnerHTML\":{\"__html\":\"{\\\"@context\\\":\\\"https://schema.org\\\",\\\"@type\\\":\\\"BlogPosting\\\",\\\"headline\\\":\\\"The Accelerator Toolkit: A Review of Profiling and Tracing for GPUs and other co-processor\\\",\\\"datePublished\\\":\\\"2025-04-11T16:00:00.000Z\\\",\\\"dateModified\\\":\\\"2025-04-11T16:00:00.000Z\\\",\\\"image\\\":\\\"/static/images/twitter-card.png\\\",\\\"url\\\":\\\"https://www.yunwei37.com/blog/gpu-profile-tools-analysis\\\",\\\"author\\\":[{\\\"@type\\\":\\\"Person\\\",\\\"name\\\":\\\"Yusheng Zheng (云微)\\\"}]}\"}}],[\"$\",\"section\",null,{\"className\":\"mx-auto max-w-3xl px-4 sm:px-6 xl:max-w-5xl xl:px-0\",\"children\":[[\"$\",\"$L29\",null,{}],[\"$\",\"article\",null,{\"className\":\"content-glass p-8\",\"children\":[\"$\",\"div\",null,{\"className\":\"xl:divide-y xl:divide-gray-200/30 xl:dark:divide-gray-700/30\",\"children\":[[\"$\",\"header\",null,{\"className\":\"pt-6 xl:pb-6\",\"children\":[\"$\",\"div\",null,{\"className\":\"space-y-4 text-center\",\"children\":[[\"$\",\"dl\",null,{\"className\":\"space-y-4\",\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"dt\",null,{\"className\":\"sr-only\",\"children\":\"Published on\"}],[\"$\",\"dd\",null,{\"className\":\"text-base leading-6 font-medium text-gray-600 dark:text-gray-300\",\"children\":[\"$\",\"time\",null,{\"dateTime\":\"2025-04-11T16:00:00.000Z\",\"children\":\"Friday, April 11, 2025\"}]}]]}]}],[\"$\",\"div\",null,{\"children\":[\"$\",\"h1\",null,{\"className\":\"text-3xl leading-9 font-extrabold tracking-tight sm:text-4xl sm:leading-10 md:text-5xl md:leading-14\",\"children\":\"The Accelerator Toolkit: A Review of Profiling and Tracing for GPUs and other co-processor\"}]}]]}]}],[\"$\",\"div\",null,{\"className\":\"grid-rows-[auto_1fr] divide-y divide-gray-200/30 pb-8 xl:grid xl:grid-cols-4 xl:gap-x-8 xl:divide-y-0 dark:divide-gray-700/30\",\"children\":[[\"$\",\"dl\",null,{\"className\":\"pt-6 pb-10 xl:border-b xl:border-gray-200/30 xl:pt-11 xl:dark:border-gray-700/30\",\"children\":[[\"$\",\"dt\",null,{\"className\":\"sr-only\",\"children\":\"Authors\"}],[\"$\",\"dd\",null,{\"children\":[\"$\",\"ul\",null,{\"className\":\"flex flex-wrap justify-center gap-4 sm:space-x-12 xl:block xl:space-y-6 xl:space-x-0\",\"children\":[[\"$\",\"li\",\"Yusheng Zheng (云微)\",{\"className\":\"flex items-center space-x-3 glass p-4 rounded-xl\",\"children\":[[\"$\",\"$L2a\",null,{\"src\":\"/static/images/avatar.png\",\"width\":38,\"height\":38,\"alt\":\"avatar\",\"className\":\"h-10 w-10 rounded-full\"}],[\"$\",\"dl\",null,{\"className\":\"text-sm leading-5 font-medium whitespace-nowrap\",\"children\":[[\"$\",\"dt\",null,{\"className\":\"sr-only\",\"children\":\"Name\"}],[\"$\",\"dd\",null,{\"className\":\"text-gray-900 dark:text-gray-100\",\"children\":\"Yusheng Zheng (云微)\"}],[\"$\",\"dt\",null,{\"className\":\"sr-only\",\"children\":\"Twitter\"}],[\"$\",\"dd\",null,{\"children\":[\"$\",\"a\",null,{\"className\":\"text-primary-600 hover:text-primary-700 dark:text-primary-400 dark:hover:text-primary-300 transition-colors\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"href\":\"https://twitter.com/yunwei37\",\"children\":\"@yunwei37\"}]}]]}]]}]]}]}]]}],[\"$\",\"div\",null,{\"className\":\"divide-y divide-gray-200/30 xl:col-span-3 xl:row-span-2 xl:pb-0 dark:divide-gray-700/30\",\"children\":[[\"$\",\"div\",null,{\"className\":\"prose dark:prose-invert max-w-none pt-10 pb-8 text-gray-700 dark:text-gray-200\",\"children\":[[\"$\",\"h1\",null,{\"className\":\"content-header\",\"id\":\"the-accelerator-toolkit-a-review-of-profiling-and-tracing-for-gpus-and-other-co-processor\",\"children\":[[\"$\",\"a\",null,{\"className\":\"break-words\",\"href\":\"#the-accelerator-toolkit-a-review-of-profiling-and-tracing-for-gpus-and-other-co-processor\",\"aria-hidden\":\"true\",\"tabIndex\":\"-1\",\"children\":[\"$\",\"span\",null,{\"className\":\"content-header-link\",\"children\":[\"$\",\"svg\",null,{\"className\":\"h-5 linkicon w-5\",\"fill\":\"currentColor\",\"viewBox\":\"0 0 20 20\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"children\":[[\"$\",\"path\",null,{\"d\":\"M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z\"}],[\"$\",\"path\",null,{\"d\":\"M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z\"}]]}]}]}],\"The Accelerator Toolkit: A Review of Profiling and Tracing for GPUs and other co-processor\"]}],\"$L2b\",\"$L2c\",\"$L2d\",\"$L2e\",\"$L2f\",\"$L30\",\"$L31\",\"$L32\",\"$L33\",\"$L34\",\"$L35\",\"$L36\",\"$L37\",\"$L38\",\"$L39\",\"$L3a\",\"$L3b\",\"$L3c\",\"$L3d\",\"$L3e\",\"$L3f\",\"$L40\",\"$L41\",\"$L42\",\"$L43\",\"$L44\",\"$L45\",\"$L46\",\"$L47\",\"$L48\",\"$L49\",\"$L4a\",\"$L4b\",\"$L4c\",\"$L4d\",\"$L4e\",\"$L4f\",\"$L50\",\"$L51\",\"$L52\",\"$L53\",\"$L54\",\"$L55\",\"$L56\",\"$L57\",\"$L58\",\"$L59\",\"$L5a\"]}],\"$L5b\",\"$L5c\"]}],\"$L5d\"]}]]}]}]]}]]\n"])</script><script>self.__next_f.push([1,"69:I[1449,[\"874\",\"static/chunks/874-8edb22cc7428423c.js\",\"63\",\"static/chunks/63-d245e42a784ca56d.js\",\"909\",\"static/chunks/app/blog/%5B...slug%5D/page-1ed2702378ba9d6b.js\"],\"default\"]\n"])</script><script>self.__next_f.push([1,"2b:[\"$\",\"p\",null,{\"children\":[\"Modern computing increasingly relies on specialized accelerators – notably \",[\"$\",\"strong\",null,{\"children\":\"GPUs\"}],\", \",[\"$\",\"strong\",null,{\"children\":\"DPUs\"}],\", and \",[\"$\",\"strong\",null,{\"children\":\"APUs\"}],\" – to handle diverse workloads. A \",[\"$\",\"em\",null,{\"children\":\"graphics processing unit (GPU)\"}],\" is a massively parallel processor originally for graphics, now essential in general-purpose computing (HPC) and AI. A \",[\"$\",\"em\",null,{\"children\":\"data processing unit (DPU)\"}],\" is a newer class of programmable processor combining CPU cores with high-performance network/storage engines. DPUs offload networking, security, and storage tasks from the CPU, and are considered the \\\"third pillar\\\" of computing alongside CPUs and GPUs. Meanwhile, \",[\"$\",\"em\",null,{\"children\":\"accelerated processing units (APUs)\"}],\" integrate CPU and GPU components on one chip – an approach pioneered by AMD's Fusion architecture – enabling unified memory and high throughput for HPC and AI workloads. These accelerators run a range of workloads: GPUs excel in parallel math (HPC simulation, deep learning training/inference, data analytics) and rendering graphics; DPUs focus on data-centric tasks (network packet processing, encryption, storage offload, virtualization); and APUs target heterogeneous workloads needing tight CPU-GPU coupling (e.g. sharing memory for AI or multimedia applications).\"]}]\n"])</script><script>self.__next_f.push([1,"2c:[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Profiling and tracing\"}],\" tools are crucial for optimizing performance on these accelerators. Such tools collect \",[\"$\",\"strong\",null,{\"children\":\"low-level hardware telemetry\"}],\" (e.g. counters for utilization, memory throughput, SM occupancy, cache misses) and can perform \",[\"$\",\"strong\",null,{\"children\":\"instruction or event-level tracing\"}],\" (capturing timelines of kernel executions, memory copies, network packet flows, etc.). The goal is to identify bottlenecks and inefficiencies in both \",[\"$\",\"em\",null,{\"children\":\"general-purpose\"}],\" code and \",[\"$\",\"em\",null,{\"children\":\"domain-specific\"}],\" pipelines (like ML model training, network function processing, or graphics rendering). However, profiling highly parallel, heterogeneous systems presents challenges of overhead, data volume, and cross-platform compatibility. This review categorizes current tracing/profiling tools by hardware type and domain, compares open-source and commercial solutions, and highlights major projects and recent research. We also discuss specialized toolsets (including eBPF-based approaches akin to Linux's BCC) adapted for GPUs/DPUs/APUs, typical workloads and matching toolchains, and the limitations and emerging directions in this landscape.\"]}]\n"])</script><script>self.__next_f.push([1,"2d:[\"$\",\"h2\",null,{\"className\":\"content-header\",\"id\":\"profiling-and-tracing-tools-for-gpus\",\"children\":[[\"$\",\"a\",null,{\"className\":\"break-words\",\"href\":\"#profiling-and-tracing-tools-for-gpus\",\"aria-hidden\":\"true\",\"tabIndex\":\"-1\",\"children\":[\"$\",\"span\",null,{\"className\":\"content-header-link\",\"children\":[\"$\",\"svg\",null,{\"className\":\"h-5 linkicon w-5\",\"fill\":\"currentColor\",\"viewBox\":\"0 0 20 20\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"children\":[[\"$\",\"path\",null,{\"d\":\"M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z\"}],[\"$\",\"path\",null,{\"d\":\"M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z\"}]]}]}]}],\"Profiling and Tracing Tools for GPUs\"]}]\n"])</script><script>self.__next_f.push([1,"2e:[\"$\",\"p\",null,{\"children\":[\"GPU profiling has matured over years of graphics and GPGPU development, yielding a rich ecosystem of vendor tools and open-source frameworks. Broadly, GPU profilers fall into two categories: \",[\"$\",\"strong\",null,{\"children\":\"development-time profilers\"}],\" that provide fine-grained insight for optimization (often with high overhead and GUI analysis), and \",[\"$\",\"strong\",null,{\"children\":\"lightweight monitors\"}],\" suitable for runtime or production use (low overhead, focusing on high-level metrics).\"]}]\n"])</script><script>self.__next_f.push([1,"2f:[\"$\",\"ul\",null,{\"children\":[[\"$\",\"li\",null,{\"children\":[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"NVIDIA's Profiling Suite:\"}],\" NVIDIA's GPUs dominate in HPC/AI, and their tools are considered industry-standard. \",[\"$\",\"em\",null,{\"children\":\"Nsight Systems\"}],\" provides system-wide timeline tracing – capturing CPU threads, CUDA kernel launches, memory copies, etc. – to pinpoint bottlenecks across the CPU-GPU boundary. It uses the CUDA Profiling Tools Interface (CUPTI) to gather detailed metrics and events. Nsight Systems is extremely powerful for deep analysis of GPU-accelerated applications, but it must explicitly start a profiling session and can incur significant overhead (often slowing programs by \",[\"$\",\"strong\",null,{\"children\":\"2×–10×\"}],\" during profiling). It's intended for development and tuning rather than continuous use. \",[\"$\",\"em\",null,{\"children\":\"Nsight Compute\"}],\", on the other hand, focuses on per-kernel deep dives: it profiles individual GPU kernels, providing instruction-level stats (e.g. instruction mix, memory transactions, warp occupancy, execution stalls) and can even associate performance metrics with source lines or PTX/SASS assembly. NVIDIA also offers \",[\"$\",\"em\",null,{\"children\":\"Visual Profiler\"}],\" (an older GUI, now largely replaced by Nsight) and command-line profiling via \",[\"$\",\"code\",null,{\"children\":\"nvprof\"}],\"/\",[\"$\",\"code\",null,{\"children\":\"nsys\"}],\". For graphics workloads (DirectX, OpenGL, Vulkan), NVIDIA's \",[\"$\",\"strong\",null,{\"children\":\"Nsight Graphics\"}],\" captures frame renderer pipelines, shader timings, and GPU state to assist game developers. All these NVIDIA tools are free but proprietary (closed-source). They are tightly optimized for NVIDIA hardware and support domain-specific analysis modes (graphics vs. compute vs. AI).\"]}]}],[\"$\",\"li\",null,{\"children\":[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"AMD's GPU Profiling Tools:\"}],\" AMD provides both open-source and proprietary tools as part of the ROCm and GPUOpen ecosystems. Historically, \",[\"$\",\"em\",null,{\"children\":\"CodeXL\"}],\" was AMD's all-in-one profiler and debugger for CPUs, GPUs, and APUs. CodeXL (now discontinued) could profile OpenCL, HIP, and HSA applications on AMD APUs/GPUs, collecting kernel execution times and hardware counters on integrated devices. In recent years, AMD shifted to ROCm-based tooling: \",[\"$\",\"strong\",null,{\"children\":\"rocProfiler\"}],\" and \",[\"$\",\"strong\",null,{\"children\":\"rocTracer\"}],\" libraries (analogous to NVIDIA's CUPTI) enable profiling and tracing of HIP and OpenCL applications. In 2024, AMD introduced new tools in ROCm 6.2: \",[\"$\",\"em\",null,{\"children\":\"Omniperf\"}],\" and \",[\"$\",\"em\",null,{\"children\":\"Omnitrace\"}],\". \",[\"$\",\"strong\",null,{\"children\":\"Omniperf\"}],\" is a kernel-level profiler for machine learning and HPC workloads on AMD Instinct GPUs, offering detailed performance counters analysis via CLI or a GUI dashboard. \",[\"$\",\"strong\",null,{\"children\":\"Omnitrace\"}],\" is a \",[\"$\",\"em\",null,{\"children\":\"multi-purpose profiling/tracing tool\"}],\" for CPU \",[\"$\",\"strong\",null,{\"children\":\"and\"}],\" GPU, supporting dynamic binary instrumentation, call-stack sampling, and even causal profiling to pinpoint which functions consume time on heterogeneous CPU-GPU executions. These tools are open-source or part of AMD's open ROCm stack. For graphics and game development, AMD provides \",[\"$\",\"strong\",null,{\"children\":\"Radeon GPU Profiler (RGP)\"}],\" and \",[\"$\",\"strong\",null,{\"children\":\"Radeon Developer Panel\"}],\" under GPUOpen. RGP offers low-level timeline and wavefront occupancy data for Vulkan/DX12 applications on Radeon GPUs, while Radeon Memory Visualizer helps track memory usage. AMD's tools can be used on their APUs as well, benefiting from unified memory (e.g., profiling an APU means GPU kernels can be traced without PCIe transfer overhead). Overall, AMD's approach emphasizes open interfaces (e.g., the \",\"$L5e\",\" library gives developers direct access to GPU performance counters) and integration with generic profilers.\"]}]}],\"$L5f\",\"$L60\",\"$L61\"]}]\n"])</script><script>self.__next_f.push([1,"30:[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Table 1\"}],\" below summarizes a sample of representative GPU profiling/tracing tools across vendors and domains, highlighting their availability and focus:\"]}]\n"])</script><script>self.__next_f.push([1,"31:[\"$\",\"div\",null,{\"className\":\"w-full overflow-x-auto\",\"children\":[\"$\",\"table\",null,{\"children\":[[\"$\",\"thead\",null,{\"children\":[\"$\",\"tr\",null,{\"children\":[[\"$\",\"th\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Tool\"}]}],[\"$\",\"th\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Vendor / Origin\"}]}],[\"$\",\"th\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Open-Source?\"}]}],[\"$\",\"th\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Primary Focus\"}]}],[\"$\",\"th\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Supported Domain\"}]}]]}]}],[\"$\",\"tbody\",null,{\"children\":[[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Nsight Systems\"}]}],[\"$\",\"td\",null,{\"children\":\"NVIDIA\"}],[\"$\",\"td\",null,{\"children\":\"No (free)\"}],[\"$\",\"td\",null,{\"children\":\"Timeline tracing (CPU \u0026 GPU), deep metrics\"}],[\"$\",\"td\",null,{\"children\":\"CUDA Compute, AI, some Graphics\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Nsight Compute\"}]}],[\"$\",\"td\",null,{\"children\":\"NVIDIA\"}],[\"$\",\"td\",null,{\"children\":\"No (free)\"}],[\"$\",\"td\",null,{\"children\":\"Kernel microarchitecture profiling\"}],[\"$\",\"td\",null,{\"children\":\"CUDA/HPC/AI (per-kernel)\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"NVIDIA PerfKit/DCGM\"}]}],[\"$\",\"td\",null,{\"children\":\"NVIDIA\"}],[\"$\",\"td\",null,{\"children\":\"No (free)\"}],[\"$\",\"td\",null,{\"children\":\"Low-level HW counters (PerfKit); Datacenter GPU monitoring (DCGM)\"}],[\"$\",\"td\",null,{\"children\":\"System GPU Telemetry\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Radeon GPU Profiler\"}]}],[\"$\",\"td\",null,{\"children\":\"AMD (GPUOpen)\"}],[\"$\",\"td\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Yes\"}],\" (free)\"]}],[\"$\",\"td\",null,{\"children\":\"Low-level GPU trace (wavefront, ISA)\"}],[\"$\",\"td\",null,{\"children\":\"Graphics (Vulkan/DX12), GPGPU\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"ROCm Omniperf\"}]}],[\"$\",\"td\",null,{\"children\":\"AMD ROCm (Instinct MI GPUs)\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Yes\"}]}],[\"$\",\"td\",null,{\"children\":\"Kernel profiling (counters \u0026 analysis)\"}],[\"$\",\"td\",null,{\"children\":\"HPC/AI Compute\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"ROCm Omnitrace\"}]}],[\"$\",\"td\",null,{\"children\":\"AMD ROCm\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Yes\"}]}],[\"$\",\"td\",null,{\"children\":\"CPU-GPU tracing, call-stack profiling\"}],[\"$\",\"td\",null,{\"children\":\"HPC/AI, Heterogeneous apps\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Intel VTune \u0026 GPA\"}]}],[\"$\",\"td\",null,{\"children\":\"Intel oneAPI\"}],[\"$\",\"td\",null,{\"children\":\"Partial (VTune closed, PTI open)\"}],[\"$\",\"td\",null,{\"children\":\"VTune: GPU offload analysis; GPA: frame analysis\"}],[\"$\",\"td\",null,{\"children\":\"Compute (oneAPI) \u0026 Graphics\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"HPCToolkit\"}]}],[\"$\",\"td\",null,{\"children\":\"Rice Univ. (HPC tool)\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Yes\"}]}],[\"$\",\"td\",null,{\"children\":\"Sampling-based profiling (CPU \u0026 GPU)\"}],[\"$\",\"td\",null,{\"children\":\"HPC/AI (CPU+GPU)\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"RenderDoc\"}]}],[\"$\",\"td\",null,{\"children\":\"Community / Baldur Karlsson\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Yes\"}]}],[\"$\",\"td\",null,{\"children\":\"Frame capture \u0026 API trace\"}],[\"$\",\"td\",null,{\"children\":\"Graphics debugging\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"PyTorch Kineto\"}]}],[\"$\",\"td\",null,{\"children\":\"FB/Intel (via PyTorch)\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Yes\"}]}],[\"$\",\"td\",null,{\"children\":\"In-framework profiler (CPU+GPU)\"}],[\"$\",\"td\",null,{\"children\":\"AI/ML (Deep Learning)\"}]]}]]}]]}]}]\n"])</script><script>self.__next_f.push([1,"32:[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Sources:\"}],\" GPU vendor documentation and tool websites.\"]}]\n"])</script><script>self.__next_f.push([1,"33:[\"$\",\"p\",null,{\"children\":[\"As the table suggests, \",[\"$\",\"strong\",null,{\"children\":\"open-source solutions\"}],\" are prominent in research and HPC (e.g. HPCToolkit, Omniperf/Omnitrace, RenderDoc), while \",[\"$\",\"strong\",null,{\"children\":\"commercial/vendor tools\"}],\" (Nsight, VTune, etc.) often provide the most optimized access to proprietary hardware features (like NVIDIA's profilers using privileged CUPTI APIs). Open tools may trade some low-level detail for broader applicability – for example, HPCToolkit can profile across NVIDIA, AMD, and Intel GPUs in a uniform way, but for deepest NVIDIA-specific metrics (e.g. SM warp stall reasons), developers still rely on Nsight Compute. Conversely, vendor tools are typically free-of-cost but \",[\"$\",\"strong\",null,{\"children\":\"closed-source\"}],\", and each vendor's toolchain is distinct, leading to fragmentation. A developer targeting multiple GPU platforms might need to juggle multiple profilers (one for CUDA, one for ROCm, one for Intel) since there is \",[\"$\",\"em\",null,{\"children\":\"no single standard interface\"}],\" for GPU performance counters across vendors. The lack of standardization has led to projects like ARM's \",[\"$\",\"strong\",null,{\"children\":\"HWCPipe\"}],\" library that attempt to abstract GPU counters for multiple architectures in one API, but such efforts are still evolving.\"]}]\n"])</script><script>self.__next_f.push([1,"34:[\"$\",\"h3\",null,{\"className\":\"content-header\",\"id\":\"gpu-tracing-with-low-overhead-and-continuous-monitoring\",\"children\":[[\"$\",\"a\",null,{\"className\":\"break-words\",\"href\":\"#gpu-tracing-with-low-overhead-and-continuous-monitoring\",\"aria-hidden\":\"true\",\"tabIndex\":\"-1\",\"children\":[\"$\",\"span\",null,{\"className\":\"content-header-link\",\"children\":[\"$\",\"svg\",null,{\"className\":\"h-5 linkicon w-5\",\"fill\":\"currentColor\",\"viewBox\":\"0 0 20 20\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"children\":[[\"$\",\"path\",null,{\"d\":\"M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z\"}],[\"$\",\"path\",null,{\"d\":\"M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z\"}]]}]}]}],\"GPU Tracing with Low-Overhead and Continuous Monitoring\"]}]\n"])</script><script>self.__next_f.push([1,"35:[\"$\",\"p\",null,{\"children\":[\"Classic GPU profilers (as described above) are extremely useful during development, but their overhead and intrusive workflows make them unsuitable for always-on monitoring in production (for instance, you can't afford a 5× slowdown on a live AI inference server just to collect traces). To fill this gap, recent tools leverage \",[\"$\",\"em\",null,{\"children\":\"low-overhead tracing techniques\"}],\" inspired by systems like Linux's eBPF. One notable approach is Meta's deployment of \",[\"$\",\"strong\",null,{\"children\":\"eBPF\"}],\" for fleet-wide GPU profiling, as presented by Selim (2023) at the eBPF Summit. Instead of instrumenting GPU code, Meta's tool attaches to GPU driver events via eBPF, enabling continuous collection of GPU metrics across thousands of machines with negligible overhead.\"]}]\n"])</script><script>self.__next_f.push([1,"36:[\"$\",\"p\",null,{\"children\":[\"An example of an open project in this vein is \",[\"$\",\"strong\",null,{\"children\":\"GPUprobe\"}],\", a Linux eBPF-based GPU observability tool. GPUprobe uses \",[\"$\",\"em\",null,{\"children\":\"uprobes\"}],\" (user-level probes) to hook into NVIDIA's CUDA runtime library functions at the kernel level. In doing so, it can monitor events like GPU memory allocations (\",[\"$\",\"code\",null,{\"children\":\"cudaMalloc/free\"}],\") and kernel launches in real time – \",[\"$\",\"strong\",null,{\"children\":\"without\"}],\" requiring any modifications or instrumentation in the target application code. The overhead is very low (measured under 4% in benchmarks), so it's feasible to run in production continuously. GPUprobe fills a \",[\"$\",\"em\",null,{\"children\":\"middle ground\"}],\" between heavy profilers and coarse monitoring: it provides richer, per-application insights than NVIDIA's Data Center GPU Manager (DCGM) – such as tracking memory leaks per process and kernel launch frequencies – but with far less overhead than Nsight's full profiling. As the GPUprobe authors note, \",[\"$\",\"strong\",null,{\"children\":\"Nsight Systems\"}],\" is like a \\\"GPU-specific debugger\\\" that's great for deep dives but not for continuous use, while \",[\"$\",\"strong\",null,{\"children\":\"DCGM\"}],\" gives high-level stats (utilization, temps, health) and misses app-specific details. Tools like GPUprobe bridge this gap, exporting metrics to standard observability systems (e.g. Prometheus/Grafana) for integration into data center dashboards. In fact, GPUprobe's design allows scraping of its metrics (memory usage maps, kernel launch counts, bandwidth usage) in \",[\"$\",\"strong\",null,{\"children\":\"OpenMetrics\"}],\" format, so operators can visualize GPU behavior over time in Grafana alongside CPU, network, and other metrics.\"]}]\n"])</script><script>self.__next_f.push([1,"37:[\"$\",\"p\",null,{\"children\":[\"This \",[\"$\",\"strong\",null,{\"children\":\"BCC/eBPF-inspired approach\"}],\" is an emerging trend for GPU profiling. It aims to bring the powerful methodology of kernel tracing (pioneered on CPUs by tools like \",[\"$\",\"code\",null,{\"children\":\"perf\"}],\", \",[\"$\",\"code\",null,{\"children\":\"bcc\"}],\", and eBPF) into the GPU realm. Research prototypes have even explored running eBPF programs \",[\"$\",\"em\",null,{\"children\":\"on the GPU\"}],\" itself (for instance, an academic project \\\"eGPU\\\" offloaded BPF bytecode to GPUs via PTX injection), though such techniques are not yet mainstream. At present, the more practical uses involve hooking GPU driver or runtime events from the CPU side. The result is a non-intrusive peek into GPU operations: for example, detecting if a GPU job is launch-bound (many small kernel launches) or memory-leak-prone, \",[\"$\",\"em\",null,{\"children\":\"without\"}],\" recompiling the application. This is particularly valuable for cloud providers or large-scale AI deployments, where continuous profiling can catch performance regressions or resource leaks in long-running GPU services.\"]}]\n"])</script><script>self.__next_f.push([1,"38:[\"$\",\"h2\",null,{\"className\":\"content-header\",\"id\":\"profiling-and-tracing-tools-for-dpus-smartnics\",\"children\":[[\"$\",\"a\",null,{\"className\":\"break-words\",\"href\":\"#profiling-and-tracing-tools-for-dpus-smartnics\",\"aria-hidden\":\"true\",\"tabIndex\":\"-1\",\"children\":[\"$\",\"span\",null,{\"className\":\"content-header-link\",\"children\":[\"$\",\"svg\",null,{\"className\":\"h-5 linkicon w-5\",\"fill\":\"currentColor\",\"viewBox\":\"0 0 20 20\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"children\":[[\"$\",\"path\",null,{\"d\":\"M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z\"}],[\"$\",\"path\",null,{\"d\":\"M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z\"}]]}]}]}],\"Profiling and Tracing Tools for DPUs (SmartNICs)\"]}]\n"])</script><script>self.__next_f.push([1,"39:[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"DPUs\"}],\" (data processing units), often manifested as \",[\"$\",\"strong\",null,{\"children\":\"SmartNICs\"}],\", combine general-purpose cores with specialized packet-processing hardware. They are used to offload networking (packet switching, virtualization), storage (NVMe-oF, encryption), and security tasks from the main CPU. Profiling DPUs presents a distinct challenge: one must consider both the on-board CPU (usually an Arm SoC running Linux) and the networking data plane which may involve FPGA logic or ASIC accelerators on the NIC.\"]}]\n"])</script><script>self.__next_f.push([1,"3a:[\"$\",\"p\",null,{\"children\":[\"In general, profiling on a DPU can leverage many of the \",[\"$\",\"em\",null,{\"children\":\"standard Linux tools\"}],\" for the embedded CPU portion. For example, NVIDIA's BlueField DPUs run Ubuntu, so one can use \",[\"$\",\"strong\",null,{\"children\":\"Linux perf\"}],\", standard CPU profilers, or even eBPF-based monitors \",[\"$\",\"em\",null,{\"children\":\"on the DPU's Arm cores\"}],\" to profile software running locally (e.g., an offloaded software switch). If a user application or agent runs on the DPU's OS, it's profiled much like on any Linux server – albeit with an awareness of the limited cores and unique tasks (often packet-handling threads). In fact, one could run BCC tools on a BlueField to measure syscalls, or use \",[\"$\",\"code\",null,{\"children\":\"perf\"}],\" to sample cache misses in the DPU's code. This is the \",[\"$\",\"em\",null,{\"children\":\"general-purpose workload\"}],\" profiling on DPUs (similar to any Linux host, but constrained resources).\"]}]\n"])</script><script>self.__next_f.push([1,"3b:[\"$\",\"p\",null,{\"children\":[\"However, much of a DPU's workload is \",[\"$\",\"em\",null,{\"children\":\"domain-specific (networking and storage)\"}],\" and handled by specialized hardware blocks. For instance, a DPU may accelerate an Open vSwitch (OVS) datapath in hardware, or perform RDMA and NVMe operations via dedicated engines. Profiling these aspects often relies on \",[\"$\",\"strong\",null,{\"children\":\"vendor-provided telemetry and counters\"}],\". Vendors like NVIDIA (Mellanox) and Broadcom supply tools to monitor packet throughput, latency, and offload engine stats on their SmartNICs. NVIDIA's \",[\"$\",\"strong\",null,{\"children\":\"DOCA\"}],\" SDK for BlueField includes profiling APIs and performance monitors for accelerated functions (e.g., crypto, RDMA). The BlueField DPU exposes metrics such as packets per second, drops, and queue depths via standard interfaces (perhaps through DPDK or /sys counters). In the case of DPDK (a common user-space packet I/O library used with SmartNICs), developers can profile their packet processing pipeline on CPU using Intel VTune or perf, and measure NIC throughput using DPDK's built-in event counters. Intel's documentation even covers using VTune to analyze DPDK event scheduling on their infrastructure processing units (IPUs).\"]}]\n"])</script><script>self.__next_f.push([1,"3c:[\"$\",\"p\",null,{\"children\":[\"Commercial SmartNIC vendors offer their own monitoring suites. For example, \",[\"$\",\"strong\",null,{\"children\":\"Napatech\"}],\" (a SmartNIC manufacturer) distributes profiling tools that report port throughput, packet counters (RMON statistics), and even host application interaction metrics. These tools often come as command-line monitors or GUI dashboards. Napatech's monitoring CLI (shown in their docs) can live-update line rate (e.g., ~48 Gbps Rx/Tx on a 100G NIC) and various packet size counters. Such vendor tools are usually proprietary (bundled with the NIC), highlighting a similarity with GPU space: to get full visibility into hardware accelerators on the DPU, you typically use the vendor's API or utility. Another example: Broadcom/Pensando DPUs (now part of AMD) have an SDK that likely includes telemetry for their packet processors, though details are often behind NDAs. \",[\"$\",\"strong\",null,{\"children\":\"Cisco\"}],\" and \",[\"$\",\"strong\",null,{\"children\":\"Marvell\"}],\" likewise provide manageability interfaces for their SmartNICs (often as part of network OS or NIC firmware), focusing on throughput and latency metrics rather than instruction-level traces.\"]}]\n"])</script><script>self.__next_f.push([1,"3d:[\"$\",\"p\",null,{\"children\":[\"That said, open-source efforts are emerging for SmartNIC performance analysis. The P4 language, used to program some NICs, has debugging tools which can simulate or log packet flow through the pipeline (though not exactly a profiler in the traditional sense). Academic research has produced tools like \",[\"$\",\"em\",null,{\"children\":\"Clara\"}],\" and \",[\"$\",\"em\",null,{\"children\":\"Pipefuse\"}],\" to analyze or even \",[\"$\",\"em\",null,{\"children\":\"predict\"}],\" network function performance on SmartNICs. These aim to answer questions like \\\"if I offload this function to a SmartNIC, what throughput can I expect?\\\" by modeling the NIC's resources. While not runtime profilers, they address the broader performance tuning of DPU workloads. Another research example is \",[\"$\",\"em\",null,{\"children\":\"LogNIC\"}],\", which provides a performance model for SmartNIC pipelines. Such tools are largely experimental but point toward future \",[\"$\",\"strong\",null,{\"children\":\"high-level profilers\"}],\" for networking tasks.\"]}]\n"])</script><script>self.__next_f.push([1,"3e:[\"$\",\"p\",null,{\"children\":[\"In summary, \",[\"$\",\"strong\",null,{\"children\":\"DPU profiling today is a patchwork\"}],\" of general CPU profiling on one hand, and specialized network telemetry on the other. One might profile the \",[\"$\",\"strong\",null,{\"children\":\"software control plane\"}],\" on the DPU using familiar tools (to ensure the DPU's CPU isn't a bottleneck) while simultaneously using NIC counters or synthetic traffic tests to profile the \",[\"$\",\"strong\",null,{\"children\":\"data plane throughput\"}],\". Coordinating these is often manual. For instance, to profile an Open vSwitch offloaded to a DPU, you'd measure the DPU's CPU usage (for control tasks, flow setup) and gather NIC stats for packet rate and latency, possibly by generating test traffic and measuring end-to-end latency. Standard performance profilers for \",[\"$\",\"em\",null,{\"children\":\"network workloads\"}],\" (like how to trace a P4 program on hardware) are still nascent. We expect that as DPUs become more common, vendor-agnostic profiling standards may emerge – perhaps an extension of eBPF/XDP to trace through a SmartNIC, or an open telemetry schema for NICs – but currently much is vendor-specific.\"]}]\n"])</script><script>self.__next_f.push([1,"3f:[\"$\",\"h2\",null,{\"className\":\"content-header\",\"id\":\"profiling-and-tracing-tools-for-apus-cpugpu-integrated-platforms\",\"children\":[[\"$\",\"a\",null,{\"className\":\"break-words\",\"href\":\"#profiling-and-tracing-tools-for-apus-cpugpu-integrated-platforms\",\"aria-hidden\":\"true\",\"tabIndex\":\"-1\",\"children\":[\"$\",\"span\",null,{\"className\":\"content-header-link\",\"children\":[\"$\",\"svg\",null,{\"className\":\"h-5 linkicon w-5\",\"fill\":\"currentColor\",\"viewBox\":\"0 0 20 20\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"children\":[[\"$\",\"path\",null,{\"d\":\"M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z\"}],[\"$\",\"path\",null,{\"d\":\"M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z\"}]]}]}]}],\"Profiling and Tracing Tools for APUs (CPU–GPU Integrated Platforms)\"]}]\n"])</script><script>self.__next_f.push([1,"40:[\"$\",\"p\",null,{\"children\":[\"Accelerated Processing Units (APUs) blend CPU and GPU on a single die, sharing memory and interconnect. AMD's latest Instinct MI300A is a prime example: a data-center APU combining 24 Zen4 CPU cores with 128 GB of HBM memory and a CDNA3 GPU in one package. Profiling APUs involves understanding the \",[\"$\",\"strong\",null,{\"children\":\"interaction between the on-chip CPU and GPU\"}],\", which can be both a blessing and a challenge. On one hand, unified memory means developers don't need to profile PCIe transfer bottlenecks – CPU and GPU can access the same HBM pool, and data movement is via pointers rather than explicit copies. On the other hand, an APU's GPU shares power/thermal budgets with CPUs, which can introduce contention that profiling tools should reveal (e.g., if the GPU is throttling when CPU is maxed out).\"]}]\n"])</script><script>self.__next_f.push([1,"41:[\"$\",\"p\",null,{\"children\":[\"Tools for APUs largely overlap with the CPU and GPU tools discussed, with added emphasis on \",[\"$\",\"strong\",null,{\"children\":\"integrated analysis\"}],\". AMD's toolchain, for example, is APU-aware: \",[\"$\",\"strong\",null,{\"children\":\"AMD uProf\"}],\" is a profiling suite that covers CPU performance (PMU events, cache misses, etc.) and also can correlate with GPU activity on supported APUs. AMD uProf and CodeXL historically allowed profiling OpenCL kernels on an APU's iGPU, reporting each kernel's performance counters. The new ROCm Omnitrace (mentioned earlier) explicitly supports profiling both CPU and GPU in one timeline, which is ideal for APUs where CPU threads launch GPU work frequently. Omnitrace's ability to use binary instrumentation and call-stack sampling on the CPU side, while tracing GPU kernels, helps map performance \\\"holographically\\\" across the APU. This means if a CPU function on the APU calls a GPU kernel, the tool can show the time in the CPU function and the nested time in the GPU kernel as part of the same call tree – a critical capability for optimizing heterogeneous code.\"]}]\n"])</script><script>self.__next_f.push([1,"42:[\"$\",\"p\",null,{\"children\":[\"For consumer APUs (like AMD Ryzen processors with Radeon graphics), developers commonly use \",[\"$\",\"strong\",null,{\"children\":\"graphics profilers\"}],\" (for gaming use-cases) or \",[\"$\",\"strong\",null,{\"children\":\"OpenCL/Vulkan profilers\"}],\" for compute. AMD's Radeon GPU Profiler, for instance, works on integrated GPUs the same as discrete. The unified memory also allows use of standard OS performance counters: on Linux, AMD's GPU drivers expose certain GPU utilization metrics via \",[\"$\",\"code\",null,{\"children\":\"drm/sysfs\"}],\", so one could even use system monitors or custom scripts to log GPU activity alongside CPU. Windows developers with APUs might use Microsoft's \",[\"$\",\"strong\",null,{\"children\":\"PIX\"}],\" or AMD's Radeon Developer tools to profile DirectX12 games running on the integrated GPU – these tools show CPU and GPU timelines and could highlight if the CPU is starving the GPU or vice versa. Essentially, APU profiling doesn't require an entirely new class of tools, but it \",[\"$\",\"strong\",null,{\"children\":\"benefits from tools that can correlate CPU and GPU performance\"}],\" tightly. This is similar to profiling on a discrete GPU system, except the latency between CPU-GPU is lower and memory is shared, which tools need to account for (e.g., a cache coherency effect might appear where CPU and GPU contend on memory).\"]}]\n"])</script><script>self.__next_f.push([1,"43:[\"$\",\"p\",null,{\"children\":[\"It's worth noting that integrated architectures spurred the development of HSA (Heterogeneous System Architecture) in the mid-2010s, and AMD's tools had HSA-specific profiling modes. For example, CodeXL included an HSA profiler for AMD APUs which could trace HSA kernel dispatches and HSAIL instructions. Much of that functionality has been absorbed into ROCm tools now. The essence remains: \",[\"$\",\"strong\",null,{\"children\":[\"APUs require profiling of the \",[\"$\",\"em\",null,{\"children\":\"whole system\"}],\" rather than just \\\"CPU vs GPU\\\" in isolation\"]}],\". Tools like Omnitrace, VTune, or HPCToolkit (with its heterogeneous call path analysis) are particularly apt for APU-based workloads because they naturally mix CPU and GPU metrics.\"]}]\n"])</script><script>self.__next_f.push([1,"44:[\"$\",\"h2\",null,{\"className\":\"content-header\",\"id\":\"open-source-vs-commercial-solutions--a-comparison\",\"children\":[[\"$\",\"a\",null,{\"className\":\"break-words\",\"href\":\"#open-source-vs-commercial-solutions--a-comparison\",\"aria-hidden\":\"true\",\"tabIndex\":\"-1\",\"children\":[\"$\",\"span\",null,{\"className\":\"content-header-link\",\"children\":[\"$\",\"svg\",null,{\"className\":\"h-5 linkicon w-5\",\"fill\":\"currentColor\",\"viewBox\":\"0 0 20 20\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"children\":[[\"$\",\"path\",null,{\"d\":\"M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z\"}],[\"$\",\"path\",null,{\"d\":\"M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z\"}]]}]}]}],\"Open-Source vs Commercial Solutions – A Comparison\"]}]\n"])</script><script>self.__next_f.push([1,"45:[\"$\",\"p\",null,{\"children\":\"There is a healthy mix of open-source (OSS) and commercial/proprietary solutions in accelerator profiling, each with pros and cons. Here we compare key aspects:\"}]\n"])</script><script>self.__next_f.push([1,"46:[\"$\",\"ul\",null,{\"children\":[[\"$\",\"li\",null,{\"children\":[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Feature Depth and Hardware Access:\"}],\" Vendor-provided tools (usually closed-source, but often free of charge) tend to have the most \",[\"$\",\"strong\",null,{\"children\":\"comprehensive access to hardware performance counters and features\"}],\". For example, NVIDIA's Nsight can report SM warp stall reasons and texture cache hit rates – metrics exposed by NVIDIA's secret sauce interfaces that open tools generally don't get. Similarly, AMD's proprietary driver might expose GPU wavefront occupancy details to RGP that generic tools can't obtain. OSS tools rely on published or reverse-engineered interfaces; for instance, AMD's GPUPerfAPI (open library) provides cross-platform counter access, which is why AMD's own tools could be open-sourced. \",[\"$\",\"strong\",null,{\"children\":\"Open-source projects\"}],\" sometimes lack the very latest hardware support until vendors release documentation, whereas commercial tools are ready on Day 1 for new GPUs (since the vendor builds them). On the other hand, open tools like HPCToolkit have innovated features like fully automated call-stack unwinding and statistical GPU instruction sampling that are not available in vendor GUIs, showing that OSS can lead in certain capabilities (particularly around integration and low overhead).\"]}]}],[\"$\",\"li\",null,{\"children\":[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Extensibility and Customization:\"}],\" Open-source profilers (HPCToolkit, TAU, etc.) allow users to modify or script them, enabling \",[\"$\",\"strong\",null,{\"children\":\"custom analyses\"}],\" or integration into automated pipelines. For instance, you can instrument code with Score-P and emit traces in Open Trace Format (OTF2), then post-process with custom analytics – all because the formats and code are open. In contrast, commercial tools often lock data in proprietary formats (e.g., Nsight's \",[\"$\",\"code\",null,{\"children\":\".nsys-rep\"}],\" trace files) that require the vendor's viewer, though some export options exist (like CSV exports). The OSS approach also fosters community contributions – e.g., support for new programming models (OpenMP offload, Kokkos, etc.) often appears first in tools like TAU or Score-P via community patches.\"]}]}],[\"$\",\"li\",null,{\"children\":[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Cross-Vendor Support:\"}],\" As noted, open-source tools are generally more \",[\"$\",\"strong\",null,{\"children\":\"vendor-neutral\"}],\". A single tool like Vampir or HPCToolkit can handle \",[\"$\",\"em\",null,{\"children\":\"multiple\"}],\" accelerator types in one run, whereas vendor tools are siloed (Nsight won't profile an AMD GPU, and AMD's rocprof won't work on NVIDIA). For a heterogeneous environment (say, an Intel CPU, an NVIDIA GPU, and maybe a Xilinx FPGA in one system), your only hope for a unified trace might be an open tool that supports all via plugins or standard APIs (OpenCL, oneAPI, etc.). This is a strong point in favor of OSS solutions in research or multi-vendor shops. The downside is that \",[\"$\",\"strong\",null,{\"children\":\"vendor tools are often better optimized\"}],\" for their own hardware – they may offer a more polished UI, or more stable data collection on that platform. For example, an open tool using unofficial GPU counters might be brittle or less accurate if drivers change.\"]}]}],[\"$\",\"li\",null,{\"children\":[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Cost and Support:\"}],\" Most vendor tools for GPUs/DPUs are free (as in beer) but closed. There are a few truly commercial (for-purchase) performance tools in HPC, such as the Arm Forge suite (which includes the MAP profiler) – these come with professional support. Open-source tools are free (as in speech/beer) but support comes from community or self-expertise. Companies with mission-critical needs sometimes prefer tools backed by vendor support (to help interpret results or get bug fixes). That said, big vendors (NVIDIA, Intel) do provide support forums even for their free tools. In niche domains like networking, some commercial analyzers (e.g., deep packet inspection performance profilers) might come from specialized firms and require licenses – but these are relatively rare.\"]}]}]]}]\n"])</script><script>self.__next_f.push([1,"47:[\"$\",\"p\",null,{\"children\":[\"In practice, environments often use a \",[\"$\",\"strong\",null,{\"children\":\"combination\"}],\": e.g., an HPC center might use vendor profilers to optimize code on a specific GPU, but then integrate HPCToolkit or IPM (an MPI profiler) for regression testing and cross-system comparisons. Notably, open and closed tools can complement each other. A developer might run an OSS tracing tool for a high-level overview and cross-check specific kernels with the vendor's detailed profiler. An example from the GPU domain: a user could run a Score-P instrumented program to get an overall MPI+GPU timeline, then zoom into a particular GPU kernel of interest with Nsight Compute to inspect its memory throughput. This layered approach plays to each tool's strengths.\"]}]\n"])</script><script>self.__next_f.push([1,"48:[\"$\",\"h2\",null,{\"className\":\"content-header\",\"id\":\"workload-specific-tool-mappings\",\"children\":[[\"$\",\"a\",null,{\"className\":\"break-words\",\"href\":\"#workload-specific-tool-mappings\",\"aria-hidden\":\"true\",\"tabIndex\":\"-1\",\"children\":[\"$\",\"span\",null,{\"className\":\"content-header-link\",\"children\":[\"$\",\"svg\",null,{\"className\":\"h-5 linkicon w-5\",\"fill\":\"currentColor\",\"viewBox\":\"0 0 20 20\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"children\":[[\"$\",\"path\",null,{\"d\":\"M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z\"}],[\"$\",\"path\",null,{\"d\":\"M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z\"}]]}]}]}],\"Workload-Specific Tool Mappings\"]}]\n"])</script><script>self.__next_f.push([1,"49:[\"$\",\"p\",null,{\"children\":\"Different workloads stress accelerators in different ways, and accordingly, certain tools are favored in each domain:\"}]\n"])</script><script>self.__next_f.push([1,"4a:[\"$\",\"ul\",null,{\"children\":[[\"$\",\"li\",null,{\"children\":[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"General-Purpose Computing / HPC:\"}],\" These workloads (scientific simulations, linear algebra, data analytics) use GPUs for throughput. Profiling focuses on kernel efficiency and GPU utilization. Tools like Nsight Compute (for compute kernel analysis) or HPCToolkit/Omniperf are well-suited. HPC codes also run on thousands of GPUs in parallel; tracing each in detail is impractical, so tools like HPCToolkit that add only ~1-5% overhead via sampling are invaluable for profiling large-scale runs. HPC workloads often use MPI + GPU, so tools that can correlate communication and computation (e.g., timeline traces via Extrae, or MPI profiles via mpiP combined with GPU profiles) map well. On DPUs in HPC (e.g., using SmartNICs for RDMA), the \\\"workload\\\" is typically just networking – here one cares about throughput and overlap (profiling ensures that the DPU handles data transfers while GPUs compute, for example). Tools: network benchmarks (like IB Profiler for InfiniBand) plus GPU profilers to see if communication overlaps with computation.\"]}]}],[\"$\",\"li\",null,{\"children\":[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Machine Learning / AI:\"}],\" ML training combines heavy GPU compute with data pipeline overhead. Profiling an ML workload might involve \",[\"$\",\"strong\",null,{\"children\":\"framework-level profilers\"}],\": e.g., \",[\"$\",\"em\",null,{\"children\":\"PyTorch Profiler\"}],\" (built on Kineto) which internally uses CUPTI to record each op's GPU time, or \",[\"$\",\"em\",null,{\"children\":\"TensorFlow Profiler\"}],\" which similarly captures timelines of ops and streams. These produce high-level views (which model layer took time) as well as low-level kernel details. NVIDIA has a \",[\"$\",\"em\",null,{\"children\":\"DLProf\"}],\" tool that integrates with TensorBoard to show GPU kernel metrics in the context of neural network operations. For multi-GPU training, Nsight Systems can trace activity across GPUs (especially if using NCCL for communication – Nsight can show NCCL calls timeline). An emerging challenge is profiling \",[\"$\",\"strong\",null,{\"children\":\"distributed training\"}],\": tools like PyTorch Profiler now have distributed traces, but it's still an area of active development to seamlessly profile 100s of GPUs training one model. On the DPU side, AI may use DPUs for preprocessing or moving data – profiling the DPU's effect (say using it to do data filtering) would involve monitoring how much the DPU speeds up data ingestion (tracked via throughput) and ensuring the GPU is not starved. In the future, \",[\"$\",\"em\",null,{\"children\":\"AI accelerators on DPUs\"}],\" (some DPUs might include tiny ML cores) could require new profilers, but currently most AI work is GPU-centric.\"]}]}],[\"$\",\"li\",null,{\"children\":[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Networking and I/O Workloads:\"}],\" For pure networking tasks on DPUs or GPUs (yes, GPUs can also do packet processing in some cases using CUDA or OpenCL), the profiling is about latency and throughput. Tools here include packet generators (to measure how many Mpps a pipeline can handle) and tracing tools for code paths. For instance, if using a GPU to accelerate packet encryption, one might use Nsight to ensure kernel launches overlap with data transfers. If using a DPU to run, say, an IDS (intrusion detection) in software, one might profile it with perf to see if it's CPU-bound and use NIC counters for drops. Networking workloads often demand real-time tracing (to catch jitter spikes), so eBPF-based monitors or even hardware telemetry (like P4 runtime logs) could be employed. There's also interest in using GPU for networking (GPU-accelerated NIC offloads via CUDA pipelines), which would involve both GPU and network profiling – but that's fairly niche and experimental.\"]}]}],\"$L62\"]}]\n"])</script><script>self.__next_f.push([1,"4b:[\"$\",\"p\",null,{\"children\":[\"In essence, each processing unit type sees use in particular domains, and the profiling solutions have evolved accordingly – but there is also convergence. A modern AI application may involve \",[\"$\",\"strong\",null,{\"children\":\"GPUs for compute, CPUs for orchestration, DPUs for data loading\"}],\", all in one pipeline. This raises the need for \",[\"$\",\"strong\",null,{\"children\":\"multi-accelerator profiling\"}],\" – the ability to trace an operation as it moves through CPU, DPU, and GPU. Today this often means running multiple tools and correlating timestamps manually. For instance, one might use Nsight to trace the GPU and \",[\"$\",\"em\",null,{\"children\":\"simultaneously\"}],\" run \",[\"$\",\"code\",null,{\"children\":\"tcpdump\"}],\" or NIC counters on the network side, then align the logs. Such multi-component workflows are cumbersome, pointing to an opportunity for more integrated profiling of heterogeneous workflows.\"]}]\n"])</script><script>self.__next_f.push([1,"4c:[\"$\",\"h2\",null,{\"className\":\"content-header\",\"id\":\"current-research-directions-and-emerging-trends\",\"children\":[[\"$\",\"a\",null,{\"className\":\"break-words\",\"href\":\"#current-research-directions-and-emerging-trends\",\"aria-hidden\":\"true\",\"tabIndex\":\"-1\",\"children\":[\"$\",\"span\",null,{\"className\":\"content-header-link\",\"children\":[\"$\",\"svg\",null,{\"className\":\"h-5 linkicon w-5\",\"fill\":\"currentColor\",\"viewBox\":\"0 0 20 20\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"children\":[[\"$\",\"path\",null,{\"d\":\"M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z\"}],[\"$\",\"path\",null,{\"d\":\"M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z\"}]]}]}]}],\"Current Research Directions and Emerging Trends\"]}]\n"])</script><script>self.__next_f.push([1,"4d:[\"$\",\"p\",null,{\"children\":\"The field of accelerator tracing/profiling is actively evolving. Several key research and development directions are apparent:\"}]\n"])</script><script>self.__next_f.push([1,"4e:[\"$\",\"ul\",null,{\"children\":[[\"$\",\"li\",null,{\"children\":[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Low-Overhead, Always-On Profiling:\"}],\" As discussed, techniques borrowing from OS telemetry (eBPF, hardware performance monitors) are being adapted for accelerators. Meta's continuous GPU profiler and tools like GPUprobe demonstrate that one can collect useful data \",[\"$\",\"em\",null,{\"children\":\"in production\"}],\" with minimal overhead. We anticipate more work in this area: for example, \",[\"$\",\"em\",null,{\"children\":\"continuous DPU monitoring\"}],\" integrated into data center observability stacks (similar to DCGM for GPUs) – perhaps using eBPF to monitor DPU NIC drivers or using in-hardware telemetry (many NICs have telemetry streams for packet pacing, queue occupancy, etc., which could be tapped into). For GPUs, researchers are exploring \",[\"$\",\"strong\",null,{\"children\":\"sampling-based profiling\"}],\" to reduce overhead further. NVIDIA's latest architectures support \",[\"$\",\"em\",null,{\"children\":\"PC sampling\"}],\", where the GPU periodically samples its program counter and reports which instructions are executing and if they stalled. This can be done in the background with little interference. HPCToolkit already leverages this on NVIDIA GPUs to sample instructions and derive stall breakdowns. Future tools might extend such sampling to gather a statistical trace of GPU activity without instrumenting each kernel launch.\"]}]}],[\"$\",\"li\",null,{\"children\":[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Unified and Standardized Interfaces:\"}],\" A recurring theme is the lack of standardization across vendors. There are calls for a \",[\"$\",\"strong\",null,{\"children\":\"vendor-neutral GPU profiling API\"}],\" – for instance, a past suggestion in the OpenGL community proposed a unified API for GPU performance counters across vendors. In the compute realm, something analogous to the CPU's PAPI (Performance API) for GPUs would be valuable. We see early steps: the Khronos Group's OpenCL and Vulkan APIs have performance query extensions (like Vulkan's \",[\"$\",\"code\",null,{\"children\":\"VK_KHR_performance_query\"}],\") that let applications gather some counters in a standardized way. Also, Intel's oneAPI aims to provide a uniform interface (Level Zero) for accelerators, including tools support. While oneAPI is Intel-centric, it sets a precedent for abstracting profiling: an application could, in theory, use oneAPI to profile code on CPUs, GPUs, and FPGAs with a single tool – but only if other vendors adopt or adapt to it. Another push is in \",[\"$\",\"strong\",null,{\"children\":\"OpenTelemetry\"}],\" for hardware – currently OpenTelemetry (popular in cloud for tracing requests) doesn't cover internal hardware events, but conceivably it could be extended to span spans across CPU, accelerator, and network events for distributed tracing of heterogeneous workloads.\"]}]}],[\"$\",\"li\",null,{\"children\":[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Integration of AI in Performance Analysis:\"}],\" With the complexity of performance data (especially from fine-grained traces), there's interest in using machine learning to assist performance analysis. Research projects have looked at learning models to predict performance from partial traces or to automatically classify bottlenecks from counter signatures. For example, an ML model might learn the patterns in counters that indicate memory bandwidth bottleneck vs computation-bound, automating what human experts do manually with roofline models. While not mainstream in tools yet, some profilers (like Intel Advisor's automated roofline analysis, or NVIDIA's Guided Analysis in Nsight Compute) incorporate heuristic guidance that could evolve into ML-driven suggestions. The goal is to help developers interpret the deluge of profiling data more easily.\"]}]}],\"$L63\",\"$L64\",\"$L65\"]}]\n"])</script><script>self.__next_f.push([1,"4f:[\"$\",\"p\",null,{\"children\":[\"In summary, \",[\"$\",\"strong\",null,{\"children\":[\"profiling/tracing research is moving toward making these tools more \",[\"$\",\"em\",null,{\"children\":\"pervasive\"}],\", \",[\"$\",\"em\",null,{\"children\":\"intelligent\"}],\", and \",[\"$\",\"em\",null,{\"children\":\"unified\"}]]}],\". The aim is to reduce the burden on developers to manually instrument and correlate performance data from disparate sources, and instead provide smarter tools that work across the complex heterogeneous systems of today's data centers.\"]}]\n"])</script><script>self.__next_f.push([1,"50:[\"$\",\"h2\",null,{\"className\":\"content-header\",\"id\":\"limitations-and-challenges-of-existing-toolchains\",\"children\":[[\"$\",\"a\",null,{\"className\":\"break-words\",\"href\":\"#limitations-and-challenges-of-existing-toolchains\",\"aria-hidden\":\"true\",\"tabIndex\":\"-1\",\"children\":[\"$\",\"span\",null,{\"className\":\"content-header-link\",\"children\":[\"$\",\"svg\",null,{\"className\":\"h-5 linkicon w-5\",\"fill\":\"currentColor\",\"viewBox\":\"0 0 20 20\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"children\":[[\"$\",\"path\",null,{\"d\":\"M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z\"}],[\"$\",\"path\",null,{\"d\":\"M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z\"}]]}]}]}],\"Limitations and Challenges of Existing Toolchains\"]}]\n"])</script><script>self.__next_f.push([1,"51:[\"$\",\"p\",null,{\"children\":\"Despite the plethora of tools discussed, practitioners face several persistent challenges when profiling GPUs, DPUs, and APUs:\"}]\n"])</script><script>self.__next_f.push([1,"52:[\"$\",\"ul\",null,{\"children\":[[\"$\",\"li\",null,{\"children\":[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Fragmentation and Vendor Lock-In:\"}],\" As noted, each vendor's accelerators often come with a siloed toolchain. This means expertise in one doesn't translate easily to another, and mixing hardware leads to multiple tools. It also risks \",[\"$\",\"em\",null,{\"children\":\"lock-in\"}],\": optimizations done with a proprietary tool might rely on vendor-specific features. There is no universal standard like \\\"perf\\\" that universally covers all accelerator types (though on CPUs, perf itself is limited to Linux). The lack of common performance counter interfaces across GPU vendors is a prime example – developers must use CUDA-specific or ROCm-specific APIs, making portable performance analysis difficult. For DPUs, which are relatively new, there isn't even a widely adopted third-party profiler – you use whatever the DPU vendor provides. This fragmentation not only complicates life for developers, but also for researchers trying to compare platforms fairly.\"]}]}],[\"$\",\"li\",null,{\"children\":[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Limited Visibility (\\\"Black Box\\\" issues):\"}],\" Some aspects of accelerator performance are effectively black boxes to current profilers. For instance, GPU internal scheduling (how warps are scheduled, how L2 cache lines are evicted) might not be fully exposed by counters. Similarly, if a DPU's packet accelerator is an FPGA or ASIC, external tools might only see a throughput number, not \",[\"$\",\"em\",null,{\"children\":\"why\"}],\" it saturates at X Gbps (e.g., a microarchitecture detail inside the NIC). Even with something like NVIDIA's counters, there are often \",[\"$\",\"em\",null,{\"children\":\"undocumented metrics\"}],\" or ones that are hard to interpret. This limited visibility is often by design (IP protection), but hampers optimization. Another visibility issue arises in \",[\"$\",\"em\",null,{\"children\":\"closed-source workloads\"}],\": if you are profiling a third-party library on the GPU, you might see a kernel name and time but not what it did internally. Tools like HPCToolkit help by attributing costs to instructions even without source, but generally it's hard to optimize what you can't inspect.\"]}]}],[\"$\",\"li\",null,{\"children\":[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Overhead vs. Intrusiveness:\"}],\" Many precise tracing tools perturb the very performance they measure. Instrumentation-based tracing (inserting hooks on each kernel launch or each packet) can cause Heisenberg effects where the act of measuring changes timing. While sampling-based methods alleviate this, they trade detail for lower overhead. There is always a challenge to find the right balance: \",[\"$\",\"em\",null,{\"children\":\"how much data to collect\"}],\" and \",[\"$\",\"em\",null,{\"children\":\"at what frequency\"}],\" to get a representative profile without drowning in overhead or data volume. High-overhead tools must be limited to test environments, meaning you might not catch issues that only manifest at scale or in production. Conversely, ultra-light monitors might only flag \\\"GPU utilization low\\\" but not explain the cause. Bridging this gap remains an ongoing challenge.\"]}]}],[\"$\",\"li\",null,{\"children\":[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Concurrency and Ordering Issues:\"}],\" Profiling multi-threaded, multi-stream workloads can run into issues aligning events. For example, timeline traces from different GPUs or different devices may have clock skew, making it tricky to know the true sequence of events. Even on one GPU, the hardware can execute many kernels concurrently (on different SMs or using async streams), and visualizing or understanding overlapping activities is complex. Tools try (Nsight Systems, for instance, shows overlapping kernels on a timeline), but as systems scale, one confronts what we might call the \\\"explosion of events\\\" problem: too many events to reason about. Filtering and focusing on the right subset is difficult. Tools are only beginning to introduce smarter filtering (like only trace kernels longer than X microseconds, or only trace GPU activity when GPU utilization drops, etc.).\"]}]}],\"$L66\",\"$L67\",\"$L68\"]}]\n"])</script><script>self.__next_f.push([1,"53:[\"$\",\"p\",null,{\"children\":[\"In summary, while current tracing and profiling tools for GPUs, DPUs, and APUs are powerful and essential, they operate in a landscape that is \",[\"$\",\"strong\",null,{\"children\":\"highly specialized and fragmented\"}],\", with visibility gaps and integration shortcomings. Overcoming these limitations will require collaborative efforts – between hardware vendors (to open up interfaces), tool developers (to create smarter, standard tools), and the research community (to pioneer new techniques for low-overhead and combined tracing).\"]}]\n"])</script><script>self.__next_f.push([1,"54:[\"$\",\"h2\",null,{\"className\":\"content-header\",\"id\":\"conclusion\",\"children\":[[\"$\",\"a\",null,{\"className\":\"break-words\",\"href\":\"#conclusion\",\"aria-hidden\":\"true\",\"tabIndex\":\"-1\",\"children\":[\"$\",\"span\",null,{\"className\":\"content-header-link\",\"children\":[\"$\",\"svg\",null,{\"className\":\"h-5 linkicon w-5\",\"fill\":\"currentColor\",\"viewBox\":\"0 0 20 20\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"children\":[[\"$\",\"path\",null,{\"d\":\"M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z\"}],[\"$\",\"path\",null,{\"d\":\"M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z\"}]]}]}]}],\"Conclusion\"]}]\n"])</script><script>self.__next_f.push([1,"55:[\"$\",\"p\",null,{\"children\":[\"Tracing and profiling accelerators has become as critical as profiling CPUs was in earlier eras. We now have a broad arsenal of tools tailored to different hardware and use cases: from NVIDIA's Nsight suite and AMD's ROCm tools for deep GPU analysis, to open-source frameworks like HPCToolkit for holistic profiling, to emerging eBPF-based approaches enabling continuous monitoring. \",[\"$\",\"strong\",null,{\"children\":\"GPUs\"}],\" enjoy the most mature tool support (reflecting their longer history in computing), while \",[\"$\",\"strong\",null,{\"children\":\"DPUs\"}],\" and other domain-specific accelerators are catching up with their own nascent profilers and telemetry systems. \",[\"$\",\"strong\",null,{\"children\":\"APUs\"}],\" illustrate the need for tools that can seamlessly profile across traditional processor boundaries, as computing moves toward tightly integrated heterogeneous designs.\"]}]\n"])</script><script>self.__next_f.push([1,"56:[\"$\",\"p\",null,{\"children\":\"This review has highlighted that both open-source and commercial solutions play important roles: open tools foster cross-platform agility and innovation, whereas vendor tools leverage proprietary knowledge for maximum insight on their hardware. The best outcomes often arise from using them in combination. In specialized domains like ML, networking, and graphics, domain-specific profiling capabilities augment general tools to provide the needed perspective (e.g., viewing a GPU timeline in terms of neural network layers, or network throughput in context of CPU cycles).\"}]\n57:[\"$\",\"p\",null,{\"children\":[\"Looking ahead, the trends point to more \",[\"$\",\"strong\",null,{\"children\":\"integration (unified timelines across accelerators), automation (intelligent analysis), and low-overhead observability\"}],\" becoming standard. Research is actively addressing many current gaps, from standardizing performance metrics to using novel techniques like causal profiling and ML-driven analysis to interpret performance data. At the same time, challenges like vendor lock-in and black-box hardware will require industry collaboration and perhaps a push for more open hardware telemetry interfaces.\"]}]\n"])</script><script>self.__next_f.push([1,"58:[\"$\",\"p\",null,{\"children\":[\"In conclusion, developers and engineers aiming to optimize accelerator-powered systems should take a \",[\"$\",\"strong\",null,{\"children\":\"hybrid approach\"}],\": leverage the rich features of vendor-specific profilers for detailed analysis, use open-source and cross-platform tools to get the \\\"big picture\\\" across diverse hardware, and keep an eye on emerging tools that can be adopted to improve continuous performance monitoring. By combining these tools and techniques, one can obtain a comprehensive understanding of performance for GPUs, DPUs, and APUs across any workload – from a single GPU kernel's instruction stalls up to the end-to-end behavior of an entire heterogeneous pipeline. Such deep and broad profiling capability will be essential to fully exploit the computational power of modern accelerators in general-purpose and domain-specific applications alike.\"]}]\n"])</script><script>self.__next_f.push([1,"59:[\"$\",\"h2\",null,{\"className\":\"content-header\",\"id\":\"references\",\"children\":[[\"$\",\"a\",null,{\"className\":\"break-words\",\"href\":\"#references\",\"aria-hidden\":\"true\",\"tabIndex\":\"-1\",\"children\":[\"$\",\"span\",null,{\"className\":\"content-header-link\",\"children\":[\"$\",\"svg\",null,{\"className\":\"h-5 linkicon w-5\",\"fill\":\"currentColor\",\"viewBox\":\"0 0 20 20\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"children\":[[\"$\",\"path\",null,{\"d\":\"M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z\"}],[\"$\",\"path\",null,{\"d\":\"M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z\"}]]}]}]}],[\"$\",\"strong\",null,{\"children\":\"References\"}]]}]\n"])</script><script>self.__next_f.push([1,"5a:[\"$\",\"ul\",null,{\"children\":[[\"$\",\"li\",null,{\"children\":[\"$\",\"a\",null,{\"className\":\"break-words\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"href\":\"https://blogs.nvidia.com/blog/whats-a-dpu-data-processing-unit/\",\"children\":\"NVIDIA Blog - \\\"What Is a DPU?\\\"\"}]}],[\"$\",\"li\",null,{\"children\":[\"$\",\"a\",null,{\"className\":\"break-words\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"href\":\"https://www.amd.com/en/products/accelerators/instinct/mi300/mi300a.html\",\"children\":\"AMD - \\\"AMD Instinct™ MI300A Accelerators\\\"\"}]}],[\"$\",\"li\",null,{\"children\":[\"$\",\"a\",null,{\"className\":\"break-words\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"href\":\"https://www.admin-magazine.com/HPC/Articles/Profiling-Is-the-Key-to-Survival\",\"children\":\"ADMIN Magazine - \\\"Profiling Is the Key to Survival\\\"\"}]}],[\"$\",\"li\",null,{\"children\":[\"$\",\"a\",null,{\"className\":\"break-words\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"href\":\"https://dev.to/ethgraham/snooping-on-your-gpu-using-ebpf-to-build-zero-instrumentation-cuda-monitoring-2hh1\",\"children\":\"DEV Community - \\\"Snooping on your GPU: Using eBPF to Build Zero-instrumentation CUDA Monitoring\\\"\"}]}],[\"$\",\"li\",null,{\"children\":[\"$\",\"a\",null,{\"className\":\"break-words\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"href\":\"https://documentation.help/codexl/using-the-gpu-profiler.htm\",\"children\":\"CodeXL Documentation - \\\"Using the GPU Profiler\\\"\"}]}],[\"$\",\"li\",null,{\"children\":[\"$\",\"a\",null,{\"className\":\"break-words\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"href\":\"https://community.amd.com/sdtpp67534/attachments/sdtpp67534/codexl-discussions/62/1/CodeXL_Release_Notes.pdf\",\"children\":\"AMD Community - \\\"CodeXL 2.6 GA Release Notes\\\"\"}]}],[\"$\",\"li\",null,{\"children\":[\"$\",\"a\",null,{\"className\":\"break-words\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"href\":\"https://rocm.docs.amd.com/en/docs-6.2.0/about/release-notes.html\",\"children\":\"ROCm Documentation - \\\"ROCm 6.2.0 release notes\\\"\"}]}],[\"$\",\"li\",null,{\"children\":[\"$\",\"a\",null,{\"className\":\"break-words\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"href\":\"https://www.performance-intensive-computing.com/objectives/developing-ai-and-hpc-solutions-check-out-the-new-amd-rocm-62-release\",\"children\":\"Performance Intensive Computing - \\\"Developing AI and HPC solutions? Check out the new AMD ROCm\\\"\"}]}],[\"$\",\"li\",null,{\"children\":[\"$\",\"a\",null,{\"className\":\"break-words\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"href\":\"https://www.lei.chat/posts/sampling-performance-counters-from-gpu-drivers/\",\"children\":\"Lei.Chat - \\\"Sampling Performance Counters from Mobile GPU Drivers\\\"\"}]}],[\"$\",\"li\",null,{\"children\":[\"$\",\"a\",null,{\"className\":\"break-words\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"href\":\"https://gpuopen.com/gpuperfapi/\",\"children\":\"AMD GPUOpen - \\\"GPUPerfAPI\\\"\"}]}],[\"$\",\"li\",null,{\"children\":[\"$\",\"a\",null,{\"className\":\"break-words\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"href\":\"https://biztechmagazine.com/article/2024/09/what-data-processing-unit-and-how-does-it-help-computing-perfcon\",\"children\":\"BizTech Magazine - \\\"What Is a DPU (Data Processing Unit)? How Does It Work?\\\"\"}]}],[\"$\",\"li\",null,{\"children\":[\"$\",\"a\",null,{\"className\":\"break-words\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"href\":\"https://www.linkedin.com/posts/ebpf-summit_gpu-profiling-with-bpf-at-meta-riham-selim-activity-7138119919023419392-prW4\",\"children\":\"LinkedIn - eBPF Summit - \\\"GPU Profiling with BPF at Meta - Riham Selim\\\"\"}]}],[\"$\",\"li\",null,{\"children\":[\"$\",\"a\",null,{\"className\":\"break-words\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"href\":\"https://github.com/GPUprobe/gpuprobe-daemon\",\"children\":\"GitHub - \\\"GPUprobe/gpuprobe-daemon\\\"\"}]}]]}]\n"])</script><script>self.__next_f.push([1,"5b:[\"$\",\"div\",null,{\"className\":\"pt-6 pb-6 text-sm text-gray-700 dark:text-gray-300 glass p-4 rounded-xl\",\"children\":[[\"$\",\"a\",null,{\"className\":\"hover:text-primary-600 dark:hover:text-primary-400 transition-colors\",\"target\":\"_blank\",\"rel\":\"nofollow\",\"href\":\"https://mobile.twitter.com/search?q=https%3A%2F%2Fwww.yunwei37.com%2Fblog%2Fgpu-profile-tools-analysis\",\"children\":\"Discuss on Twitter\"}],\" • \",[\"$\",\"a\",null,{\"className\":\"hover:text-primary-600 dark:hover:text-primary-400 transition-colors\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"href\":\"https://github.com/yunwei37/yunwei37-blog/blob/main/data/blog/gpu-profile-tools-analysis.mdx\",\"children\":\"View on GitHub\"}]]}]\n5c:[\"$\",\"div\",null,{\"className\":\"pt-6 pb-6 text-center text-gray-700 dark:text-gray-300 glass p-6 rounded-xl\",\"id\":\"comment\",\"children\":[\"$\",\"$L69\",null,{\"slug\":\"gpu-profile-tools-analysis\"}]}]\n"])</script><script>self.__next_f.push([1,"5d:[\"$\",\"footer\",null,{\"children\":[[\"$\",\"div\",null,{\"className\":\"divide-gray-200/30 text-sm leading-5 font-medium xl:col-start-1 xl:row-start-2 xl:divide-y dark:divide-gray-700/30\",\"children\":[[\"$\",\"div\",null,{\"className\":\"py-4 xl:py-8\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"text-xs tracking-wide text-gray-600 uppercase dark:text-gray-300 mb-4\",\"children\":\"Tags\"}],[\"$\",\"div\",null,{\"className\":\"flex flex-wrap gap-2\",\"children\":[[\"$\",\"$L6\",\"ebpf\",{\"href\":\"/tags/ebpf\",\"className\":\"inline-block px-3 py-1 text-xs font-medium uppercase tracking-wide bg-primary-100 hover:bg-primary-200 dark:bg-primary-900/30 dark:hover:bg-primary-800/40 rounded-full transition-all duration-200 text-primary-800 hover:text-primary-900 dark:text-primary-300 dark:hover:text-primary-200 border border-primary-200/50 dark:border-primary-700/50\",\"children\":\"ebpf\"}],[\"$\",\"$L6\",\"systems\",{\"href\":\"/tags/systems\",\"className\":\"inline-block px-3 py-1 text-xs font-medium uppercase tracking-wide bg-primary-100 hover:bg-primary-200 dark:bg-primary-900/30 dark:hover:bg-primary-800/40 rounded-full transition-all duration-200 text-primary-800 hover:text-primary-900 dark:text-primary-300 dark:hover:text-primary-200 border border-primary-200/50 dark:border-primary-700/50\",\"children\":\"systems\"}]]}]]}],[\"$\",\"div\",null,{\"className\":\"flex justify-between py-4 xl:block xl:space-y-6 xl:py-8\",\"children\":[[\"$\",\"div\",null,{\"className\":\"glass p-4 rounded-xl\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"text-xs tracking-wide text-gray-600 uppercase dark:text-gray-300 mb-2\",\"children\":\"Previous Article\"}],[\"$\",\"div\",null,{\"className\":\"text-primary-600 hover:text-primary-700 dark:text-primary-400 dark:hover:text-primary-300 transition-colors\",\"children\":[\"$\",\"$L6\",null,{\"className\":\"break-words\",\"href\":\"/blog/aios\",\"children\":\"OS-Level Challenges in LLM Inference and Optimizations\"}]}]]}],[\"$\",\"div\",null,{\"className\":\"glass p-4 rounded-xl\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"text-xs tracking-wide text-gray-600 uppercase dark:text-gray-300 mb-2\",\"children\":\"Next Article\"}],[\"$\",\"div\",null,{\"className\":\"text-primary-600 hover:text-primary-700 dark:text-primary-400 dark:hover:text-primary-300 transition-colors\",\"children\":[\"$\",\"$L6\",null,{\"className\":\"break-words\",\"href\":\"/blog/gpu-profile-tool-impl\",\"children\":\"GPU Profiling Under the Hood: An Implementation-Focused Survey of Modern Accelerator Tracing Tools\"}]}]]}]]}]]}],[\"$\",\"div\",null,{\"className\":\"pt-4 xl:pt-8\",\"children\":[\"$\",\"$L6\",null,{\"className\":\"inline-flex items-center glass px-4 py-2 rounded-xl text-primary-600 hover:text-primary-700 dark:text-primary-400 dark:hover:text-primary-300 transition-all duration-200 hover:scale-105\",\"href\":\"/blog\",\"aria-label\":\"Back to the blog\",\"children\":\"← Back to the blog\"}]}]]}]\n"])</script><script>self.__next_f.push([1,"5e:[\"$\",\"em\",null,{\"children\":\"GPUPerfAPI\"}]\n"])</script><script>self.__next_f.push([1,"5f:[\"$\",\"li\",null,{\"children\":[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Intel and Other GPU Tools:\"}],\" Intel's GPUs (integrated Iris Xe and data-center GPUs like Ponte Vecchio) can be profiled by Intel's oneAPI toolset. \",[\"$\",\"strong\",null,{\"children\":\"Intel VTune Profiler\"}],\" supports GPU offload analysis, providing kernel timelines, EU (execution unit) occupancy, and memory bandwidth for Intel GPUs. Intel also offers \",[\"$\",\"strong\",null,{\"children\":\"Graphics Performance Analyzers (GPA)\"}],\" for game/graphics profiling on Intel hardware. Notably, Intel has open-sourced a suite called \",[\"$\",\"em\",null,{\"children\":\"Profiling Tools Interface (PTI) for GPU\"}],\", which includes lightweight tracing tools for oneAPI Level Zero and OpenCL applications. These command-line tools (available on GitHub) can trace GPU kernel submissions, memory operations, etc., on Intel GPUs, reflecting Intel's push for an open profiling ecosystem. Beyond the big three vendors, there are domain-specific GPU profilers: e.g., ARM's \",[\"$\",\"em\",null,{\"children\":\"Mali\"}],\" GPUs (for mobile) have \",[\"$\",\"strong\",null,{\"children\":\"ARM Mobile Studio\"}],\" with tools like Streamline for profiling mobile GPU workloads; Qualcomm Adreno GPUs can be analyzed with Qualcomm's Snapdragon Profiler. These are more specialized but underscore that across vendors, profiling often requires \",[\"$\",\"em\",null,{\"children\":\"proprietary SDKs or tools\"}],\" unique to each architecture, with little standardization – a pain point if one needs to support multiple GPU vendors.\"]}]}]\n"])</script><script>self.__next_f.push([1,"60:[\"$\",\"li\",null,{\"children\":[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"HPC and Cross-Platform Profiling:\"}],\" Outside vendor-specific utilities, the HPC community has developed powerful \",[\"$\",\"strong\",null,{\"children\":\"open-source profiling frameworks\"}],\" that work across CPUs and accelerators. \",[\"$\",\"strong\",null,{\"children\":\"HPCToolkit\"}],\" is a prominent example: it uses statistical sampling to profile both CPU and GPU execution with minimal overhead (often \",[\"$\",\"code\",null,{\"children\":\"\u003c5%\"}],\"). HPCToolkit can trace GPU operations (kernels, memcopies, sync) on NVIDIA, AMD, and Intel GPUs, and on NVIDIA it even leverages hardware PC sampling to measure instruction-level execution and stall cycles. The tool correlates GPU activity back to CPU call stacks, allowing a \",[\"$\",\"em\",null,{\"children\":\"unified profile\"}],\" attributing GPU costs to the calling CPU code context. This is invaluable for \",[\"$\",\"em\",null,{\"children\":\"heterogeneous applications\"}],\", e.g. identifying which CPU-side function launched a slow GPU kernel. Other cross-platform tools include \",[\"$\",\"strong\",null,{\"children\":\"TAU\"}],\" and \",[\"$\",\"strong\",null,{\"children\":\"Score-P/Extrae\"}],\", which can instrument MPI+GPU programs and produce integrated traces. For instance, the Extrae tracer (from BSC) records CPU events and CUDA runtime events, enabling visualization in Paraver of both CPU timeline and GPU kernel timelines. These OSS tools typically support multiple accelerators via plugins (CUPTI for CUDA, ROCm tools for AMD, etc.), providing vendor-neutral analysis. They may not always expose the full depth of vendor tools' metrics, but they excel in \",[\"$\",\"em\",null,{\"children\":\"coordinating multi-node, multi-accelerator traces\"}],\". Academic efforts like \",[\"$\",\"strong\",null,{\"children\":\"Paraver/Extrae, Vampir, and Allinea MAP (Arm Forge)\"}],\" have evolved to handle GPU-accelerated HPC codes, indicating a trend toward unified performance analysis for heterogeneous systems.\"]}]}]\n"])</script><script>self.__next_f.push([1,"61:[\"$\",\"li\",null,{\"children\":[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Graphics and Game Profiling:\"}],\" In graphics domains, \",[\"$\",\"strong\",null,{\"children\":\"tracing GPU workloads\"}],\" often involves capturing API calls and GPU command streams. Open-source projects like \",[\"$\",\"strong\",null,{\"children\":\"RenderDoc\"}],\" allow frame-by-frame capture of Vulkan/OpenGL/Direct3D calls, with introspection of GPU draw call timing (helpful for graphics debugging/performance). While RenderDoc is more of a debugger, it can give insights on whether the GPU is bound by certain draw calls or shaders. Platform-specific tools exist too: Microsoft's \",[\"$\",\"strong\",null,{\"children\":\"PIX\"}],\" (for DirectX on Windows/Xbox) provides detailed GPU timing for each render pass, and even Linux has tools like \",[\"$\",\"strong\",null,{\"children\":\"GPUView\"}],\" that trace kernel-level GPU scheduling events for graphics workloads. These are highly domain-specific (targeting graphics pipelines rather than general compute). They complement general GPU profilers by focusing on frame rendering latency, vs. kernel throughput.\"]}]}]\n"])</script><script>self.__next_f.push([1,"62:[\"$\",\"li\",null,{\"children\":[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Graphics and Visualization:\"}],\" For game engines or VR apps on GPUs (especially APUs in consoles or laptops), tools like RenderDoc, Nsight Graphics, and platform profilers (e.g., Apple's Xcode Instruments for Metal) are tailored to measure \",[\"$\",\"em\",null,{\"children\":\"frame times\"}],\", \",[\"$\",\"em\",null,{\"children\":\"GPU pipeline stages\"}],\", and \",[\"$\",\"em\",null,{\"children\":\"CPU-GPU synchronization\"}],\". A graphics workload is typically limited by either the GPU shader throughput or the CPU draw-call submission rate. Profiling maps to checking if the GPU's frame time budget is being exceeded and why (which stage – vertex shading? fragment? memory?). These tools often provide specialized visualizations (HUD overlays, frame scrubbers) that general compute profilers don't. For APUs handling graphics, one must also consider that the CPU and GPU share memory bandwidth – graphics debuggers can show if CPU memory traffic (e.g., updating textures) is affecting the GPU. Additionally, in professional visualization (CAD, etc.), GPU memory usage can be limiting; tools like NVIDIA's Nsight or AMD's RGPA can profile VRAM usage and cache behavior to optimize large models.\"]}]}]\n"])</script><script>self.__next_f.push([1,"63:[\"$\",\"li\",null,{\"children\":[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Causal Tracing and Causal Profiling:\"}],\" Traditional profiling observes performance passively. A newer research direction is \",[\"$\",\"em\",null,{\"children\":\"causal profiling\"}],\", where the profiler experiments by perturbing execution to gauge impact on performance (e.g., artificially slow down one component to see if others idle, determining causality of bottlenecks). Omnitrace's mention of causal profiling support hints that such techniques are being implemented for GPU/CPU combos. Causal tracing could identify, for instance, that GPU kernel X finishing late is what delays CPU task Y, by seeing how timings change if X is made faster or slower. This is an advanced capability with potential to untangle complex dependencies in asynchronous pipelines.\"]}]}]\n"])</script><script>self.__next_f.push([1,"64:[\"$\",\"li\",null,{\"children\":[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Multi-Accelerator and Distributed Coordination:\"}],\" As systems integrate CPUs + various accelerators (GPUs, DPUs, FPGAs, TPUs, etc.), one research challenge is coordinating profiling across them. How do we get a coherent timeline or profile when parts of the work happen on different chips with their own clocks and trace buffers? Tools like \",[\"$\",\"strong\",null,{\"children\":\"Extrae/Paraver\"}],\" in HPC can merge traces from CPU and GPU by aligning timestamps (assuming synchronized clocks) and allow analyzing them together. We expect further development here, possibly with standard timestamping (PTP – precision time protocol – could be used to sync time between host and DPU, for example). Also, orchestrating triggers across tools – e.g., start a GPU profiler when a network event happens – is being explored. Some current tools allow triggers (Nsight can start/stop based on CUDA API calls or markers); extending this across devices (start GPU trace when DPU's packet queue overflows) could be extremely useful for diagnosing cross-stack performance issues (like a slow GPU causing packet backlog on a SmartNIC).\"]}]}]\n"])</script><script>self.__next_f.push([1,"65:[\"$\",\"li\",null,{\"children\":[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Better Profiling for New Accelerator Types:\"}],\" While this report focuses on GPUs, DPUs, and APUs, the universe of accelerators includes FPGAs, AI ASICs (e.g., Google's TPUs), and more. Each is spawning its own tooling – Xilinx (AMD) FPGAs have the Vivado and Vitis analyzer for hardware kernels, Google TPUs have a profiler in TensorBoard, etc. A clear direction is to bring these together. If a cloud has CPUs, GPUs, DPUs, TPUs all working in tandem, the dream is a \",[\"$\",\"em\",null,{\"children\":\"single pane of glass\"}],\" to observe them. Industry consortia may eventually collaborate on open standards for accelerator telemetry (analogous to how OpenCL was a standard for compute). Until then, research often steps in: for example, academic work on monitoring FPGAs in datacenters via in-fabric monitors, or using eBPF-like techniques on other devices.\"]}]}]\n"])</script><script>self.__next_f.push([1,"66:[\"$\",\"li\",null,{\"children\":[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Lack of Multi-Accelerator Coordination:\"}],\" Today's tools largely operate in isolation per device. If you have a heterogeneous node with CPU, GPU, DPU, FPGA, each might be profiled separately, yielding separate reports that the user must correlate. Suppose a performance issue is due to a mismatch between GPU throughput and NIC throughput – a GPU profiler might just show the GPU is idle 20% of time (waiting for data), and a NIC monitor shows 100% utilization on a queue – it's up to the engineer to correlate those and deduce the cause (network-bound). Ideally, a profiler would capture such cross-device dependency automatically (e.g., a visual cue that GPU idleness correlates with NIC saturation). Achieving that requires a holistic view and perhaps standardized trace events that can link across devices (like an event on NIC \\\"frame delivered\\\" could be tied to an event \\\"frame processed on GPU\\\"). Without common standards, such correlation is mostly manual or via ad-hoc instrumentation (inserting timestamps in app code).\"]}]}]\n"])</script><script>self.__next_f.push([1,"67:[\"$\",\"li\",null,{\"children\":[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Scaling and Big Data Problems:\"}],\" When profiling large-scale workloads (think 1000 GPUs or a DPU handling millions of packets per second), the volume of profiling data can be enormous. Storing and analyzing trace logs from even a few seconds of operation may be non-trivial. There is a challenge in \",[\"$\",\"strong\",null,{\"children\":\"data reduction\"}],\" – how to summarize performance data meaningfully. Current tools offer some summaries (like average kernel time, top 10 memory consumers, etc.), but more automated summarization is needed, possibly with hierarchical or statistical techniques to condense traces. HPC centers have dealt with this by selective tracing (capturing only on a few ranks, etc.) or using sampling. Future tools might incorporate on-line analysis, where the tool itself does some processing of data as it's collected (for example, computing distributions instead of logging every event).\"]}]}]\n"])</script><script>self.__next_f.push([1,"68:[\"$\",\"li\",null,{\"children\":[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Education and Usability:\"}],\" Finally, it's worth noting a practical challenge: the learning curve. Each tool often comes with its own GUI or output format, and understanding metrics like \\\"warp serialize\\\" or \\\"bank conflict\\\" or \\\"DPU cache hit\\\" requires some architecture knowledge. Performance analysis on these accelerators is somewhat a dark art, and although tools provide data, making sense of it is not always straightforward. Efforts like roofline models are attempts to simplify interpretation, but users still struggle to go from profiler output to concrete optimizations. This is partially an educational challenge – documentation and training need to accompany tools. It's also a design challenge for tool builders to present data in intuitive ways (e.g., Nvidia now often integrates AI performance metrics like \\\"utilization of Tensor Cores\\\" directly, to tell ML engineers how well they used the GPU).\"]}]}]\n"])</script><script>self.__next_f.push([1,"23:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"meta\",\"1\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}]]\n1f:null\n"])</script><script>self.__next_f.push([1,"21:{\"metadata\":[[\"$\",\"title\",\"0\",{\"children\":\"The Accelerator Toolkit: A Review of Profiling and Tracing for GPUs and other co-processor | 云微的胡思乱想\"}],[\"$\",\"meta\",\"1\",{\"name\":\"robots\",\"content\":\"index, follow\"}],[\"$\",\"meta\",\"2\",{\"name\":\"googlebot\",\"content\":\"index, follow, max-video-preview:-1, max-image-preview:large, max-snippet:-1\"}],[\"$\",\"link\",\"3\",{\"rel\":\"canonical\",\"href\":\"https://www.yunwei37.com/blog/gpu-profile-tools-analysis\"}],[\"$\",\"link\",\"4\",{\"rel\":\"alternate\",\"type\":\"application/rss+xml\",\"href\":\"https://www.yunwei37.com/feed.xml\"}],[\"$\",\"meta\",\"5\",{\"property\":\"og:title\",\"content\":\"The Accelerator Toolkit: A Review of Profiling and Tracing for GPUs and other co-processor\"}],[\"$\",\"meta\",\"6\",{\"property\":\"og:url\",\"content\":\"https://www.yunwei37.com/blog/gpu-profile-tools-analysis\"}],[\"$\",\"meta\",\"7\",{\"property\":\"og:site_name\",\"content\":\"云微的胡思乱想\"}],[\"$\",\"meta\",\"8\",{\"property\":\"og:locale\",\"content\":\"en_US\"}],[\"$\",\"meta\",\"9\",{\"property\":\"og:image\",\"content\":\"https://www.yunwei37.com/static/images/twitter-card.png\"}],[\"$\",\"meta\",\"10\",{\"property\":\"og:type\",\"content\":\"article\"}],[\"$\",\"meta\",\"11\",{\"property\":\"article:published_time\",\"content\":\"2025-04-11T16:00:00.000Z\"}],[\"$\",\"meta\",\"12\",{\"property\":\"article:modified_time\",\"content\":\"2025-04-11T16:00:00.000Z\"}],[\"$\",\"meta\",\"13\",{\"property\":\"article:author\",\"content\":\"Yusheng Zheng (云微)\"}],[\"$\",\"meta\",\"14\",{\"name\":\"twitter:card\",\"content\":\"summary_large_image\"}],[\"$\",\"meta\",\"15\",{\"name\":\"twitter:title\",\"content\":\"The Accelerator Toolkit: A Review of Profiling and Tracing for GPUs and other co-processor\"}],[\"$\",\"meta\",\"16\",{\"name\":\"twitter:image\",\"content\":\"https://www.yunwei37.com/static/images/twitter-card.png\"}]],\"error\":null,\"digest\":\"$undefined\"}\n"])</script><script>self.__next_f.push([1,"26:\"$21:metadata\"\n"])</script></body></html>