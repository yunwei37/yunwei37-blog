1:"$Sreact.fragment"
2:I[1478,["874","static/chunks/874-8edb22cc7428423c.js","650","static/chunks/650-5bb235cd9bfca45d.js","177","static/chunks/app/layout-66df3ac28fcfbced.js"],"ThemeProviders"]
3:I[4091,["874","static/chunks/874-8edb22cc7428423c.js","650","static/chunks/650-5bb235cd9bfca45d.js","177","static/chunks/app/layout-66df3ac28fcfbced.js"],"default"]
4:I[9243,["874","static/chunks/874-8edb22cc7428423c.js","650","static/chunks/650-5bb235cd9bfca45d.js","177","static/chunks/app/layout-66df3ac28fcfbced.js"],""]
5:I[7392,["874","static/chunks/874-8edb22cc7428423c.js","650","static/chunks/650-5bb235cd9bfca45d.js","177","static/chunks/app/layout-66df3ac28fcfbced.js"],"KBarSearchProvider"]
6:I[6874,["874","static/chunks/874-8edb22cc7428423c.js","63","static/chunks/63-d245e42a784ca56d.js","909","static/chunks/app/blog/%5B...slug%5D/page-1ed2702378ba9d6b.js"],""]
e:I[8393,[],""]
:HL["/_next/static/media/36966cca54120369-s.p.woff2","font",{"crossOrigin":"","type":"font/woff2"}]
:HL["/_next/static/css/4ac7c0f872b74ee7.css","style"]
:HL["/_next/static/css/a06d7e24bf9a7d93.css","style"]
:HL["/_next/static/css/7246298b30c42979.css","style"]
0:{"P":null,"b":"MYXrWUjg0HELsl6zLYF_6","p":"","c":["","blog","gpu-profile-tool-impl"],"i":false,"f":[[["",{"children":["blog",{"children":[["slug","gpu-profile-tool-impl","c"],{"children":["__PAGE__",{}]}]}]},"$undefined","$undefined",true],["",["$","$1","c",{"children":[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/4ac7c0f872b74ee7.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}],["$","link","1",{"rel":"stylesheet","href":"/_next/static/css/a06d7e24bf9a7d93.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}]],["$","html",null,{"lang":"en-us","className":"__variable_dd5b2f scroll-smooth","suppressHydrationWarning":true,"children":[["$","link",null,{"rel":"apple-touch-icon","sizes":"76x76","href":"/static/favicons/apple-touch-icon.png"}],["$","link",null,{"rel":"icon","type":"image/svg+xml","href":"/static/favicons/favicon.svg"}],["$","link",null,{"rel":"icon","type":"image/png","sizes":"32x32","href":"/static/favicons/favicon-32x32.png"}],["$","link",null,{"rel":"icon","type":"image/png","sizes":"16x16","href":"/static/favicons/favicon-16x16.png"}],["$","link",null,{"rel":"manifest","href":"/static/favicons/site.webmanifest"}],["$","link",null,{"rel":"mask-icon","href":"/static/favicons/safari-pinned-tab.svg","color":"#5bbad5"}],["$","meta",null,{"name":"msapplication-TileColor","content":"#000000"}],["$","meta",null,{"name":"theme-color","media":"(prefers-color-scheme: light)","content":"#fff"}],["$","meta",null,{"name":"theme-color","media":"(prefers-color-scheme: dark)","content":"#000"}],["$","link",null,{"rel":"alternate","type":"application/rss+xml","href":"/feed.xml"}],["$","body",null,{"className":"pl-[calc(100vw-100%)] text-black antialiased dark:text-white min-h-screen","children":["$","$L2",null,{"children":[["$","$L3",null,{}],["$undefined","$undefined","$undefined",["$","$L4",null,{"async":true,"defer":true,"src":"https://analytics.umami.is/script.js"}],"$undefined","$undefined"],["$","div",null,{"className":"min-h-screen flex flex-col justify-center py-8","children":["$","div",null,{"className":"content-glass mx-auto w-full max-w-5xl px-4 sm:px-6 xl:px-8 py-8","children":["$","$L5",null,{"kbarConfig":{"searchDocumentsPath":"/search.json"},"children":[["$","header",null,{"className":"flex items-center w-full justify-between py-6 border-b border-gray-200/30 dark:border-gray-700/30","children":[["$","$L6",null,{"className":"break-words","href":"/","aria-label":"yunwei37","children":["$","div",null,{"className":"flex items-center","children":[["$","div",null,{"className":"mr-3","children":["$","svg",null,{"xmlns":"http://www.w3.org/2000/svg","width":40,"height":40,"fill":"none","children":[["$","defs",null,{"children":[["$","linearGradient",null,{"id":"logo_svg__a","x1":"0%","x2":"100%","y1":"0%","y2":"100%","children":[["$","stop",null,{"offset":"0%","style":{"stopColor":"#3b82f6","stopOpacity":1}}],["$","stop",null,{"offset":"100%","style":{"stopColor":"#06b6d4","stopOpacity":1}}]]}],["$","linearGradient",null,{"id":"logo_svg__b","x1":"0%","x2":"100%","y1":"0%","y2":"100%","children":[["$","stop",null,{"offset":"0%","style":{"stopColor":"#8b5cf6","stopOpacity":1}}],["$","stop",null,{"offset":"100%","style":{"stopColor":"#3b82f6","stopOpacity":1}}]]}]]}],["$","circle",null,{"cx":20,"cy":20,"r":18,"fill":"url(#logo_svg__a)","opacity":0.1}],["$","path",null,{"fill":"url(#logo_svg__a)","d":"m12 8 6 10v10h4V18l6-10h-4l-4 6-4-6Z"}],["$","path",null,{"fill":"url(#logo_svg__b)","d":"m8 24 4 8 4-8 4 8 4-8h8v4h-6l-4 8-4-8-4 8-4-8H8Z","opacity":0.8}],["$","circle",null,{"cx":32,"cy":12,"r":2,"fill":"url(#logo_svg__b)"}],["$","circle",null,{"cx":8,"cy":12,"r":1.5,"fill":"url(#logo_svg__a)","opacity":0.6}]]}]}],["$","div",null,{"className":"hidden text-2xl font-bold sm:block hover:text-primary-600 dark:hover:text-primary-400 transition-colors","children":"yunwei37"}]]}]}],["$","div",null,{"className":"flex items-center space-x-4","children":[["$","div",null,{"className":"no-scrollbar hidden items-center space-x-2 sm:flex","children":[["$","$L6","Blog",{"className":"px-3 py-2 rounded-lg transition-all duration-200 font-medium hover:text-primary-600 dark:hover:text-primary-400","href":"/blog","children":"Blog"}],["$","$L6","Docs",{"className":"px-3 py-2 rounded-lg transition-all duration-200 font-medium hover:text-primary-600 dark:hover:text-primary-400","href":"/docs","children":"Docs"}],["$","$L6","Tags",{"className":"px-3 py-2 rounded-lg transition-all duration-200 font-medium hover:text-primary-600 dark:hover:text-primary-400","href":"/tags","children":"Tags"}],["$","$L6","About",{"className":"px-3 py-2 rounded-lg transition-all duration-200 font-medium hover:text-primary-600 dark:hover:text-primary-400","href":"/about","children":"About"}]]}],"$L7"]}]]}],"$L8","$L9"]}]}]}]]}]}]]}]]}],{"children":["blog","$La",{"children":[["slug","gpu-profile-tool-impl","c"],"$Lb",{"children":["__PAGE__","$Lc",{},null,false]},null,false]},null,false]},null,false],"$Ld",false]],"m":"$undefined","G":["$e",[]],"s":false,"S":true}
f:I[4159,["874","static/chunks/874-8edb22cc7428423c.js","650","static/chunks/650-5bb235cd9bfca45d.js","177","static/chunks/app/layout-66df3ac28fcfbced.js"],"KBarButton"]
10:I[1762,["874","static/chunks/874-8edb22cc7428423c.js","650","static/chunks/650-5bb235cd9bfca45d.js","177","static/chunks/app/layout-66df3ac28fcfbced.js"],"default"]
11:I[7298,["874","static/chunks/874-8edb22cc7428423c.js","650","static/chunks/650-5bb235cd9bfca45d.js","177","static/chunks/app/layout-66df3ac28fcfbced.js"],"default"]
12:I[7555,[],""]
13:I[1295,[],""]
1e:I[9665,[],"OutletBoundary"]
20:I[4911,[],"AsyncMetadataOutlet"]
22:I[9665,[],"ViewportBoundary"]
24:I[9665,[],"MetadataBoundary"]
25:"$Sreact.suspense"
7:["$","div",null,{"className":"flex items-center space-x-2","children":[["$","$Lf",null,{"aria-label":"Search","className":"p-2 rounded-lg transition-all duration-200","children":["$","svg",null,{"xmlns":"http://www.w3.org/2000/svg","fill":"none","viewBox":"0 0 24 24","strokeWidth":1.5,"stroke":"currentColor","className":"h-6 w-6 hover:text-primary-600 dark:hover:text-primary-400 transition-colors","children":["$","path",null,{"strokeLinecap":"round","strokeLinejoin":"round","d":"M21 21l-5.197-5.197m0 0A7.5 7.5 0 105.196 5.196a7.5 7.5 0 0010.607 10.607z"}]}]}],["$","$L10",null,{}],["$","$L11",null,{}]]}]
8:["$","main",null,{"className":"py-6","children":["$","$L12",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L13",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[["$","div",null,{"className":"flex flex-col items-start justify-start md:mt-24 md:flex-row md:items-center md:justify-center md:space-x-6","children":[["$","div",null,{"className":"space-x-2 pt-6 pb-8 md:space-y-5","children":["$","h1",null,{"className":"text-6xl leading-9 font-extrabold tracking-tight md:border-r-2 md:px-6 md:text-8xl md:leading-14","children":"404"}]}],["$","div",null,{"className":"max-w-md","children":[["$","p",null,{"className":"mb-4 text-xl leading-normal font-bold md:text-2xl","children":"Sorry we couldn't find this page."}],["$","p",null,{"className":"mb-8","children":"But dont worry, you can find plenty of other things on our homepage."}],["$","$L6",null,{"className":"focus:shadow-outline-blue inline rounded-lg border border-transparent bg-blue-600 px-4 py-2 text-sm leading-5 font-medium text-white shadow-xs transition-colors duration-150 hover:bg-blue-700 focus:outline-hidden dark:hover:bg-blue-500","href":"/","children":"Back to homepage"}]]}]]}],[]],"forbidden":"$undefined","unauthorized":"$undefined"}]}]
9:["$","footer",null,{"className":"border-t border-gray-200/30 dark:border-gray-700/30 pt-8","children":["$","div",null,{"className":"flex flex-col items-center space-y-6","children":[["$","div",null,{"className":"flex flex-wrap justify-center gap-4","children":[["$","a",null,{"className":"text-sm text-gray-500 transition hover:text-gray-600","target":"_blank","rel":"noopener noreferrer","href":"mailto:yunwei356@gmail.com","children":[["$","span",null,{"className":"sr-only","children":"mail"}],["$","svg",null,{"xmlns":"http://www.w3.org/2000/svg","viewBox":"0 0 20 20","className":"hover:text-primary-500 dark:hover:text-primary-400 fill-current text-gray-700 dark:text-gray-200 h-6 w-6","children":[["$","title",null,{"children":"Mail"}],["$","path",null,{"d":"M2.003 5.884L10 9.882l7.997-3.998A2 2 0 0016 4H4a2 2 0 00-1.997 1.884z"}],["$","path",null,{"d":"M18 8.118l-8 4-8-4V14a2 2 0 002 2h12a2 2 0 002-2V8.118z"}]]}]]}],["$","a",null,{"className":"text-sm text-gray-500 transition hover:text-gray-600","target":"_blank","rel":"noopener noreferrer","href":"https://github.com/yunwei37","children":[["$","span",null,{"className":"sr-only","children":"github"}],["$","svg",null,{"xmlns":"http://www.w3.org/2000/svg","viewBox":"0 0 24 24","className":"hover:text-primary-500 dark:hover:text-primary-400 fill-current text-gray-700 dark:text-gray-200 h-6 w-6","children":[["$","title",null,{"children":"GitHub"}],["$","path",null,{"d":"M12 .297c-6.63 0-12 5.373-12 12 0 5.303 3.438 9.8 8.205 11.385.6.113.82-.258.82-.577 0-.285-.01-1.04-.015-2.04-3.338.724-4.042-1.61-4.042-1.61C4.422 18.07 3.633 17.7 3.633 17.7c-1.087-.744.084-.729.084-.729 1.205.084 1.838 1.236 1.838 1.236 1.07 1.835 2.809 1.305 3.495.998.108-.776.417-1.305.76-1.605-2.665-.3-5.466-1.332-5.466-5.93 0-1.31.465-2.38 1.235-3.22-.135-.303-.54-1.523.105-3.176 0 0 1.005-.322 3.3 1.23.96-.267 1.98-.399 3-.405 1.02.006 2.04.138 3 .405 2.28-1.552 3.285-1.23 3.285-1.23.645 1.653.24 2.873.12 3.176.765.84 1.23 1.91 1.23 3.22 0 4.61-2.805 5.625-5.475 5.92.42.36.81 1.096.81 2.22 0 1.606-.015 2.896-.015 3.286 0 .315.21.69.825.57C20.565 22.092 24 17.592 24 12.297c0-6.627-5.373-12-12-12"}]]}]]}],["$","a",null,{"className":"text-sm text-gray-500 transition hover:text-gray-600","target":"_blank","rel":"noopener noreferrer","href":"https://facebook.com","children":[["$","span",null,{"className":"sr-only","children":"facebook"}],["$","svg",null,{"xmlns":"http://www.w3.org/2000/svg","viewBox":"0 0 24 24","className":"hover:text-primary-500 dark:hover:text-primary-400 fill-current text-gray-700 dark:text-gray-200 h-6 w-6","children":[["$","title",null,{"children":"Facebook"}],["$","path",null,{"d":"M24 12.073c0-6.627-5.373-12-12-12s-12 5.373-12 12c0 5.99 4.388 10.954 10.125 11.854v-8.385H7.078v-3.47h3.047V9.43c0-3.007 1.792-4.669 4.533-4.669 1.312 0 2.686.235 2.686.235v2.953H15.83c-1.491 0-1.956.925-1.956 1.874v2.25h3.328l-.532 3.47h-2.796v8.385C19.612 23.027 24 18.062 24 12.073z"}]]}]]}],["$","a",null,{"className":"text-sm text-gray-500 transition hover:text-gray-600","target":"_blank","rel":"noopener noreferrer","href":"https://youtube.com","children":[["$","span",null,{"className":"sr-only","children":"youtube"}],["$","svg",null,{"xmlns":"http://www.w3.org/2000/svg","viewBox":"0 0 24 24","className":"hover:text-primary-500 dark:hover:text-primary-400 fill-current text-gray-700 dark:text-gray-200 h-6 w-6","children":[["$","title",null,{"children":"Youtube"}],["$","path",null,{"d":"M23.499 6.203a3.008 3.008 0 00-2.089-2.089c-1.87-.501-9.4-.501-9.4-.501s-7.509-.01-9.399.501a3.008 3.008 0 00-2.088 2.09A31.258 31.26 0 000 12.01a31.258 31.26 0 00.523 5.785 3.008 3.008 0 002.088 2.089c1.869.502 9.4.502 9.4.502s7.508 0 9.399-.502a3.008 3.008 0 002.089-2.09 31.258 31.26 0 00.5-5.784 31.258 31.26 0 00-.5-5.808zm-13.891 9.4V8.407l6.266 3.604z"}]]}]]}],"$L14","$L15","$L16","$L17","$L18","$L19","$L1a"]}],"$L1b","$L1c"]}]}]
a:["$","$1","c",{"children":[null,["$","$L12",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L13",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}]
b:["$","$1","c",{"children":[null,["$","$L12",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L13",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}]
c:["$","$1","c",{"children":["$L1d",[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/7246298b30c42979.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}]],["$","$L1e",null,{"children":["$L1f",["$","$L20",null,{"promise":"$@21"}]]}]]}]
d:["$","$1","h",{"children":[null,[["$","$L22",null,{"children":"$L23"}],["$","meta",null,{"name":"next-size-adjust","content":""}]],["$","$L24",null,{"children":["$","div",null,{"hidden":true,"children":["$","$25",null,{"fallback":null,"children":"$L26"}]}]}]]}]
14:["$","a",null,{"className":"text-sm text-gray-500 transition hover:text-gray-600","target":"_blank","rel":"noopener noreferrer","href":"https://www.linkedin.com/in/yusheng-zheng-611920280","children":[["$","span",null,{"className":"sr-only","children":"linkedin"}],["$","svg",null,{"xmlns":"http://www.w3.org/2000/svg","viewBox":"0 0 24 24","className":"hover:text-primary-500 dark:hover:text-primary-400 fill-current text-gray-700 dark:text-gray-200 h-6 w-6","children":[["$","title",null,{"children":"Linkedin"}],["$","path",null,{"d":"M20.447 20.452h-3.554v-5.569c0-1.328-.027-3.037-1.852-3.037-1.853 0-2.136 1.445-2.136 2.939v5.667H9.351V9h3.414v1.561h.046c.477-.9 1.637-1.85 3.37-1.85 3.601 0 4.267 2.37 4.267 5.455v6.286zM5.337 7.433a2.062 2.062 0 01-2.063-2.065 2.064 2.064 0 112.063 2.065zm1.782 13.019H3.555V9h3.564v11.452zM22.225 0H1.771C.792 0 0 .774 0 1.729v20.542C0 23.227.792 24 1.771 24h20.451C23.2 24 24 23.227 24 22.271V1.729C24 .774 23.2 0 22.222 0h.003z"}]]}]]}]
15:null
16:["$","a",null,{"className":"text-sm text-gray-500 transition hover:text-gray-600","target":"_blank","rel":"noopener noreferrer","href":"https://bsky.app/","children":[["$","span",null,{"className":"sr-only","children":"bluesky"}],["$","svg",null,{"xmlns":"http://www.w3.org/2000/svg","viewBox":"0 0 24 24","className":"hover:text-primary-500 dark:hover:text-primary-400 fill-current text-gray-700 dark:text-gray-200 h-6 w-6","children":[["$","title",null,{"children":"Bluesky"}],["$","path",null,{"d":"M12 10.8c-1.087-2.114-4.046-6.053-6.798-7.995C2.566.944 1.561 1.266.902 1.565C.139 1.908 0 3.08 0 3.768c0 .69.378 5.65.624 6.479c.815 2.736 3.713 3.66 6.383 3.364q.204-.03.415-.056q-.207.033-.415.056c-3.912.58-7.387 2.005-2.83 7.078c5.013 5.19 6.87-1.113 7.823-4.308c.953 3.195 2.05 9.271 7.733 4.308c4.267-4.308 1.172-6.498-2.74-7.078a9 9 0 0 1-.415-.056q.21.026.415.056c2.67.297 5.568-.628 6.383-3.364c.246-.828.624-5.79.624-6.478c0-.69-.139-1.861-.902-2.206c-.659-.298-1.664-.62-4.3 1.24C16.046 4.748 13.087 8.687 12 10.8"}]]}]]}]
17:["$","a",null,{"className":"text-sm text-gray-500 transition hover:text-gray-600","target":"_blank","rel":"noopener noreferrer","href":"https://x.com/yunwei37","children":[["$","span",null,{"className":"sr-only","children":"x"}],["$","svg",null,{"xmlns":"http://www.w3.org/2000/svg","viewBox":"0 0 24 24","className":"hover:text-primary-500 dark:hover:text-primary-400 fill-current text-gray-700 dark:text-gray-200 h-6 w-6","children":[["$","title",null,{"children":"X"}],["$","path",null,{"d":"M18.901 1.153h3.68l-8.04 9.19L24 22.846h-7.406l-5.8-7.584-6.638 7.584H.474l8.6-9.83L0 1.154h7.594l5.243 6.932ZM17.61 20.644h2.039L6.486 3.24H4.298Z"}]]}]]}]
27:T69f,M12 0C8.74 0 8.333.015 7.053.072 5.775.132 4.905.333 4.14.63c-.789.306-1.459.717-2.126 1.384S.935 3.35.63 4.14C.333 4.905.131 5.775.072 7.053.012 8.333 0 8.74 0 12s.015 3.667.072 4.947c.06 1.277.261 2.148.558 2.913.306.788.717 1.459 1.384 2.126.667.666 1.336 1.079 2.126 1.384.766.296 1.636.499 2.913.558C8.333 23.988 8.74 24 12 24s3.667-.015 4.947-.072c1.277-.06 2.148-.262 2.913-.558.788-.306 1.459-.718 2.126-1.384.666-.667 1.079-1.335 1.384-2.126.296-.765.499-1.636.558-2.913.06-1.28.072-1.687.072-4.947s-.015-3.667-.072-4.947c-.06-1.277-.262-2.149-.558-2.913-.306-.789-.718-1.459-1.384-2.126C21.319 1.347 20.651.935 19.86.63c-.765-.297-1.636-.499-2.913-.558C15.667.012 15.26 0 12 0zm0 2.16c3.203 0 3.585.016 4.85.071 1.17.055 1.805.249 2.227.415.562.217.96.477 1.382.896.419.42.679.819.896 1.381.164.422.36 1.057.413 2.227.057 1.266.07 1.646.07 4.85s-.015 3.585-.074 4.85c-.061 1.17-.256 1.805-.421 2.227-.224.562-.479.96-.899 1.382-.419.419-.824.679-1.38.896-.42.164-1.065.36-2.235.413-1.274.057-1.649.07-4.859.07-3.211 0-3.586-.015-4.859-.074-1.171-.061-1.816-.256-2.236-.421-.569-.224-.96-.479-1.379-.899-.421-.419-.69-.824-.9-1.38-.165-.42-.359-1.065-.42-2.235-.045-1.26-.061-1.649-.061-4.844 0-3.196.016-3.586.061-4.861.061-1.17.255-1.814.42-2.234.21-.57.479-.96.9-1.381.419-.419.81-.689 1.379-.898.42-.166 1.051-.361 2.221-.421 1.275-.045 1.65-.06 4.859-.06l.045.03zm0 3.678c-3.405 0-6.162 2.76-6.162 6.162 0 3.405 2.76 6.162 6.162 6.162 3.405 0 6.162-2.76 6.162-6.162 0-3.405-2.76-6.162-6.162-6.162zM12 16c-2.21 0-4-1.79-4-4s1.79-4 4-4 4 1.79 4 4-1.79 4-4 4zm7.846-10.405c0 .795-.646 1.44-1.44 1.44-.795 0-1.44-.646-1.44-1.44 0-.794.646-1.439 1.44-1.439.793-.001 1.44.645 1.44 1.439z18:["$","a",null,{"className":"text-sm text-gray-500 transition hover:text-gray-600","target":"_blank","rel":"noopener noreferrer","href":"https://www.instagram.com","children":[["$","span",null,{"className":"sr-only","children":"instagram"}],["$","svg",null,{"xmlns":"http://www.w3.org/2000/svg","viewBox":"0 0 24 24","className":"hover:text-primary-500 dark:hover:text-primary-400 fill-current text-gray-700 dark:text-gray-200 h-6 w-6","children":[["$","title",null,{"children":"Instagram"}],["$","path",null,{"d":"$27"}]]}]]}]
28:T498,M12.186 24h-.007c-3.581-.024-6.334-1.205-8.184-3.509C2.35 18.44 1.5 15.586 1.472 12.01v-.017c.03-3.579.879-6.43 2.525-8.482C5.845 1.205 8.6.024 12.18 0h.014c2.746.02 5.043.725 6.826 2.098 1.677 1.29 2.858 3.13 3.509 5.467l-2.04.569c-1.104-3.96-3.898-5.984-8.304-6.015-2.91.022-5.11.936-6.54 2.717C4.307 6.504 3.616 8.914 3.589 12c.027 3.086.718 5.496 2.057 7.164 1.43 1.783 3.631 2.698 6.54 2.717 2.623-.02 4.358-.631 5.8-2.045 1.647-1.613 1.618-3.593 1.09-4.798-.31-.71-.873-1.3-1.634-1.75-.192 1.352-.622 2.446-1.284 3.272-.886 1.102-2.14 1.704-3.73 1.79-1.202.065-2.361-.218-3.259-.801-1.063-.689-1.685-1.74-1.752-2.964-.065-1.19.408-2.285 1.33-3.082.88-.76 2.119-1.207 3.583-1.291a13.853 13.853 0 0 1 3.02.142c-.126-.742-.375-1.332-.75-1.757-.513-.586-1.308-.883-2.359-.89h-.029c-.844 0-1.992.232-2.721 1.32L7.734 7.847c.98-1.454 2.568-2.256 4.478-2.256h.044c3.194.02 5.097 1.975 5.287 5.388.108.046.216.094.321.142 1.49.7 2.58 1.761 3.154 3.07.797 1.82.871 4.79-1.548 7.158-1.85 1.81-4.094 2.628-7.277 2.65Zm1.003-11.69c-.242 0-.487.007-.739.021-1.836.103-2.98.946-2.916 2.143.067 1.256 1.452 1.839 2.784 1.767 1.224-.065 2.818-.543 3.086-3.71a10.5 10.5 0 0 0-2.215-.221z19:["$","a",null,{"className":"text-sm text-gray-500 transition hover:text-gray-600","target":"_blank","rel":"noopener noreferrer","href":"https://www.threads.net","children":[["$","span",null,{"className":"sr-only","children":"threads"}],["$","svg",null,{"xmlns":"http://www.w3.org/2000/svg","viewBox":"0 0 24 24","className":"hover:text-primary-500 dark:hover:text-primary-400 fill-current text-gray-700 dark:text-gray-200 h-6 w-6","children":[["$","title",null,{"children":"Threads"}],["$","path",null,{"d":"$28"}]]}]]}]
1a:["$","a",null,{"className":"text-sm text-gray-500 transition hover:text-gray-600","target":"_blank","rel":"noopener noreferrer","href":"https://medium.com","children":[["$","span",null,{"className":"sr-only","children":"medium"}],["$","svg",null,{"xmlns":"http://www.w3.org/2000/svg","viewBox":"0 0 24 24","className":"hover:text-primary-500 dark:hover:text-primary-400 fill-current text-gray-700 dark:text-gray-200 h-6 w-6","children":[["$","title",null,{"children":"Medium"}],["$","path",null,{"d":"M13.54 12a6.8 6.8 0 01-6.77 6.82A6.8 6.8 0 010 12a6.8 6.8 0 016.77-6.82A6.8 6.8 0 0113.54 12zM20.96 12c0 3.54-1.51 6.42-3.38 6.42-1.87 0-3.39-2.88-3.39-6.42s1.52-6.42 3.39-6.42 3.38 2.88 3.38 6.42M24 12c0 3.17-.53 5.75-1.19 5.75-.66 0-1.19-2.58-1.19-5.75s.53-5.75 1.19-5.75C23.47 6.25 24 8.83 24 12z"}]]}]]}]
1b:["$","div",null,{"className":"flex space-x-2 text-sm opacity-80","children":[["$","div",null,{"children":"Yusheng Zheng"}],["$","div",null,{"children":" • "}],["$","div",null,{"children":"© 2026"}],["$","div",null,{"children":" • "}],["$","$L6",null,{"className":"hover:text-primary-500 transition-colors","href":"/","children":"云微的胡思乱想"}]]}]
1c:["$","div",null,{"className":"text-sm opacity-60","children":["$","a",null,{"className":"hover:text-primary-500 transition-colors","target":"_blank","rel":"noopener noreferrer","href":"https://github.com/timlrx/tailwind-nextjs-starter-blog","children":"Tailwind Nextjs Theme"}]}]
29:I[1839,["874","static/chunks/874-8edb22cc7428423c.js","63","static/chunks/63-d245e42a784ca56d.js","909","static/chunks/app/blog/%5B...slug%5D/page-1ed2702378ba9d6b.js"],"default"]
2a:I[3063,["874","static/chunks/874-8edb22cc7428423c.js","63","static/chunks/63-d245e42a784ca56d.js","909","static/chunks/app/blog/%5B...slug%5D/page-1ed2702378ba9d6b.js"],"Image"]
1d:[["$","script",null,{"type":"application/ld+json","dangerouslySetInnerHTML":{"__html":"{\"@context\":\"https://schema.org\",\"@type\":\"BlogPosting\",\"headline\":\"GPU Profiling Under the Hood: An Implementation-Focused Survey of Modern Accelerator Tracing Tools\",\"datePublished\":\"2025-04-21T16:00:00.000Z\",\"dateModified\":\"2025-04-21T16:00:00.000Z\",\"image\":\"/static/images/twitter-card.png\",\"url\":\"https://www.yunwei37.com/blog/gpu-profile-tool-impl\",\"author\":[{\"@type\":\"Person\",\"name\":\"Yusheng Zheng (云微)\"}]}"}}],["$","section",null,{"className":"mx-auto max-w-3xl px-4 sm:px-6 xl:max-w-5xl xl:px-0","children":[["$","$L29",null,{}],["$","article",null,{"className":"content-glass p-8","children":["$","div",null,{"className":"xl:divide-y xl:divide-gray-200/30 xl:dark:divide-gray-700/30","children":[["$","header",null,{"className":"pt-6 xl:pb-6","children":["$","div",null,{"className":"space-y-4 text-center","children":[["$","dl",null,{"className":"space-y-4","children":["$","div",null,{"children":[["$","dt",null,{"className":"sr-only","children":"Published on"}],["$","dd",null,{"className":"text-base leading-6 font-medium text-gray-600 dark:text-gray-300","children":["$","time",null,{"dateTime":"2025-04-21T16:00:00.000Z","children":"Monday, April 21, 2025"}]}]]}]}],["$","div",null,{"children":["$","h1",null,{"className":"text-3xl leading-9 font-extrabold tracking-tight sm:text-4xl sm:leading-10 md:text-5xl md:leading-14","children":"GPU Profiling Under the Hood: An Implementation-Focused Survey of Modern Accelerator Tracing Tools"}]}]]}]}],["$","div",null,{"className":"grid-rows-[auto_1fr] divide-y divide-gray-200/30 pb-8 xl:grid xl:grid-cols-4 xl:gap-x-8 xl:divide-y-0 dark:divide-gray-700/30","children":[["$","dl",null,{"className":"pt-6 pb-10 xl:border-b xl:border-gray-200/30 xl:pt-11 xl:dark:border-gray-700/30","children":[["$","dt",null,{"className":"sr-only","children":"Authors"}],["$","dd",null,{"children":["$","ul",null,{"className":"flex flex-wrap justify-center gap-4 sm:space-x-12 xl:block xl:space-y-6 xl:space-x-0","children":[["$","li","Yusheng Zheng (云微)",{"className":"flex items-center space-x-3 glass p-4 rounded-xl","children":[["$","$L2a",null,{"src":"/static/images/avatar.png","width":38,"height":38,"alt":"avatar","className":"h-10 w-10 rounded-full"}],["$","dl",null,{"className":"text-sm leading-5 font-medium whitespace-nowrap","children":[["$","dt",null,{"className":"sr-only","children":"Name"}],["$","dd",null,{"className":"text-gray-900 dark:text-gray-100","children":"Yusheng Zheng (云微)"}],["$","dt",null,{"className":"sr-only","children":"Twitter"}],["$","dd",null,{"children":["$","a",null,{"className":"text-primary-600 hover:text-primary-700 dark:text-primary-400 dark:hover:text-primary-300 transition-colors","target":"_blank","rel":"noopener noreferrer","href":"https://twitter.com/yunwei37","children":"@yunwei37"}]}]]}]]}]]}]}]]}],["$","div",null,{"className":"divide-y divide-gray-200/30 xl:col-span-3 xl:row-span-2 xl:pb-0 dark:divide-gray-700/30","children":[["$","div",null,{"className":"prose dark:prose-invert max-w-none pt-10 pb-8 text-gray-700 dark:text-gray-200","children":[["$","h1",null,{"className":"content-header","id":"gpu-profiling-under-the-hood-an-implementation-focused-survey-of-modern-accelerator-tracing-tools","children":[["$","a",null,{"className":"break-words","href":"#gpu-profiling-under-the-hood-an-implementation-focused-survey-of-modern-accelerator-tracing-tools","aria-hidden":"true","tabIndex":"-1","children":["$","span",null,{"className":"content-header-link","children":["$","svg",null,{"className":"h-5 linkicon w-5","fill":"currentColor","viewBox":"0 0 20 20","xmlns":"http://www.w3.org/2000/svg","children":[["$","path",null,{"d":"M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z"}],["$","path",null,{"d":"M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z"}]]}]}]}],"GPU Profiling Under the Hood: An Implementation-Focused Survey of Modern Accelerator Tracing Tools"]}],"$L2b","$L2c","$L2d","$L2e","$L2f","$L30","$L31","$L32","$L33","$L34","$L35","$L36","$L37","$L38","$L39","$L3a","$L3b","$L3c","$L3d","$L3e","$L3f","$L40","$L41","$L42","$L43","$L44","$L45","$L46","$L47","$L48","$L49","$L4a","$L4b","$L4c","$L4d","$L4e","$L4f","$L50","$L51","$L52","$L53","$L54","$L55","$L56","$L57","$L58","$L59","$L5a","$L5b","$L5c","$L5d","$L5e","$L5f","$L60","$L61","$L62","$L63","$L64","$L65","$L66","$L67","$L68","$L69","$L6a","$L6b","$L6c","$L6d","$L6e","$L6f","$L70","$L71","$L72","$L73","$L74","$L75","$L76","$L77","$L78","$L79","$L7a","$L7b","$L7c","$L7d","$L7e","$L7f","$L80","$L81","$L82","$L83","$L84","$L85","$L86","$L87"]}],"$L88","$L89"]}],"$L8a"]}]]}]}]]}]]
a7:I[1449,["874","static/chunks/874-8edb22cc7428423c.js","63","static/chunks/63-d245e42a784ca56d.js","909","static/chunks/app/blog/%5B...slug%5D/page-1ed2702378ba9d6b.js"],"default"]
2b:["$","p",null,{"children":["Profiling and tracing heterogeneous accelerators (GPUs, DPUs, and APUs) is crucial for optimizing performance in modern systems. This survey provides a deep implementation-oriented review of how state-of-the-art tools capture low-level execution details. We target two audiences: (1) tool developers seeking insight into ",["$","strong",null,{"children":"how existing profilers are built"}]," – including what interfaces they hook and how they trace both CPUs and accelerators – and (2) system engineers deciding ",["$","strong",null,{"children":"which tools to integrate"}],", based on capabilities and overhead. We explore internal architectures of key profiling tools, the runtime libraries and driver APIs they intercept, whether they trace events on CPU, GPU, or both, and how they handle cross-device correlation. We then examine implementation strategies (instrumentation vs. sampling, use of performance counters, dynamic binary instrumentation, kernel hooks like eBPF), as well as GPU-specific techniques (e.g. warp-level instruction sampling). Data handling and visualization approaches are compared, including trace data formats, storage and export methods, and GUI/CLI/Web interfaces (with integration into dashboards like TensorBoard or Grafana). We catalog the performance metrics these tools collect – from kernel latencies and throughput to SM occupancy, cache misses, and interconnect utilization – explaining how raw hardware counters are translated into ",["$","strong",null,{"children":"derived metrics"}]," and the granularity/accuracy trade-offs involved. Next, we discuss what insights users can glean from visualizations (timelines, flame charts, dependency graphs, etc.), such as detecting pipeline stalls, memory bottlenecks, or CPU–GPU desynchronization. Tool ",["$","em",null,{"children":"extensibility"}]," (support for plugins or user-defined instrumentation) is reviewed, as are the relative overheads and intrusiveness of different measurement techniques. We also address security and sandboxing concerns – e.g. profiling in containerized or multi-tenant environments – and outline known limitations plus future directions (including support for emerging DPUs and integrated CPU-GPU APUs). The goal is a comprehensive systems-level survey that ",["$","strong",null,{"children":"goes beyond feature lists"}]," to compare ",["$","em",null,{"children":"how"}]," these profilers work under the hood, including any reverse-engineered or undocumented methods used by open-source tools."]}]
2c:["$","h2",null,{"className":"content-header","id":"internal-architectures-of-key-profiling-tools","children":[["$","a",null,{"className":"break-words","href":"#internal-architectures-of-key-profiling-tools","aria-hidden":"true","tabIndex":"-1","children":["$","span",null,{"className":"content-header-link","children":["$","svg",null,{"className":"h-5 linkicon w-5","fill":"currentColor","viewBox":"0 0 20 20","xmlns":"http://www.w3.org/2000/svg","children":[["$","path",null,{"d":"M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z"}],["$","path",null,{"d":"M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z"}]]}]}]}],"Internal Architectures of Key Profiling Tools"]}]
2d:["$","p",null,{"children":"Understanding each tool’s internal design reveals what levels of the software/hardware stack it can observe. Below we review representative GPU profiling and tracing tools – both vendor-provided and open-source – focusing on their architecture, data collection mechanisms, and target platforms."}]
2e:["$","h3",null,{"className":"content-header","id":"nvidia-nsight-systems","children":[["$","a",null,{"className":"break-words","href":"#nvidia-nsight-systems","aria-hidden":"true","tabIndex":"-1","children":["$","span",null,{"className":"content-header-link","children":["$","svg",null,{"className":"h-5 linkicon w-5","fill":"currentColor","viewBox":"0 0 20 20","xmlns":"http://www.w3.org/2000/svg","children":[["$","path",null,{"d":"M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z"}],["$","path",null,{"d":"M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z"}]]}]}]}],"NVIDIA Nsight Systems"]}]
2f:["$","p",null,{"children":["NVIDIA Nsight Systems is a system-wide profiler that provides a timeline view of CPU and GPU activities for CUDA applications. Internally, Nsight Systems combines lightweight ",["$","strong",null,{"children":"CPU sampling"}]," with GPU activity tracing. It leverages the Linux Performance Events interface under the hood to capture CPU thread scheduling and sample CPU instruction pointers. In practice, this means it relies on ",["$","code",null,{"children":"perf_event_open"}]," for minimal-overhead sampling, requiring an appropriate ",["$","code",null,{"children":"perf_event_paranoid"}]," setting on Linux. For GPU tracing, Nsight Systems interfaces with NVIDIA’s CUDA Profiling Tools Interface (CUPTI) to receive callback “activity” records whenever GPU operations occur (kernel launches, memory copies, etc.). By hooking into the CUDA driver/runtime via CUPTI, Nsight Systems records timestamps for each GPU kernel execution and memory transfer. It synchronizes CPU and GPU timelines through a common timestamp base (CUPTI events are timestamped and merged with CPU events) so that events on both processors can be correlated in a unified timeline. The tool traces a variety of ",["$","strong",null,{"children":"runtimes"}]," beyond CUDA as well: it can capture OS events, storage I/O, and GPU-accelerated libraries (e.g. cuDNN, cuBLAS) when instrumented via NVTX annotations. Nsight Systems primarily uses ",["$","strong",null,{"children":"event-based instrumentation"}]," for GPU (via CUPTI callbacks) and ",["$","strong",null,{"children":"statistical sampling"}]," for CPU, making it a ",["$","em",null,{"children":"hybrid"}]," approach. This design achieves low overhead—usually only a few percent—sufficient for profiling full applications. Data is recorded to a proprietary trace file (e.g. ",["$","code",null,{"children":".nsys-rep"}],") which can be viewed in the Nsight Systems GUI or exported (the CLI can generate JSON or CSV summary stats). It supports ",["$","strong",null,{"children":"multi-threaded and multi-GPU"}]," tracing, showing context switches and concurrency on a timeline. However, being vendor-specific, Nsight Systems focuses on NVIDIA GPUs and CUDA or related NVIDIA APIs; it does not directly support AMD or Intel GPU runtimes."]}]
30:["$","h3",null,{"className":"content-header","id":"nvidia-nsight-compute","children":[["$","a",null,{"className":"break-words","href":"#nvidia-nsight-compute","aria-hidden":"true","tabIndex":"-1","children":["$","span",null,{"className":"content-header-link","children":["$","svg",null,{"className":"h-5 linkicon w-5","fill":"currentColor","viewBox":"0 0 20 20","xmlns":"http://www.w3.org/2000/svg","children":[["$","path",null,{"d":"M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z"}],["$","path",null,{"d":"M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z"}]]}]}]}],"NVIDIA Nsight Compute"]}]
31:["$","p",null,{"children":["Nsight Compute is NVIDIA’s kernel-level profiler for deep analysis of individual GPU kernels. Instead of a timeline, it provides a detailed report of metrics for each kernel launch. Internally, Nsight Compute instruments and measures ",["$","strong",null,{"children":"GPU hardware performance counters"}]," extensively. It uses NVIDIA’s profiling interfaces (such as CUPTI’s Event and Metric API or the newer CUDA Profiling API) to program GPU performance monitors to count events like instructions executed, memory transactions, warp occupancy, etc. Because GPUs have limited counters available per run, Nsight Compute often ",["$","strong",null,{"children":"replays each kernel"}]," multiple times to gather all requested metrics. For example, a kernel might run dozens of times (e.g. 46 passes) to collect different counter sets, which are later aggregated. This ",["$","strong",null,{"children":"post-mortem instrumentation"}]," approach yields comprehensive metrics (including derived ones like achieved occupancy or memory throughput), but incurs significant runtime overhead – thus Nsight Compute is used typically on small kernels or in selective profiling sessions. The tool’s architecture includes a ",["$","strong",null,{"children":"kernel injection mechanism"}],": it pauses the target program at each kernel launch, sets up counters (or even inserts timing instruments around the kernel), then resumes execution and collects results. It supports only NVIDIA GPUs (starting from Volta architecture) and primarily ",["$","strong",null,{"children":"CUDA kernels"}]," (though it can also profile kernels from GPU-accelerated libraries or CUDA Graphs by hooking those calls). Results are stored in a report file or shown in an interactive GUI/CLI with “Sections” grouping metrics. Nsight Compute’s metrics calculation system is advanced – it computes high-level stats (e.g. achieved occupancy, stall reasons breakdown) from low-level counters using formulas embedded in the tool. The output can be exported to text or CSV for further analysis. While extremely detailed, Nsight Compute’s tight integration with NVIDIA’s driver means it is not extensible to other vendors and must be used offline (it’s too intrusive for continuous monitoring)."]}]
32:["$","h3",null,{"className":"content-header","id":"amd-omnitrace","children":[["$","a",null,{"className":"break-words","href":"#amd-omnitrace","aria-hidden":"true","tabIndex":"-1","children":["$","span",null,{"className":"content-header-link","children":["$","svg",null,{"className":"h-5 linkicon w-5","fill":"currentColor","viewBox":"0 0 20 20","xmlns":"http://www.w3.org/2000/svg","children":[["$","path",null,{"d":"M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z"}],["$","path",null,{"d":"M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z"}]]}]}]}],"AMD Omnitrace"]}]
33:["$","p",null,{"children":["Omnitrace is an open-source profiling and tracing tool developed by AMD Research. Its design emphasizes comprehensive tracing of both CPU and GPU execution via ",["$","strong",null,{"children":"dynamic binary instrumentation"}]," and ",["$","strong",null,{"children":"sampling"}],". Omnitrace can attach to unmodified applications and instrument function calls (including HIP and OpenCL API calls) at runtime. Under the hood, it employs a dynamic binary instrumentation framework to insert probes into the target program’s functions (the documentation suggests it may use tools like Dyninst or similar). Users can configure which functions or modules to instrument prior to execution, allowing them to limit overhead by selecting only specific regions. At runtime, the instrumentation logs entry/exit events and timestamps, while a parallel ",["$","strong",null,{"children":"call-stack sampling"}]," component periodically samples all threads to capture CPU execution hotspots. For GPU work, Omnitrace hooks into AMD’s ROCm stack: it uses ",["$","strong",null,{"children":"ROCm’s roctracer"}]," callbacks to trace HIP runtime events and GPU kernel executions on AMD GPUs. (Currently, Omnitrace supports AMD GPUs via the HIP and HSA APIs only – NVIDIA GPUs are not yet supported, reflecting its AMD focus.) Collected trace data is written in the open Perfetto format (protobuf-based) which can be visualized in a Chrome/Perfetto web viewer. This choice of format enables rich timeline views in any browser (Perfetto UI). In addition, Omnitrace produces aggregated profiles (e.g. total time per function, call graphs) in JSON and text files. The JSON outputs integrate with analysis tools like ",["$","strong",null,{"children":"Hatchet"}]," (which loads the data into pandas for custom analysis). Omnitrace’s architecture is highly extensible due to its open-source nature: users can modify it to support new events or use its ",["$","strong",null,{"children":"user API"}]," to mark custom intervals. It effectively merges capabilities of a statistical profiler and a tracer, giving both high-level summaries and full execution traces. Because it instruments at the binary level, there is some overhead, but this is mitigated by letting users choose sampling frequency and which functions to instrument. Overall, Omnitrace provides a blueprint for building cross-platform profilers (it is designed to work on CPU-only apps as well, supporting any CPU vendor, while GPU support is currently AMD-specific)."]}]
34:["$","h3",null,{"className":"content-header","id":"rice-hpctoolkit","children":[["$","a",null,{"className":"break-words","href":"#rice-hpctoolkit","aria-hidden":"true","tabIndex":"-1","children":["$","span",null,{"className":"content-header-link","children":["$","svg",null,{"className":"h-5 linkicon w-5","fill":"currentColor","viewBox":"0 0 20 20","xmlns":"http://www.w3.org/2000/svg","children":[["$","path",null,{"d":"M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z"}],["$","path",null,{"d":"M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z"}]]}]}]}],"Rice HPCToolkit"]}]
35:["$","p",null,{"children":["HPCToolkit is a longstanding open-source performance analysis toolkit, notable for its ability to profile large HPC codes with low overhead. Its recent extensions support GPU-accelerated programs by combining CPU sampling with GPU activity tracing. The internal architecture of HPCToolkit uses a ",["$","strong",null,{"children":"vendor-neutral GPU monitoring substrate"}],". This substrate connects to vendor-specific drivers: on NVIDIA, it uses CUPTI; on AMD, it uses ROC-tracer; and for Intel GPUs, it wraps the Level Zero API. Essentially, HPCToolkit registers callback functions so that whenever the GPU runtime (CUDA, HIP, Level0, OpenCL) launches a kernel or performs a data transfer, HPCToolkit is notified. In addition, HPCToolkit installs its own intercepting wrappers for OpenCL and Level Zero to catch events on platforms where a built-in tracer isn’t available. All GPU events (kernel launch begin/end, etc.) are time-stamped and recorded in a ",["$","strong",null,{"children":"GPU operations trace"}],". Meanwhile, the CPU side of the application is typically profiled via ",["$","strong",null,{"children":"sampling"}]," – HPCToolkit’s ",["$","code",null,{"children":"hpcrun"}]," uses timer or hardware-counter interrupts to sample call stacks of CPU threads, attributing costs to code regions. These two data sources are merged during analysis to correlate CPU call paths with GPU activity. HPCToolkit goes further by providing ",["$","strong",null,{"children":"fine-grained GPU profiling"}],": on NVIDIA GPUs, it can perform ",["$","strong",null,{"children":"PC sampling"}]," on the GPU itself. NVIDIA hardware since Maxwell can periodically sample a warp’s program counter during execution. HPCToolkit leverages this via CUPTI to collect a statistical profile of GPU instruction addresses, which are later mapped back to source lines and functions (HPCToolkit performs binary analysis of CUDA CUBIN files to recover function names and line mappings). This yields a ",["$","strong",null,{"children":"hierarchical profile"}]," of GPU code, identifying hotspots at the source level similar to how CPU sampling does. (On AMD GPUs, such fine-grained sampling is currently not available due to lack of hardware support, so HPCToolkit only traces kernel-level timings there.) The tool also can trace GPU memory allocation calls and other API usage by wrapping those APIs. Data collected by HPCToolkit is stored in a structured format (experiment directory with binary performance data files). Visualization is done via HPCToolkit’s dedicated viewers: HPC‐Viewer for aggregated profiles and HPC‐TraceViewer for timelines. The design emphasizes ",["$","strong",null,{"children":"post-mortem analysis"}]," – data is collected with low interference and later analyzed to produce call path profiles, mixture of tracing and profiling. HPCToolkit’s open design allows adding support for new platforms (recent versions added Intel oneAPI Level0 and even OpenMP offload tracing). Its strengths are in correlating GPU work back to CPU context (reconstructing a ",["$","strong",null,{"children":"GPU calling context tree"}],") and doing so in a vendor-agnostic way (e.g., presenting warps and wavefronts uniformly as abstract threads). One limitation is that configuring and processing the traces can be complex, and fine-grained GPU sampling (especially on NVIDIA) can perturb execution (earlier CUDA versions serialized kernels with PC sampling enabled, causing overhead). Nonetheless, HPCToolkit illustrates a ","$L8b",": sampling + tracing, with heavy use of vendor interfaces but also static analysis to attribute performance metrics to code."]}]
36:["$","h3",null,{"className":"content-header","id":"ebpf-based-gpu-performance-tracing","children":[["$","a",null,{"className":"break-words","href":"#ebpf-based-gpu-performance-tracing","aria-hidden":"true","tabIndex":"-1","children":["$","span",null,{"className":"content-header-link","children":["$","svg",null,{"className":"h-5 linkicon w-5","fill":"currentColor","viewBox":"0 0 20 20","xmlns":"http://www.w3.org/2000/svg","children":[["$","path",null,{"d":"M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z"}],["$","path",null,{"d":"M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z"}]]}]}]}],"eBPF-Based GPU Performance Tracing"]}]
37:["$","p",null,{"children":["eGPU and bpftime are two recently merged open-source projects that demonstrate a novel approach to true zero-instrumentation GPU performance monitoring using Linux eBPF technology. Unlike traditional tools that often require explicit modification of the target application, recompilation, or reliance on vendor SDKs (such as NVIDIA Nsight), this new solution employs operating system-level dynamic tracing to achieve real-time monitoring of GPU execution phases without altering the target program’s binary. On the CPU side, bpftime utilizes eBPF uprobes (user-space probes) to dynamically attach to key APIs in the NVIDIA CUDA runtime library (",["$","code",null,{"children":"libcudart.so"}],"), such as ",["$","code",null,{"children":"cudaMalloc"}],", ",["$","code",null,{"children":"cudaMemcpy"}],", ",["$","code",null,{"children":"cudaFree"}],", and ",["$","code",null,{"children":"cudaLaunchKernel"}],". These probes intercept and log application calls to CUDA APIs, as detailed in ",["$","a",null,{"className":"break-words","target":"_blank","rel":"noopener noreferrer","href":"https://eunomia.dev/tutorials/47-cuda-events/","children":"eBPF Implementation for CUDA Event Tracing"}],". When a hooked CUDA function call is triggered, the uprobe executes an extremely lightweight BPF program in the Linux kernel, recording call parameters (e.g., memory allocation size, data transfer size) and precise CPU timestamps. This data is stored in the kernel’s ring buffer structure."]}]
38:["$","p",null,{"children":["Compared to earlier methods like uprobes, which were limited to CPU-side tracing, the innovation of ",["$","a",null,{"className":"break-words","target":"_blank","rel":"noopener noreferrer","href":"https://dl.acm.org/doi/10.1145/3723851.3726984","children":"eGPU"}]," lies in its ability to implement full eBPF probe capabilities at the GPU kernel level. eGPU performs just-in-time (JIT) translation of user-written BPF bytecode into NVIDIA GPU native instructions (PTX) and directly injects it into running CUDA kernels. This enables real-time capture of internal GPU events (such as kernel launches, thread block scheduling, event synchronization, and compute unit execution). Since this kernel-level injection is transparent and requires no modification to the GPU kernel source or binary, it significantly enhances the precision of GPU internal tracing, providing sub-microsecond GPU-side timestamps. The real-time nature and accuracy of the tracing data far surpass traditional user-space tools. eGPU has now been integrated into ",["$","a",null,{"className":"break-words","target":"_blank","rel":"noopener noreferrer","href":"https://github.com/eunomia-bpf/bpftime","children":"https://github.com/eunomia-bpf/bpftime"}],", achieving unified aggregation and synchronization of CPU and GPU events. Both are merged and correlated through an efficient kernel-space ring buffer, ensuring data and timestamp consistency with low latency during tracing. Functionally, eGPU and bpftime fill a gap among existing GPU performance tools: they require neither developer instrumentation nor recompilation, nor dependence on vendor-specific toolchains, yet they can reveal memory usage patterns, data transfer performance bottlenecks, and the true launch latency and execution efficiency of GPU kernels in real time. This efficient and truly non-intrusive design greatly simplifies the complexity of real-world deployment."]}]
39:["$","h3",null,{"className":"content-header","id":"nvidia-dcgm-data-center-gpu-manager","children":[["$","a",null,{"className":"break-words","href":"#nvidia-dcgm-data-center-gpu-manager","aria-hidden":"true","tabIndex":"-1","children":["$","span",null,{"className":"content-header-link","children":["$","svg",null,{"className":"h-5 linkicon w-5","fill":"currentColor","viewBox":"0 0 20 20","xmlns":"http://www.w3.org/2000/svg","children":[["$","path",null,{"d":"M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z"}],["$","path",null,{"d":"M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z"}]]}]}]}],"NVIDIA DCGM (Data Center GPU Manager)"]}]
3a:["$","p",null,{"children":["DCGM is not a profiler for individual applications so much as a ",["$","strong",null,{"children":"GPU telemetry and monitoring"}]," suite at the cluster/node level. Its architecture is service-oriented: DCGM runs as a background ",["$","strong",null,{"children":"host engine"}]," service (",["$","code",null,{"children":"nv-hostengine"}],") or can be embedded in management applications via its ",["$","code",null,{"children":"libdcgm.so"}]," API. The core DCGM library interacts with NVIDIA driver interfaces to gather a wide range of ",["$","strong",null,{"children":"metrics"}]," about GPUs – including utilization (SM busy %, memory usage), temperatures, power draw, clock speeds, memory errors, and even throughput stats like PCIe bytes transferred. Many of these metrics come from the same underlying counters as used by nvidia-smi (via NVML, the NVIDIA Management Library). DCGM aggregates this data for all GPUs on a server. It can be configured to record metrics periodically and trigger alerts or apply power caps based on policies. The open-core design means the core library and APIs are open source, while some diagnostic modules remain proprietary. DCGM’s host engine can accept commands via ",["$","strong",null,{"children":"CLI (dcgmi)"}]," or serve data via the ",["$","strong",null,{"children":"DCGM-Exporter"}],", which integrates with Kubernetes. The DCGM-Exporter provides an HTTP endpoint that continuously ",["$","strong",null,{"children":"exports GPU metrics in Prometheus format"}],", allowing easy scraping and visualization in Grafana dashboards. This makes DCGM ideal for long-running monitoring in containerized environments. In terms of internal tracing, DCGM mostly uses polling of driver counters and event callbacks for certain health events (like double-bit ECC errors). It is less about tracing individual kernel launches and more about ",["$","strong",null,{"children":"aggregated performance and health over time"}],". For example, DCGM can report average SM occupancy or memory bandwidth of a job, but not which function caused a cache miss. It operates at a coarser granularity than tools like Nsight or HPCToolkit – often at timescales of seconds – and is designed to have very low overhead (“low-overhead, non-invasive health monitoring while jobs run”). Security-wise, DCGM can be run with limited privileges: by default, GPU performance counters that might leak details are restricted to admin, but configurations (like setting ",["$","code",null,{"children":"NVreg_RestrictProfilingToAdminUsers=0"}],") can allow non-root DCGM use. In summary, DCGM’s architecture serves ",["$","em",null,{"children":"system administrators"}],": it provides a unified API and service for GPU metrics collection across nodes, with integration points to cluster managers and cloud monitoring. It complements the per-application profilers by focusing on ",["$","em",null,{"children":"global GPU behavior and health"}]," rather than code-specific performance events."]}]
3b:["$","h3",null,{"className":"content-header","id":"tau-tuning-and-analysis-utility","children":[["$","a",null,{"className":"break-words","href":"#tau-tuning-and-analysis-utility","aria-hidden":"true","tabIndex":"-1","children":["$","span",null,{"className":"content-header-link","children":["$","svg",null,{"className":"h-5 linkicon w-5","fill":"currentColor","viewBox":"0 0 20 20","xmlns":"http://www.w3.org/2000/svg","children":[["$","path",null,{"d":"M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z"}],["$","path",null,{"d":"M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z"}]]}]}]}],"TAU (Tuning and Analysis Utility)"]}]
3c:["$","p",null,{"children":["TAU is a portable performance profiling/tracing toolkit widely used in HPC, which has evolved to support GPU offloading in addition to CPU and network tracing. TAU’s architecture is modular: it can use either ",["$","strong",null,{"children":"source-level instrumentation, binary rewriting, or interception wrappers"}]," to instrument an application, and it supports many programming models (MPI, OpenMP, Pthreads, etc.). For GPU support, TAU leverages existing performance tool interfaces: it supports NVIDIA GPUs via CUPTI, AMD GPUs via ROCm’s ROCprofiler or roctracer, and Intel GPUs via Level Zero tools. Specifically, TAU’s runtime library can subscribe to CUPTI’s callback API to track CUDA kernel launches, memory copies, etc., similar to HPCToolkit’s approach. It also supports the ",["$","strong",null,{"children":"MPI Tools Interface (MPI_T)"}]," and ",["$","strong",null,{"children":"Kokkos profiling hooks"}]," to correlate high-level parallel runtime events with GPU activity. TAU is often used by launching the program with a wrapper (",["$","code",null,{"children":"tau_exec"}],"), which can automatically inject instrumentation. For example, ",["$","code",null,{"children":"tau_exec -cuda"}]," would enable CUPTI-based instrumentation of all CUDA API calls and kernels. TAU records ",["$","strong",null,{"children":"timers"}]," for each function, so if a GPU kernel launch is intercepted, TAU will log the time spent in that launch call on the CPU as well as the kernel execution time on the GPU (through CUPTI’s asynchronous activity records). The tool can also track data transfer sizes and timings (e.g. cudaMemcpy). Beyond explicit instrumentation, TAU supports ",["$","strong",null,{"children":"event-based sampling"}]," as well. It can configure hardware counters or timers to periodically sample the program counter, and then use binary debugging info (DWARF via libdwarf/BFD) to map those samples to source lines. This is useful to profile codes without function instrumentation or to capture system-level anomalies. TAU stores profile results in either a packed binary format or a database (it has a PerfDMF framework for data management). Visualization can be done with TAU’s ParaProf GUI (which shows tables of metrics per function, timelines per thread, call graphs, etc.), or by converting traces to formats like OTF2 for use with tools such as Vampir. TAU is ",["$","strong",null,{"children":"extensible"}]," – users can use the TAU API to mark custom events or phases in their code, which then show up in profiles. Plugin-like functionality exists in that TAU can interface with new back-end measurement sources relatively easily (the TAU team added support for new GPUs and even OMPT without changing the user-facing API). TAU’s design philosophy is to intercept as much as possible through standard tool interfaces (CUDA/CUPTI, ROCm tools, MPI PMPI, OpenMP OMPT) so that it doesn’t require modifying the target code. This makes it a ",["$","strong",null,{"children":"multi-platform “glue” profiler"}]," but also means TAU’s capabilities on a given platform are bounded by what the platform’s APIs expose. For instance, TAU can trace time spent in GPU kernels and data moves, but for in-kernel details on NVIDIA it would rely on CUPTI metrics (which TAU can collect similarly to Nsight Compute, though TAU typically focuses on timelines and aggregate times rather than every hardware metric). In summary, TAU’s internal architecture combines ","$L8c"," (automatically inserted) with ","$L8d",", and unifies the data in a performance database. It provides a holistic view across CPU, GPU, and MPI, which is valuable in HPC settings where an application spans all those layers."]}]
3d:["$","p",null,{"children":["$","em",null,{"children":["(Other tools: In addition to the above, there are vendor-specific GPU profilers such as AMD’s Radeon GPU Profiler (RGP) for low-level AMD GPU traces, Intel’s VTune Profiler and Graphics Performance Analyzers for Intel GPUs, and research tools like ",["$","strong",null,{"children":"rocProf"}]," (the ROCm command-line profiler on AMD). These generally follow similar patterns: e.g. RGP uses driver-level instrumentation and GPU shader performance counters to produce a timeline of graphics/compute waves, VTune uses Intel’s drivers to trace Level0 or OpenCL kernels on Intel GPUs. Due to space, we focus on the major tools above, which cover a range of implementation strategies.)"]}]}]
3e:["$","h2",null,{"className":"content-header","id":"apis-and-runtimes-traced-by-these-tools","children":[["$","a",null,{"className":"break-words","href":"#apis-and-runtimes-traced-by-these-tools","aria-hidden":"true","tabIndex":"-1","children":["$","span",null,{"className":"content-header-link","children":["$","svg",null,{"className":"h-5 linkicon w-5","fill":"currentColor","viewBox":"0 0 20 20","xmlns":"http://www.w3.org/2000/svg","children":[["$","path",null,{"d":"M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z"}],["$","path",null,{"d":"M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z"}]]}]}]}],"APIs and Runtimes Traced by These Tools"]}]
3f:["$","p",null,{"children":["Profiling tools achieve coverage by hooking into various layers of the software stack. ",["$","strong",null,{"children":"Table 1"}]," summarizes which runtimes, libraries, or driver APIs each tool is capable of tracing:"]}]
40:["$","ul",null,{"children":[["$","li",null,{"children":[["$","strong",null,{"children":"CUDA Driver/Runtime (NVIDIA):"}]," Nearly all tools (Nsight Systems/Compute, HPCToolkit, TAU, GPUprobe) support CUDA. Nsight and TAU use CUPTI to intercept CUDA calls, HPCToolkit uses CUPTI for activities, GPUprobe hooks ",["$","code",null,{"children":"libcudart"}]," via uprobes. Open-source Omnitrace currently does ",["$","em",null,{"children":"not"}]," support CUDA (focusing on AMD), and DCGM monitors at a higher level via NVML (it sees overall CUDA usage but not individual calls)."]}],["$","li",null,{"children":[["$","strong",null,{"children":"HIP/ROCm (AMD):"}]," Omnitrace and TAU support AMD’s HIP runtime via ROCtracer/ROCprofiler. HPCToolkit also uses ROC-tracer for AMD GPU kernels. These allow tracing of HIP kernel launches, memory copies, etc. Tools like Nsight (NVIDIA) do not support AMD APIs. AMD’s RGP and rocProf of course target ROCm/HIP natively."]}],["$","li",null,{"children":[["$","strong",null,{"children":"OpenCL:"}]," HPCToolkit has function wrappers to intercept OpenCL API calls for GPUs that lack a better interface. TAU also supports OpenCL tracing. Nsight Systems can trace OpenCL on NVIDIA platforms (through CUPTI if the OpenCL implementation routes through CUDA driver). Omnitrace likely can handle OpenCL on AMD via its binary instrumentation (if configured to instrument those calls)."]}],["$","li",null,{"children":[["$","strong",null,{"children":"Vulkan/OpenGL (Graphics APIs):"}]," Nsight Systems and NVIDIA tools can trace GPU work originating from graphics APIs (showing GPU work such as Vulkan launches on the timeline), but our focus is compute. Tools like RGP target graphics as well. In general, HPC-focused tools (HPCToolkit, TAU) do not trace graphics APIs by default."]}],["$","li",null,{"children":[["$","strong",null,{"children":"Intel oneAPI Level Zero / DPC++:"}]," HPCToolkit and TAU have added support for Intel GPU offload. HPCToolkit wraps the Level Zero (oneAPI) API to catch kernel submissions. TAU supports oneAPI through the OMPT (OpenMP offload) interface and Level Zero interceptors. Intel’s VTune uses a driver to trace these as well, but third-party support is emerging."]}],["$","li",null,{"children":[["$","strong",null,{"children":"Driver-level Hooks:"}]," Some tools integrate at the driver level: DCGM interfaces with the GPU driver via NVML for metrics, and possibly directly with kernel driver for things like throttling or health events. Most tracing of individual ops is done at the user API level (driver-level instrumentation by third-parties is rare since GPU drivers are proprietary closed-source, except for AMD’s open driver where ROCtracer plugs in)."]}]]}]
41:["$","p",null,{"children":["In summary, ",["$","strong",null,{"children":"NVIDIA’s CUPTI"}]," is a common interface used by Nsight, HPCToolkit, TAU (and formerly by nvprof, Visual Profiler) to trace CUDA at both driver API level and collect counters. ",["$","strong",null,{"children":"AMD’s ROCm tools"}]," (rocTracer/rocProfiler) play a similar role for HIP/HSA. ",["$","strong",null,{"children":"Intel’s Level Zero"}]," is newer; support is available in HPC Toolkit/TAU. Tools cover CPU threading libraries too (pthreads, OpenMP via OMPT, MPI via PMPI/MPI_T), to correlate CPU-side phases with GPU activity."]}]
42:["$","h2",null,{"className":"content-header","id":"tracing-on-cpu-vs-gpu-and-correlation-of-events","children":[["$","a",null,{"className":"break-words","href":"#tracing-on-cpu-vs-gpu-and-correlation-of-events","aria-hidden":"true","tabIndex":"-1","children":["$","span",null,{"className":"content-header-link","children":["$","svg",null,{"className":"h-5 linkicon w-5","fill":"currentColor","viewBox":"0 0 20 20","xmlns":"http://www.w3.org/2000/svg","children":[["$","path",null,{"d":"M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z"}],["$","path",null,{"d":"M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z"}]]}]}]}],"Tracing on CPU vs GPU and Correlation of Events"]}]
43:["$","p",null,{"children":["An important aspect of these profilers is whether they capture events on the CPU side, the GPU side, or both – and how they merge these timelines. ",["$","strong",null,{"children":"CPU-side tracing"}]," typically involves recording when the application thread invokes certain API calls or when certain functions execute. For example, instrumentation-based tools like TAU or Omnitrace will log a timestamp when a CUDA kernel launch function is called on the CPU. Sampling-based tools like Nsight Systems or HPCToolkit also record CPU-side samples (which can indicate where the CPU was busy or idle). ",["$","strong",null,{"children":"GPU-side tracing"}]," means capturing events that occur on the GPU device, such as the actual execution interval of a kernel on the GPU, or other GPU engine activities (memory copies on GPU engines, etc.)."]}]
44:["$","p",null,{"children":["Most tools rely on vendor support to get GPU-side event timing. For instance, CUPTI on NVIDIA provides timestamps for kernel start and stop on the GPU, which Nsight, HPCToolkit, and TAU use. Similarly, on AMD, ROCm’s activity callbacks give GPU kernel durations. By collecting these, the tools can plot ",["$","em",null,{"children":"actual GPU utilization timelines"}],". GPU hardware often has a separate clock domain, but the profiling interfaces ensure timestamps can be translated to the host clock (often by sampling a common clock at start of profiling)."]}]
45:["$","p",null,{"children":[["$","strong",null,{"children":"Correlation"}]," is then achieved by merging CPU and GPU event streams by time. Tools like Nsight Systems do this in their timeline: one can see a CPU thread launching a kernel and later (on the same timeline) the GPU activity occurring, aligned by timestamps. HPCToolkit takes a different approach by correlating via call context: it records that a particular CPU call path launched a kernel X, and in the profile attributes the kernel’s GPU time to that call path, effectively linking the GPU time into the CPU calling context tree. This answers questions like “which CPU function caused this GPU work and how long did it take?”. TAU similarly attributes GPU kernel time to the CPU code region that launched it (with features like ",["$","code",null,{"children":"TAU_CALLSITE"}]," to pinpoint the source line)."]}]
46:["$","p",null,{"children":["Handling ",["$","strong",null,{"children":"context switches"}]," and concurrency is another challenge. Nsight Systems explicitly traces context switch events on CPU (via perf or kernel tracepoints) so it can show which thread was running vs preempted. When correlating with GPU events, tools must consider that GPU kernels run asynchronously: e.g., the CPU might enqueue a kernel and continue, while the GPU executes it later. Profilers often include ",["$","strong",null,{"children":"stream synchronization info"}]," so they can show GPU work still in progress after the CPU dispatch."]}]
47:["$","p",null,{"children":"If multiple CPU threads launch work to multiple GPU streams, correlation ensures the correct matching. Unique IDs (like CUDA stream IDs or HIP queue IDs) are used internally to tag events. For example, CUPTI will report a kernel launch with an ID that corresponds to a specific CPU thread and stream, allowing the tool to match it with the CPU-side launch API call record."}]
48:["$","p",null,{"children":["In summary, advanced profilers trace both ",["$","strong",null,{"children":"CPU events"}]," (function calls, thread scheduling, etc.) and ",["$","strong",null,{"children":"GPU events"}]," (kernel execution intervals, device memory transfers). They align these by time and metadata, enabling a coherent view. This is critical to identify pipeline delays (e.g., if CPU is waiting for GPU or vice versa). Without GPU-side data, one might only know when the CPU launched a kernel, not when it finished on the GPU. Conversely, GPU-only traces would lack the context of what the CPU was doing. Thus, all the reviewed tools use some form of ",["$","strong",null,{"children":"combined tracing"}],": for instance, ",["$","strong",null,{"children":"HPCToolkit’s profile shows combined CPU+GPU call paths"}],", and ",["$","strong",null,{"children":"Nsight Systems uses a timeline covering both CPU cores and GPU timelines"}]," in parallel. Ensuring the clocks are synced (CUPTI handles this for CUDA) and handling asynchrony is a key part of their implementation."]}]
49:["$","h2",null,{"className":"content-header","id":"instrumentation-vs-sampling-strategies-for-data-collection","children":[["$","a",null,{"className":"break-words","href":"#instrumentation-vs-sampling-strategies-for-data-collection","aria-hidden":"true","tabIndex":"-1","children":["$","span",null,{"className":"content-header-link","children":["$","svg",null,{"className":"h-5 linkicon w-5","fill":"currentColor","viewBox":"0 0 20 20","xmlns":"http://www.w3.org/2000/svg","children":[["$","path",null,{"d":"M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z"}],["$","path",null,{"d":"M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z"}]]}]}]}],"Instrumentation vs. Sampling: Strategies for Data Collection"]}]
4a:["$","p",null,{"children":["Profiling tools generally employ one (or a mix) of two strategies to collect performance data: ",["$","strong",null,{"children":"instrumentation (event tracing)"}]," or ",["$","strong",null,{"children":"statistical sampling"}],". Each strategy has implications for overhead, accuracy, and intrusiveness."]}]
4b:["$","ul",null,{"children":[["$","li",null,{"children":["$","p",null,{"children":[["$","strong",null,{"children":"Instrumentation-based Profiling:"}]," This involves inserting hooks or probes to record events of interest ",["$","em",null,{"children":"each time"}]," they occur. For example, wrapping a function with start/stop timers (as TAU does), or using CUPTI callbacks that fire on every kernel launch. Instrumentation can be done at ",["$","strong",null,{"children":"compile-time"}]," (inserting code in the source or binary), ",["$","strong",null,{"children":"runtime injection"}]," (using LD_PRELOAD to intercept library calls, or dynamic binary instrumentation like Omnitrace), or at the ",["$","strong",null,{"children":"OS level"}]," (using kprobes/uprobes as GPUprobe does). The advantage is complete information: every event is logged, giving exact counts and timings. The disadvantage is overhead can be high if events are frequent. For instance, instrumenting every function call in a fine-grained application can slow it dramatically. Tools like ",["$","strong",null,{"children":"TAU and Omnitrace use instrumentation"}]," to get detailed traces (Omnitrace through binary instrumentation, TAU through wrapper libraries and compiler-based instrumentation). ",["$","strong",null,{"children":"GPU driver APIs"}]," are often instrumented via official callback interfaces (CUPTI, ROCtracer) to trace GPU events without modifying vendor code. Instrumentation can also use ",["$","strong",null,{"children":"dynamic runtime methods"}],": e.g., ",["$","strong",null,{"children":"LD_PRELOAD"}]," is used to intercept malloc or CUDA calls by overriding symbols at load time; binary rewriting can permanently add probes in the code; and ",["$","strong",null,{"children":"uprobes/kprobes"}]," allow inserting breakpoints at function entry (GPUprobe attaches at runtime to ",["$","code",null,{"children":"cudaLaunchKernel"}]," etc. using uprobes). Each approach has trade-offs in flexibility vs. performance."]}]}],["$","li",null,{"children":["$","p",null,{"children":[["$","strong",null,{"children":"Sampling-based Profiling:"}]," This technique gathers information at intervals (time-based or event-based) rather than logging every event. For CPU, this often means using hardware timers or performance counter overflow interrupts to periodically sample the program counter (PC) and call stack. For GPU, sampling can mean periodically querying utilization or, in NVIDIA’s case, using PC sampling on GPU as described. ",["$","strong",null,{"children":"Nsight Systems"}]," uses timed sampling for CPU threads (e.g., capturing stack traces at intervals to statistically infer hot code). ",["$","strong",null,{"children":"HPCToolkit"}]," heavily relies on sampling for CPU (and optionally uses GPU PC sampling for fine-grained GPU profiles). The advantage of sampling is much lower overhead – e.g., capturing 1000 samples per second imposes negligible slowdown, yet provides statistical coverage of where time is spent. It also automatically filters out very fast events (if a function executes 100ns, it might never be sampled, effectively focusing on major costs). The downside is reduced precision: you may miss infrequent events or introduce sampling error. Also, for event timings (like exact kernel duration), sampling is not suitable – it’s better for aggregate metrics like “which code consumed how many cycles on average.”"]}]}],["$","li",null,{"children":["$","p",null,{"children":[["$","strong",null,{"children":"Hybrid Approaches:"}]," Many tools combine both. For example, ","$L8e"," instruments GPU events (for precise timing of kernels) but samples CPU activity (to reduce overhead). ","$L8f"," instruments at a coarse level (GPU kernels) and uses sampling for everything else. ","$L90"," instruments function entry/exit and also does sampling of call stacks. This hybrid strategy allows capturing key events while keeping overhead manageable."]}]}]]}]
4c:["$","p",null,{"children":["In modern GPU profiling, ",["$","strong",null,{"children":"event tracing"}]," is typically used for GPU operations because the GPU work is often sparse (kernels, memcopies) and well-defined by the runtime APIs – it’s feasible to log each kernel launch. Meanwhile, ",["$","strong",null,{"children":"sampling"}]," is often used for CPU computation or within long GPU kernels to understand their internal behavior (like PC sampling inside a kernel, which essentially samples an event within the GPU execution). A special case is ",["$","strong",null,{"children":"Hardware counter sampling"}],": some tools set hardware PMUs to sample specific events, e.g. counting cache misses and sampling an instruction address on overflow (this merges both worlds: instrumentation to set up the counter, sampling to catch events)."]}]
4d:["$","p",null,{"children":[["$","strong",null,{"children":"Summary of strategies:"}]," If a developer needs exact event timelines and counts, instrumentation is preferred (used by tracing tools, timeline visualizers, etc.). If needing low overhead profiling of large applications to find hotspots, sampling is preferred (used by HPC profilers, statistical profilers). Many tools provide modes for both. The implementation must manage the enabling/disabling of instrumentation carefully (e.g., support for selective instrumentation—Omnitrace allows configuring which functions to instrument, TAU allows toggling instrumenting MPI, I/O, GPUs via flags). This helps target the analysis and reduce data volume."]}]
4e:["$","h2",null,{"className":"content-header","id":"use-of-hardware-performance-counters-pmus","children":[["$","a",null,{"className":"break-words","href":"#use-of-hardware-performance-counters-pmus","aria-hidden":"true","tabIndex":"-1","children":["$","span",null,{"className":"content-header-link","children":["$","svg",null,{"className":"h-5 linkicon w-5","fill":"currentColor","viewBox":"0 0 20 20","xmlns":"http://www.w3.org/2000/svg","children":[["$","path",null,{"d":"M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z"}],["$","path",null,{"d":"M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z"}]]}]}]}],"Use of Hardware Performance Counters (PMUs)"]}]
4f:["$","p",null,{"children":"Hardware Performance Monitoring Units (PMUs) are essential for collecting low-level metrics on both CPUs and GPUs. Profiling tools tap into these counters to measure events like instruction counts, cache misses, memory throughput, etc., which can’t be obtained by high-level timing alone."}]
50:["$","p",null,{"children":[["$","strong",null,{"children":"CPU PMUs:"}]," Most CPUs expose counters via interfaces like Linux ",["$","code",null,{"children":"perf"}],". Tools like HPCToolkit and TAU can use the PAPI library to configure CPU counters (cycles, cache misses, FLOPs, etc.) and sample on overflow. For instance, HPCToolkit’s command ",["$","code",null,{"children":"hpcrun -e PAPI_TOT_CYC@500000 -e PAPI_L2_TCM@200000"}]," would sample on total cycles and L2 cache misses after certain counts. By using such counters as sampling triggers, the tools attribute hardware events to code locations. TAU also integrates with PAPI, and can record counters per function invocation if instrumented (adding up counts inside each region). In kernel-level analyses (like OS scheduling), one might use CPU PMUs via ftrace or BPF too, but that’s less common for the user-facing profilers."]}]
51:["$","p",null,{"children":[["$","strong",null,{"children":"GPU PMUs:"}]," Modern GPUs have extensive performance counters (for each Streaming Multiprocessor on NVIDIA, or each Compute Unit on AMD). Access to these is through vendor libraries:"]}]
52:["$","ul",null,{"children":[["$","li",null,{"children":["NVIDIA provides CUPTI/Event and Metric APIs that allow tools to list available counters (e.g., “dram_read_transactions”, “active_warps”) and read them. Tools like Nsight Compute program these counters and read them after kernel execution. Some counters can be read ",["$","em",null,{"children":"after"}]," a kernel (giving aggregated counts for that kernel) using CUDA’s profiling modes. Others might be sampled continuously (NVIDIA supports a “streaming performance monitor” mode for continuous collection, but that is typically internal)."]}],["$","li",null,{"children":"AMD provides a similar interface via ROCprofiler or the GPUPerfAPI (GPAPI) for performance counters. Radeon GPU Profiler (RGP) and rocprof can gather counters for AMD GPUs, such as wavefront occupancies, cache hits, etc. Omnitrace likely uses ROCprofiler under the hood (since it’s built for AMD) to get counters in addition to trace callbacks."}],["$","li",null,{"children":"Intel GPUs (Xe) expose counters through their VTune or Metrics Discovery API, which HPC tools are only beginning to support. HPCToolkit mentions support for Intel GPU metrics via Level Zero in a limited fashion (perhaps linking to Intel’s GT-Pin or using the oneAPI VTune backend)."}]]}]
53:["$","p",null,{"children":[["$","strong",null,{"children":"In-Kernel Counters and PMUs:"}]," Some profiling approaches also use OS-level counters. For example, Linux has tracepoints for GPU scheduler events (on some open-source drivers) that could be used to track context switches on GPU engines. Also, NVIDIA’s driver tracks things like memory page migrations, NVLink usage, etc., which DCGM can retrieve. DCGM uses a subset of counters for health (like memory ECC counts) and throughput (PCIe bytes) not typically exposed to app profilers."]}]
54:["$","p",null,{"children":[["$","strong",null,{"children":"PMU Data Usage:"}]," The raw counter values often need post-processing to be meaningful:"]}]
55:["$","ul",null,{"children":[["$","li",null,{"children":[["$","strong",null,{"children":"Derived metrics:"}]," Many tools provide computed ratios or percentages. E.g., ",["$","em",null,{"children":"SM occupancy"}]," is derived from “active warps” vs. “theoretical max warps” counters. ",["$","em",null,{"children":"Cache hit rate"}]," is derived from cache hit and miss counters. Nsight Compute explicitly computes such metrics and even offers guided analysis rules (like if a certain stall reason counter is high, suggest optimizations). HPCToolkit presents raw metrics in a neutral way (counts or percentages) but doesn’t focus on guided hints."]}],["$","li",null,{"children":[["$","strong",null,{"children":"Sampling vs. aggregate:"}]," When counters are sampled (like setting a counter to trigger an interrupt), you get sample-based attribution (like “20 samples of L2 misses fell in function X”). When counters are aggregated per kernel (as Nsight Compute does), you get exact counts for that kernel but have to attribute to the whole kernel invocation. Some tools might break down within a kernel by source line using PC sampling combined with counters (e.g., correlate PC samples with an “issued instructions” count metric to estimate how many instructions were executed in each part of the code)."]}]]}]
56:["$","p",null,{"children":[["$","strong",null,{"children":"Overheads:"}]," Accessing GPU counters often requires exclusive control of the GPU’s performance monitor hardware, which is why only one profiling session can run at a time. On NVIDIA, if a counter profiler (Nsight Compute or CUPTI metrics) is running, another cannot, and by default only privileged users can use counters because of potential side-channel leaks. AMD’s ROCm might allow more concurrent use but still has overhead. Tools have to stagger counter reads to avoid perturbing the GPU pipeline (hence kernel replays for Nsight Compute ensure the original execution isn’t slowed by reading too many counters at once)."]}]
57:["$","p",null,{"children":["In summary, ",["$","strong",null,{"children":"PMUs are the foundation for low-level metrics"}],". Tool implementers work with these via vendor APIs (CUPTI, ROCprofiler, etc.), which abstract away direct register fiddling. In some cases, open-source tools have reverse-engineered parts of this: e.g., reading Nvidia GPU performance counters via undocumented perf events – but generally using official APIs is the norm due to complexity. The ",["$","strong",null,{"children":"use of PMUs"}]," enables insights into memory bandwidth (by counting bytes transferred), execution efficiency (issued vs retired instructions), stalls (hardware can count stall reasons by category on Nvidia since Volta), and more. These metrics greatly enrich a profiler’s output beyond simple timings, at the cost of more complex data collection."]}]
58:["$","h2",null,{"className":"content-header","id":"hooks-and-techniques-for-instrumentation","children":[["$","a",null,{"className":"break-words","href":"#hooks-and-techniques-for-instrumentation","aria-hidden":"true","tabIndex":"-1","children":["$","span",null,{"className":"content-header-link","children":["$","svg",null,{"className":"h-5 linkicon w-5","fill":"currentColor","viewBox":"0 0 20 20","xmlns":"http://www.w3.org/2000/svg","children":[["$","path",null,{"d":"M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z"}],["$","path",null,{"d":"M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z"}]]}]}]}],"Hooks and Techniques for Instrumentation"]}]
59:["$","p",null,{"children":"To implement instrumentation, tools may hook into different layers of the software stack. Here we outline common techniques:"}]
5a:["$","ul",null,{"children":[["$","li",null,{"children":["$","p",null,{"children":[["$","strong",null,{"children":"User-Mode API Wrapping:"}]," Many tools create wrapper libraries that intercept calls to standard APIs. Examples include TAU’s intercept of MPI (via PMPI), or wrapping CUDA runtime calls. This can be done by providing a library that defines the same symbols (e.g., a ",["$","code",null,{"children":"cudaLaunchKernel"}]," that calls the real one after logging) and using LD_PRELOAD or link order to ensure the wrapper is used. HPCToolkit’s substrate has ",["$","em",null,{"children":"function wrappers"}]," for OpenCL and Level Zero, which likely operate this way. This approach is straightforward and requires no special privileges, but you must implement a wrapper for each function of interest. It’s used for high-level events (e.g., log that a kernel was launched, then call through to actual launch)."]}]}],["$","li",null,{"children":["$","p",null,{"children":[["$","strong",null,{"children":"Compiler/Source Instrumentation:"}]," TAU can instrument source code or use compiler flags (like using ",["$","code",null,{"children":"-finstrument-functions"}]," or using Clang/OMPT for OpenMP). This compiles probes into the program. The advantage is rich context (you can instrument at loop or function entry with minimal overhead for timers), but it requires recompiling or at least adding an extra compilation step."]}]}],["$","li",null,{"children":["$","p",null,{"children":[["$","strong",null,{"children":"Dynamic Binary Instrumentation (DBI):"}]," Tools like Omnitrace perform DBI, meaning they inject instrumentation into the binary at runtime or ahead-of-time without source code. Frameworks for DBI include Dyninst, Intel PIN, or DynamoRIO. Omnitrace is described as doing dynamic instrumentation for functions. This is powerful because it can instrument even third-party library code (for which source is not available) and be configured on the fly. The cost is complexity and some overhead due to code patching and running under a DBI environment."]}]}],["$","li",null,{"children":["$","p",null,{"children":[["$","strong",null,{"children":"Kernel Probes (kprobes) and eBPF:"}]," For system-wide or kernel-level events, profilers can use kprobes (instrument kernel functions) or tracepoints. For example, to measure GPU driver events, one could insert kprobes in the GPU driver if you know the symbol (not common outside debugging). More practically, ",["$","strong",null,{"children":"eBPF"}]," is used by GPUprobe to attach to user-level functions (uprobes) and could also attach to kernel tracepoints (like scheduling or GPU interrupts). eBPF’s advantage is it’s dynamic (no app rebuild) and safe to run in production (sandboxed in kernel). We saw GPUprobe’s use of uprobes; similarly, one could attach kprobes to, say, the GPU scheduler’s job submission function to know when a kernel is scheduled on hardware. Few profiling tools currently use eBPF aside from GPUprobe, but this technique is gaining interest for low-overhead instrumentation of both kernel and user events. For example, one might imagine an eBPF-based profiler that samples GPU utilization via polling a device file every few ms, or traces OS events that affect GPU (like GPU interrupts, DMA completion events)."]}]}],["$","li",null,{"children":["$","p",null,{"children":[["$","strong",null,{"children":"Driver/Kernel Instrumentation Hooks:"}]," Some vendors allow inserting hooks at the driver level. NVIDIA’s CUPTI essentially registers hooks inside the CUDA driver – NVIDIA built this in for profilers. AMD’s ROCtracer does similarly by interfacing with the ROCm driver stack. If source is available (AMD’s runtime is open source), one could even modify or extend it with custom probes, but generally the provided hooks suffice. On CPUs, the OS provides tracepoints (like context switch events, system call events) that tools like Nsight Systems use to track scheduling. These are essentially instrumentation points in the kernel (e.g., ","$L91"," tracepoint). Tools may enable these via the perf subsystem or ftrace interface."]}]}],"$L92","$L93","$L94"]}]
5b:["$","p",null,{"children":["Overall, implementing instrumentation is about carefully choosing ",["$","em",null,{"children":"where"}]," to intercept (user API, library, or kernel), ",["$","em",null,{"children":"how"}]," to inject the probe (source vs binary vs dynamic), and ",["$","em",null,{"children":"how to handle the data"}]," (immediate print vs buffered). The tools we discussed illustrate these choices: e.g., TAU uses mostly user API wrapping, Omnitrace uses DBI, GPUprobe uses kernel uprobes, Nsight relies on driver-provided hooks, HPCToolkit uses a mix of driver hooks and its own wrappers."]}]
5c:["$","h2",null,{"className":"content-header","id":"gpu-instruction-level-tracing-techniques","children":[["$","a",null,{"className":"break-words","href":"#gpu-instruction-level-tracing-techniques","aria-hidden":"true","tabIndex":"-1","children":["$","span",null,{"className":"content-header-link","children":["$","svg",null,{"className":"h-5 linkicon w-5","fill":"currentColor","viewBox":"0 0 20 20","xmlns":"http://www.w3.org/2000/svg","children":[["$","path",null,{"d":"M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z"}],["$","path",null,{"d":"M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z"}]]}]}]}],"GPU Instruction-Level Tracing Techniques"]}]
5d:["$","p",null,{"children":"Profiling at the level of individual GPU instructions or warps is extremely challenging due to the volume of operations and limited observability. However, some approaches exist:"}]
5e:["$","ul",null,{"children":[["$","li",null,{"children":[["$","strong",null,{"children":"GPU PC Sampling:"}]," As mentioned, NVIDIA GPUs support periodic sampling of the program counter on each SM. This is analogous to CPU PC sampling and provides a statistical view of which instructions (or source lines) are executing frequently on the GPU. HPCToolkit uses this to attribute time or cycles to lines in GPU kernels. The CUPTI PC sampling mechanism can also attribute ",["$","em",null,{"children":"stall reasons"}]," for sampled warps (e.g., whether the warp was stalled on memory, execution dependency, etc.) – giving insight into why performance is lost. This effectively provides instruction-level insight (which parts of the code are hotspots, and what stalls they incur). The overhead can be non-trivial and early implementations serialized execution, but newer GPUs handle PC sampling more efficiently by offloading sample collection to hardware buffers."]}],["$","li",null,{"children":[["$","strong",null,{"children":"Hardware Pipeline Statistics:"}]," Rather than tracing every instruction, GPUs have counters for things like “number of warp instructions issued per cycle” or “number of stalled cycles on X reason”. Nsight Compute reads these to produce an ",["$","em",null,{"children":"instruction pipeline utilization"}]," analysis (e.g., showing IPC, issue stall breakdown). While not a literal trace, it’s an aggregated view at instruction-level categories. Some of these counters essentially bucket the cycles of warps into categories (memory, compute, etc.), which is almost like tracing the cause of pipeline bubbles over time."]}],["$","li",null,{"children":[["$","strong",null,{"children":"Wavefront/warp trace debug modes:"}]," In debugging contexts, GPUs can be put into modes where a single warp runs to completion or logs certain events. AMD’s GPU PerfStudio had low-level modes, and NVIDIA has tools like Nsight Graphics that can capture per-draw call timing down to shader instructions (for graphics). But for general compute, such instruction traces are rarely used due to overhead."]}],["$","li",null,{"children":[["$","strong",null,{"children":"Reverse-engineering and Undocumented Methods:"}]," Open-source developers sometimes reverse-engineer shader ISA to get at instruction info. For example, HPCToolkit’s developers wrote their own CUBIN analyzer to map instruction addresses to line numbers, since NVIDIA doesn’t provide source-line mapping for SASS. Similarly, on AMD, one might parse the code object (their ISA) to correlate with HSAIL or source. These are offline analyses but crucial for instruction-level profiling because the tool must ",["$","em",null,{"children":"attribute performance metrics to individual instructions or lines"}],". Without vendor documentation, this involves a lot of custom tooling."]}]]}]
5f:["$","p",null,{"children":["In summary, ",["$","strong",null,{"children":"instruction-level tracing on GPU is done via smart sampling and counters rather than literal event-by-event tracing"}]," (which would overwhelm any system). The combination of PC sampling and specialized counters gives an approximation of an instruction trace, sufficient to highlight inefficient instructions or stalled portions of code. This level of detail is typically only needed when optimizing a specific kernel (hence tools like Nsight Compute or academic projects like ",["$","strong",null,{"children":"GPU performance simulators"}],"). Most general-purpose profilers don’t emit a log of every instruction executed (that’s more the domain of simulators or very special debug modes)."]}]
60:["$","h2",null,{"className":"content-header","id":"data-collection-transfer-and-storage","children":[["$","a",null,{"className":"break-words","href":"#data-collection-transfer-and-storage","aria-hidden":"true","tabIndex":"-1","children":["$","span",null,{"className":"content-header-link","children":["$","svg",null,{"className":"h-5 linkicon w-5","fill":"currentColor","viewBox":"0 0 20 20","xmlns":"http://www.w3.org/2000/svg","children":[["$","path",null,{"d":"M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z"}],["$","path",null,{"d":"M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z"}]]}]}]}],"Data Collection, Transfer, and Storage"]}]
61:["$","p",null,{"children":"The profilers gather large amounts of data – how they manage it is as important as the collection itself. Key considerations are: how data moves from GPU to host, how it’s buffered, what file formats are used, and how the user ultimately accesses it."}]
62:["$","ul",null,{"children":[["$","li",null,{"children":["$","p",null,{"children":[["$","strong",null,{"children":"On-GPU to CPU Data Transfer:"}]," When collecting GPU performance data (counters or traces), the data must be transferred to the host. CUPTI’s Activity API, for example, accumulates records in a buffer and then flushes them to the host memory via driver callbacks. This typically happens after a kernel completes or when a buffer fills. For continuous metrics, DCGM or others periodically query the GPU via driver calls (which under the hood read registers over PCIe). The overhead of data transfer is mitigated by batching: profilers use large buffers and only occasionally interrupt the GPU to drain data. Some advanced usage might pin a buffer and have the GPU DMA out the data (like NVIDIA’s NVPW for streaming counters does something akin to that). But details are often abstracted by the vendor library."]}]}],["$","li",null,{"children":["$","p",null,{"children":[["$","strong",null,{"children":"Buffering and Streaming:"}]," Most tools implement a double-buffer or queue so that data collection and writing to disk don’t block the application. For example, HPCToolkit logs GPU events into an in-memory buffer and writes to a file after the run. Nsight Systems writes to a memory-mapped file incrementally (it can even stream to the UI if connected). GPUprobe uses a BPF queue that the user-space reads every few seconds. Streaming trace in real-time is tricky due to volume, but some tools (like certain debug modes or Intel GPA) allow live viewing by sending events over a socket to a GUI. In HPC, typically data is written to local storage for post-mortem analysis because the volume is high (multi-GB traces)."]}]}],["$","li",null,{"children":[["$","p",null,{"children":[["$","strong",null,{"children":"File Formats:"}]," There are a variety of formats:"]}],["$","ul",null,{"children":[["$","li",null,{"children":[["$","strong",null,{"children":"Proprietary Binary"}],": Nsight Systems and Compute use their own binary formats (",["$","code",null,{"children":".nsys-rep"}]," is actually a SQLite database in Nsight Systems; Nsight Compute ",["$","code",null,{"children":".ncu-rep"}]," is a structured binary with sections). These are optimized for their GUIs and not intended to be hand-edited."]}],["$","li",null,{"children":[["$","strong",null,{"children":"Open Trace Formats"}],": Some open tools use standard formats like ",["$","strong",null,{"children":"OTF2 (Open Trace Format)"}]," or ",["$","strong",null,{"children":"CTF"}],". TAU can export to OTF2 for example. These formats are designed for merging traces from multiple ranks in HPC."]}],["$","li",null,{"children":[["$","strong",null,{"children":"Perfetto/Chrome JSON"}],": Perfetto’s format (protobuf trace) or the older Chrome Trace Event format (JSON) is used by several tools because Chrome’s tracing infrastructure is a de-facto standard. PyTorch’s profiler, for instance, outputs a Chrome JSON trace that TensorBoard or chrome://tracing can display. Omnitrace writes Perfetto ",["$","code",null,{"children":".proto"}]," files viewable in Perfetto UI. GPUprobe doesn’t produce a full timeline, but it could be extended to output a Chrome trace JSON of events."]}],["$","li",null,{"children":[["$","strong",null,{"children":"Plain text/CSV"}],": Many tools also generate summary reports in human-readable form. Nsight Compute CLI can output CSV of metrics per kernel. HPCToolkit produces text reports of top hotspots. DCGM’s dcgmi can print current metrics to console, and DCGM exporter outputs text for Prometheus."]}],"$L95"]}]]}],"$L96","$L97"]}]
63:["$","p",null,{"children":["To sum up, ",["$","strong",null,{"children":"data handling"}]," in these tools involves careful buffering of events, using efficient file formats (often binary or compressed), and providing ways to either visualize directly or export to other analysis frameworks. The trends are toward standardizing on formats like ",["$","strong",null,{"children":"Perfetto/Chromium trace"}]," for timeline data and ",["$","strong",null,{"children":"JSON/CSV"}]," for summary metrics to ease integration. Tools like Omnitrace explicitly embrace web visualization by producing Perfetto traces, while HPC tools remain more custom but offer conversion utilities. In cluster monitoring, streaming export (Prometheus via DCGM exporter) is key for real-time observability. So we see both ",["$","strong",null,{"children":"post-mortem analysis workflows"}]," (HPC, desktop profiling) and ",["$","strong",null,{"children":"live monitoring"}]," (datacenter, cloud) depending on the use case."]}]
64:["$","h2",null,{"className":"content-header","id":"visualization-and-analysis-interfaces","children":[["$","a",null,{"className":"break-words","href":"#visualization-and-analysis-interfaces","aria-hidden":"true","tabIndex":"-1","children":["$","span",null,{"className":"content-header-link","children":["$","svg",null,{"className":"h-5 linkicon w-5","fill":"currentColor","viewBox":"0 0 20 20","xmlns":"http://www.w3.org/2000/svg","children":[["$","path",null,{"d":"M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z"}],["$","path",null,{"d":"M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z"}]]}]}]}],"Visualization and Analysis Interfaces"]}]
65:["$","p",null,{"children":"The way profiling data is presented is crucial for deriving insights. Different visualizations help answer different questions:"}]
66:["$","ul",null,{"children":[["$","li",null,{"children":["$","p",null,{"children":[["$","strong",null,{"children":"Timeline Graphs:"}]," These are offered by Nsight Systems, Chrome/Perfetto trace viewer, HPCToolkit’s TraceViewer, etc. A timeline shows ",["$","em",null,{"children":"time on the horizontal axis"}]," and different activities (threads, GPU streams, network transfers) on the vertical axis. This is excellent for identifying concurrency issues and idle gaps. For example, one can see if the GPU is idle while the CPU does something (indicating a potential CPU bottleneck or insufficient work queued), or if CPU threads are idle waiting for GPU (maybe synchronization delays). In a timeline, you might see overlapping rectangles where one kernel is executed while another CPU thread runs – which is good utilization – versus large gaps which signal pipeline stalls. ",["$","strong",null,{"children":"Dependency or event links"}]," can be drawn (Nsight shows arrows from a CPU launch event to the GPU kernel execution event it triggered), so you see relationships and any queuing delay. Timelines can also show ",["$","strong",null,{"children":"context switches"}]," on CPU (with different thread colors before/after a switch), helping identify if too many context switches are hurting performance. Overall, timeline visualization yields insights like pipeline bottlenecks (e.g., GPU waiting on data from disk/CPU), overlapping computation/communication, and load balance across devices."]}]}],["$","li",null,{"children":["$","p",null,{"children":[["$","strong",null,{"children":"Flame Charts / Call Stack Graphs:"}]," A flame chart is typically produced from sampling data – it visualizes the call stack profile over time or aggregate. HPCToolkit’s HPCViewer and TAU’s ParaProf can display something akin to a flame graph: basically, which functions called which, and how much time was spent in each (often drawn as a stacked bar or “flame” where width corresponds to time). This helps identify hotspots in terms of call context: e.g., you can see that function ",["$","code",null,{"children":"foo()"}]," takes 30% of time, mostly when called from ",["$","code",null,{"children":"main()"}]," vs when called from ",["$","code",null,{"children":"init()"}],". It also can expose recursion or expensive call chains. In GPU context, HPCToolkit can show a flame graph of GPU kernel execution attributed to the CPU call path that launched them – which is unique, as it connects device time into the flame graph. Flame charts (like those generated by Brendan Gregg’s flamegraph tool) are usually static visualizations of sample data; many performance engineers use them to quickly see where time is going."]}]}],["$","li",null,{"children":["$","p",null,{"children":[["$","strong",null,{"children":"Directed Acyclic Graphs (Dependency Graphs):"}]," Some tools (especially in a parallel runtime context) can visualize a task graph or dependencies. For example, in a pipeline of GPU operations, a graph view might show that Kernel A and B run in parallel, then feed into C. NVIDIA’s Nsight Compute doesn’t do this, but Nsight Systems implicitly shows dependencies via timeline. Other analysis tools might reconstruct a graph of computation and data movement (this is more niche, often done manually or via custom analysis, but not a primary feature of the listed tools except perhaps as part of visualizing asynchronous dependencies with arrows)."]}]}],["$","li",null,{"children":["$","p",null,{"children":["$L98"," These can be useful when you have many similar events and want to see distribution. For instance, plotting kernel execution time vs. kernel launch index can show if later kernels got slower (maybe due to thermal throttling or contention). Scatter plots might also show correlation between two metrics – e.g., one could plot achieved occupancy vs. execution time for kernels to see if low occupancy correlates with longer runtime. Tools don’t typically include scatter plots in their UI, but users export data to do such analysis. An exception is some research/professional tools that let you plot one metric vs another across kernels."]}]}],"$L99","$L9a"]}]
67:["$","p",null,{"children":"Each visualization provides different insights:"}]
68:["$","ul",null,{"children":[["$","li",null,{"children":["From a ",["$","strong",null,{"children":"timeline"}],", one could observe a CPU-GPU desynchronization: e.g., CPU launches a kernel then does nothing (idle) until result comes back – indicating the CPU could have done other work or the kernel is a bottleneck. Or the opposite: GPU is idle because CPU isn’t feeding it (maybe single-threaded CPU code causing GPU starvation). These scenarios show up clearly on a timeline as big gaps."]}],["$","li",null,{"children":[["$","strong",null,{"children":"Pipeline bottlenecks"}],": In deep learning, for instance, a timeline might show that data loading on CPU is slow, making GPU wait (common issue). Or if multiple GPUs, one might see one GPU finishes early and then waits at a synchronization – indicating load imbalance."]}],["$","li",null,{"children":[["$","strong",null,{"children":"Memory bandwidth contention"}],": If the profiler captures memory throughput, one might see it maxed out (near 100% utilization) whenever certain kernels run, explaining why those kernels don’t speed up further (memory-bound). DCGM and Nsight can report PCIe or NVLink usage; high usage there might explain delays if the job moves a lot of data. If multiple processes run, a timeline might even show overlapping data transfers that saturate the bus."]}],["$","li",null,{"children":[["$","strong",null,{"children":"Cache misses and stalls"}],": Tools like Nsight Compute list cache hit rates and stall reasons. They might not ",["$","em",null,{"children":"visualize"}]," them, but the report effectively highlights memory contention (e.g., a high L2 miss rate with many stall cycles on memory). A user can infer that memory latency is hurting performance from those metrics."]}],["$","li",null,{"children":[["$","strong",null,{"children":"CPU thread issues"}],": A flame graph or timeline may show excessive context switching or lock contention (if instrumented). Nsight Systems can show mutex wait times for CPU threads, helping find concurrency issues on CPU that indirectly slow GPU feeding."]}],["$","li",null,{"children":[["$","strong",null,{"children":"Dependency graphs"}]," (if manually analyzed) could highlight a critical path in asynchronous work – e.g., maybe kernels could run concurrently but are scheduled sequentially due to a dependency that could be optimized."]}],["$","li",null,{"children":[["$","strong",null,{"children":"Flame charts"}]," on GPU code (via PC sampling) can reveal which part of a kernel’s code is the hotspot. For example, HPCToolkit’s GPU PC sampling might show that 40% of time in a kernel is spent in a particular loop (source line X), hinting that that loop is memory-bound or compute-heavy and ripe for optimization."]}]]}]
69:["$","p",null,{"children":"In practice, many of these tools integrate multiple views. Nsight Systems has timeline plus the ability to zoom in and see per-thread call stacks at an instant. TAU’s ParaProf can show both profile tables and timeline (if tracing enabled). HPCToolkit has separate viewers for aggregate profile (like a top-down call tree with metrics) and the event timeline (if one wants the fine detail). The combination is powerful: one might use a profile to find the main culprit function, then examine timeline to see why it’s slow (perhaps concurrency issues), then use hardware metrics view to see if it’s CPU or memory-bound, etc."}]
6a:["$","h2",null,{"className":"content-header","id":"extensibility-and-custom-instrumentation","children":[["$","a",null,{"className":"break-words","href":"#extensibility-and-custom-instrumentation","aria-hidden":"true","tabIndex":"-1","children":["$","span",null,{"className":"content-header-link","children":["$","svg",null,{"className":"h-5 linkicon w-5","fill":"currentColor","viewBox":"0 0 20 20","xmlns":"http://www.w3.org/2000/svg","children":[["$","path",null,{"d":"M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z"}],["$","path",null,{"d":"M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z"}]]}]}]}],"Extensibility and Custom Instrumentation"]}]
6b:["$","p",null,{"children":"Different tools offer varying degrees of extensibility – the ability to incorporate new event sources or define custom metrics and plugins:"}]
6c:["$","ul",null,{"children":[["$","li",null,{"children":["$","p",null,{"children":[["$","strong",null,{"children":"User Markers and Annotations:"}]," Many profilers allow the user to mark sections of code. NVIDIA provides NVTX (NVIDIA Tools Extension) – a library where developers can annotate code regions or events. Nsight Systems will show these NVTX ranges on the timeline (useful for labeling phases of an app). Similarly, for CPU, there are annotation APIs like Intel’s ITT (Instrumentation and Tracing Technology) that some tools (like VTune or oneAPI tools) use; TAU has its own API (e.g., ",["$","code",null,{"children":"TAU_PROFILE_TIMER_START(name)"}],"). These are not extensions to the tool per se, but they allow the user to inject custom events that the tool will record and visualize. This is crucial for understanding high-level phases (like “Data Preprocessing”, “Training step”, etc. on a timeline)."]}]}],["$","li",null,{"children":["$","p",null,{"children":[["$","strong",null,{"children":"Plugin Architecture:"}]," Some tools have a modular design where new modules can be added. For example, ",["$","strong",null,{"children":"DCGM’s diagnostics"}]," are modular – one can add a new diagnostic plugin (though writing one might require NVIDIA’s cooperation since not fully open). HPCToolkit is not really plugin-based, but being open source, users have added features (like one could add support for a new GPU by writing a new connector for its APIs). TAU is fairly extensible; it’s part of the academic Performance Tools community (aligned with standards like MPI_T, OMPT). If a new runtime comes out (say, a new tasking library), TAU could add an interface for it."]}]}],["$","li",null,{"children":["$","p",null,{"children":[["$","strong",null,{"children":"Defining New Events:"}]," In tools like TAU and HPCToolkit, you can instrument new events by either using the tool’s API (for an app developer) or by the tool maintainers adding wrappers for new library functions. For instance, when OpenACC and OMPT interfaces appeared, TAU quickly integrated those, meaning they effectively “taught” TAU to handle new event types (OpenACC offload events, OpenMP runtime events). Similarly, HPCToolkit adding Level Zero support was a matter of implementing new callback handlers and wrappers. Because HPCToolkit’s design is vendor-neutral, adding an API meant extending its substrate with another connector. Omnitrace, as an AMD research project, could potentially be extended to support CUDA by adding a CUPTI integration (if someone implemented it) – the framework is there (binary instrumentation and Perfetto output), it just needs event sources from NVIDIA."]}]}],["$","li",null,{"children":["$","p",null,{"children":[["$","strong",null,{"children":"Customization of Data Collection:"}]," Some profilers allow scripting or configuration for what to collect. Nsight Compute, for example, has “sections” and rules – advanced users can define custom metric sets or even custom formulas for derived metrics in Nsight Compute’s section files (there’s a way to write custom analysis rules in Nsight). This is a limited form of extensibility targeted at metrics. Similarly, one can choose which counters to collect (reducing overhead). Tools like ",["$","strong",null,{"children":"rocprof"}]," for AMD allow users to specify which counters and which API callbacks to enable via a config file."]}]}],["$","li",null,{"children":["$","p",null,{"children":[["$","strong",null,{"children":"Output Integration:"}]," Extensibility also means how easily the data can be used elsewhere. Tools providing standard formats (JSON, CSV, Prometheus, etc.) are effectively extensible because they allow plugging the data into custom pipelines (like a custom ML performance dashboard or automated regression tester). Omnitrace’s compatibility with Hatchet (which is an analysis library)means users can extend analysis by writing Python to filter or compare profiles. TAU has an interface to merge profiles from multiple runs, which can be used to automate testing and analysis."]}]}],"$L9b","$L9c"]}]
6d:["$","p",null,{"children":["In summary, ",["$","strong",null,{"children":"extensibility"}]," ranges from simple in-app annotations (for end users to mark events) to the ability to integrate new back-ends (for tool developers to support new hardware). The survey tools show that open frameworks and HPC tools prioritize flexibility (because they need to target evolving platforms), whereas vendor GUIs prioritize a polished experience within a fixed scope. For someone building a new profiler, leveraging standards (like CUPTI, ROCtracer, OMPT) is a good starting point because it immediately grants support for various events, and designing the tool to accept ",["$","strong",null,{"children":"plugin modules or config files"}]," for new metrics can future-proof it."]}]
6e:["$","h2",null,{"className":"content-header","id":"overhead-and-intrusiveness-trade-offs","children":[["$","a",null,{"className":"break-words","href":"#overhead-and-intrusiveness-trade-offs","aria-hidden":"true","tabIndex":"-1","children":["$","span",null,{"className":"content-header-link","children":["$","svg",null,{"className":"h-5 linkicon w-5","fill":"currentColor","viewBox":"0 0 20 20","xmlns":"http://www.w3.org/2000/svg","children":[["$","path",null,{"d":"M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z"}],["$","path",null,{"d":"M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z"}]]}]}]}],"Overhead and Intrusiveness Trade-offs"]}]
6f:["$","p",null,{"children":"Profiling inherently perturbs the target program – the key is to minimize and understand this overhead. Different tools have different footprints:"}]
70:["$","ul",null,{"children":[["$","li",null,{"children":["$","p",null,{"children":[["$","strong",null,{"children":"Lightweight Sampling – Low Overhead:"}]," Sampling at a low frequency (e.g., 100-1000 samples per second) typically incurs under 5% overhead, often unnoticeable. HPCToolkit’s CPU sampling falls in this category, making it suitable for large HPC jobs where adding even 10% overhead would be too much. HPCToolkit explicitly aims for a few hundred samples/sec per thread to keep overhead reasonable. Similarly, Nsight Systems’ CPU sampling is designed to be very low overhead, using OS-level perf which is optimized for this use. The trade-off is that you might not catch very short-lived functions or get exact counts."]}]}],["$","li",null,{"children":["$","p",null,{"children":[["$","strong",null,{"children":"Function Instrumentation – Moderate to High Overhead:"}]," Wrapping or inserting code on every function entry/exit or every CUDA call can add significant time, especially if those functions are frequent. TAU’s overhead depends on how fine-grained the instrumentation is: instrumenting only outer loops or MPI calls is cheap, but instrumenting every C++ inline function could be huge. TAU allows choosing the level (e.g., you might not instrument every single GPU kernel launch, only the overall region, if trying to reduce overhead). Omnitrace’s dynamic instrumentation will have some startup overhead to instrument code and runtime overhead for each instrumented function call (they try to minimize it, but it’s there). The Omnitrace docs emphasize using it for overall execution profiling rather than super-fine detail in every kernel (in fact recommending if you want deep insight into individual kernels, use another tool – implying Omnitrace focuses on broad profiling, not micro-optimizing one kernel)."]}]}],["$","li",null,{"children":["$","p",null,{"children":[["$","strong",null,{"children":"GPU Trace Overheads:"}]," Simply tracing kernel events via CUPTI has low overhead – a few microseconds to log an event per kernel. If kernels are numerous (thousands per second), this can start to matter. CUPTI’s activity buffering typically can handle many events efficiently. But when you turn on more detailed GPU profiling (like counters or PC sampling), overhead increases. CUPTI’s PC sampling can degrade application performance, especially in older GPUs where it prevented overlap of kernels. Tools usually warn about this; e.g., Nsight Compute replays kernels because trying to gather all metrics in one go would slow the kernel drastically or is impossible due to counter limits. That replay itself is overhead (often huge – 46x runtime for one kernel in the example). So Nsight Compute is ",["$","em",null,{"children":"intentionally intrusive"}]," – it sacrifices real-time performance for data."]}]}],["$","li",null,{"children":["$","p",null,{"children":[["$","strong",null,{"children":"Memory and Storage Overhead:"}]," Profiling can also increase memory usage (buffers) and produce large output files. If an application is memory-sensitive, large trace buffers might perturb cache usage or memory allocation patterns. For example, enabling detailed tracing might allocate hundreds of MB for buffers, possibly causing more cache misses for the application. Most tools let you configure buffer sizes to trade off between lost events vs memory overhead."]}]}],["$","li",null,{"children":["$","p",null,{"children":[["$","strong",null,{"children":"Perturbation of concurrency:"}]," Intrusive instrumentation can change scheduling. If profiling causes a function to take 2x longer, that could alter how threads interleave or how the GPU stream gets work. For instance, heavy instrumentation on CPU could feed work slower to GPU, making the GPU appear underutilized when in reality it’s the profiler’s overhead causing it. Good profilers try to quantify or bound their overhead to avoid misleading the user. Some provide an “overhead compensation” or at least measure how much time was spent in the profiling code itself (TAU sometimes includes metrics for “profiler overhead”)."]}]}],"$L9d","$L9e"]}]
71:["$","p",null,{"children":["$","strong",null,{"children":"Comparison of the tools:"}]}]
72:["$","ul",null,{"children":[["$","li",null,{"children":[["$","em",null,{"children":"Nsight Systems:"}]," Designed for low overhead system profiling, typically ",["$","code",null,{"children":"<5%"}]," overhead with default settings (more if CPU sampling frequency increased or if tracing extremely many events). It’s generally acceptable to run in production for a short period."]}],["$","li",null,{"children":[["$","em",null,{"children":"Nsight Compute:"}]," Very high overhead (can be orders of magnitude) for target kernels, not meant for production runs – it’s a diagnostics tool for devs."]}],["$","li",null,{"children":[["$","em",null,{"children":"HPCToolkit:"}]," Very low overhead for sampling (suitable for long HPC runs). If GPU PC sampling is used, overhead on GPU could be moderate (maybe 2x slowdown for that kernel depending on sampling rate). But HPCToolkit’s default of only tracing kernels adds little overhead."]}],["$","li",null,{"children":[["$","em",null,{"children":"Omnitrace:"}]," Medium overhead – dynamic instrumentation of potentially many functions and sampling combined. It’s optimized for use on large apps but it’s still doing a lot. They likely achieve overhead in the low tens of percent for moderate instrumentation, but the user should choose narrower focus to keep overhead manageable."]}],["$","li",null,{"children":[["$","em",null,{"children":"GPUprobe:"}]," Very low overhead for what it does (",["$","code",null,{"children":"<4%"}],"), because it only hooks a few slow calls. It’s explicitly touted as lightweight."]}],["$","li",null,{"children":[["$","em",null,{"children":"DCGM:"}]," Extremely low overhead. It’s running in the background and only polling metrics (which is usually negligible on GPU – reading a counter register isn’t free but it’s minor, and DCGM is optimized for minimal impact). Unless you set it to sample at an insanely high frequency, it won’t bother the GPU or CPU much. It’s safe for always-on monitoring."]}],["$","li",null,{"children":[["$","em",null,{"children":"TAU:"}]," Varies widely. If using only sampling or coarse instrumentation (MPI calls, high-level loops), overhead is low. If using automatic source instrumentation at fine granularity, overhead can be high (possibly 2x or more runtime). TAU provides many options to adjust this, like filtering out small functions from instrumentation."]}]]}]
73:["$","p",null,{"children":[["$","strong",null,{"children":"Trade-off summary:"}]," For day-to-day performance engineering, one often starts with low-overhead broad tools (like Nsight Systems or HPCToolkit sampling) to find major issues, then uses higher overhead targeted tools (like Nsight Compute or adding more instrumentation in TAU) on specific kernels or code regions to dive deeper. The implementer of a new tool must decide where on this spectrum to sit – a new always-on monitoring tool should be low overhead (maybe sampling-based, like GPUperf counters streaming), whereas a new deep analysis tool can be intrusive but used sparingly. The user often needs to be aware of the overhead and not confuse profiler-induced slowdowns with real behavior (which is why cross-checking with multiple tools or running at different sampling rates is good practice)."]}]
74:["$","h2",null,{"className":"content-header","id":"security-and-sandboxing-considerations","children":[["$","a",null,{"className":"break-words","href":"#security-and-sandboxing-considerations","aria-hidden":"true","tabIndex":"-1","children":["$","span",null,{"className":"content-header-link","children":["$","svg",null,{"className":"h-5 linkicon w-5","fill":"currentColor","viewBox":"0 0 20 20","xmlns":"http://www.w3.org/2000/svg","children":[["$","path",null,{"d":"M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z"}],["$","path",null,{"d":"M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z"}]]}]}]}],"Security and Sandboxing Considerations"]}]
75:["$","p",null,{"children":"Profiling tools often need low-level access, which raises security and isolation concerns, especially on shared systems or in the cloud:"}]
76:["$","ul",null,{"children":[["$","li",null,{"children":[["$","strong",null,{"children":"Privilege Requirements:"}]," As noted, accessing hardware counters on GPUs is often restricted to administrators by default (NVIDIA’s driver default ",["$","code",null,{"children":"NVreg_RestrictProfilingToAdminUsers=1"}]," prevents non-root from using CUPTI metrics or NSight). Many tools (Nsight, HPCToolkit) will simply not capture certain metrics unless that is disabled or run as root. The reason is that performance counters could potentially be used as side-channels (e.g., timing another process’s GPU usage). DCGM similarly might require elevated privileges for certain ops, but its open-source core runs in user space with proper config."]}],["$","li",null,{"children":[["$","strong",null,{"children":"Containerization:"}]," Running profilers inside containers (Docker/Kubernetes) is tricky. By design, containers restrict access to performance monitoring. For instance, the ",["$","code",null,{"children":"perf_event_paranoid"}]," sysctl applies system-wide – inside a container you might not be allowed to profile unless the host setting is permissive. Tools like Nsight can run in containers if given the right privileges (the NVIDIA container toolkit can pass through GPU profiling capability). There is an official DCGM container for monitoring GPUs in Kubernetes, which runs privileged to collect data and then provides metrics in a safe way to unprivileged consumers (via network endpoint). If one tries to run HPCToolkit or TAU in a container on a cluster without privileges, they might not be able to attach to GPU events. Some cloud providers explicitly disable profiling on their GPUs for security."]}],["$","li",null,{"children":[["$","strong",null,{"children":"Multi-tenant GPUs:"}]," If two users share a GPU (via MPS or virtualization), profiling could either fail or potentially expose the other user’s activity. Typically, vendor drivers prevent that by the admin-only restriction. AMD’s ROCm might allow multiple user processes but rocm profiling likely only sees your own queues (still, usually root access is needed for perf counters). In multi-tenant scenarios, often the operator will allow ",["$","em",null,{"children":"monitoring"}]," (like DCGM aggregate stats) but not detailed tracing by arbitrary users. This is something a new profiling tool must consider – you may have to integrate with permission models. For example, a site might run the profiler as a privileged service that users can request data from, rather than giving the binary to users to run."]}],["$","li",null,{"children":[["$","strong",null,{"children":"Security of instrumentation methods:"}]," Techniques like eBPF and ptrace (which some tools use for attaching to processes) are restricted by Linux capabilities. By default, a process cannot ptrace another process of a different user, and unprivileged BPF might be limited (recent kernels allow some unprivileged BPF with restrictions, but uprobes might need CAP_SYS_ADMIN depending on config). Therefore, GPUprobe likely needs root or at least the BPF capability to attach uprobes to another process’s libcudart. This limits using such a tool in production unless proper privileges are arranged."]}],["$","li",null,{"children":[["$","strong",null,{"children":"Data Sanitization:"}]," Profiling data might inadvertently capture sensitive information (e.g., addresses, sizes of allocations that hint at data sizes, or even kernel names that might be proprietary algorithms). So sharing profiles must be done carefully. Some tools anonymize certain data when needed (e.g., one could filter out kernel names or memory addresses)."]}],"$L9f","$La0"]}]
77:["$","p",null,{"children":["In short, profiling in sandboxed environments often requires deliberate enabling by administrators. An ",["$","strong",null,{"children":"in-house observability platform"}]," might use a privileged agent (like DCGM exporter) to gather metrics and expose them in a controlled way to users, rather than letting users run arbitrary profilers on shared hardware. Many vendors are working on making profiling more accessible in containers (NVIDIA’s support for NVTX and certain CUPTI features in containers improved with the NVIDIA Docker integration)."]}]
78:["$","p",null,{"children":"For someone building new tools, it’s wise to be aware of these restrictions and possibly design the tool so it can either:"}]
79:["$","ul",null,{"children":[["$","li",null,{"children":"operate in a reduced functionality mode without special privileges (e.g., only use user-space timing, not hardware counters), or"}],["$","li",null,{"children":"be deployed as a privileged service that multiple users can safely use (maybe via an authenticated API)."}]]}]
7a:["$","h2",null,{"className":"content-header","id":"limitations-caveats-and-future-directions","children":[["$","a",null,{"className":"break-words","href":"#limitations-caveats-and-future-directions","aria-hidden":"true","tabIndex":"-1","children":["$","span",null,{"className":"content-header-link","children":["$","svg",null,{"className":"h-5 linkicon w-5","fill":"currentColor","viewBox":"0 0 20 20","xmlns":"http://www.w3.org/2000/svg","children":[["$","path",null,{"d":"M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z"}],["$","path",null,{"d":"M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z"}]]}]}]}],"Limitations, Caveats, and Future Directions"]}]
7b:["$","p",null,{"children":"Despite their power, current GPU/DPU profiling tools have limitations. Being aware of these helps identify areas for future work:"}]
7c:["$","ul",null,{"children":[["$","li",null,{"children":["$","p",null,{"children":[["$","strong",null,{"children":"Limited Vendor Support in One Tool:"}]," No single tool covers all accelerators well. Nsight is NVIDIA-only; Omnitrace currently AMD-only; Intel’s tools are Intel-only. Tools like HPCToolkit and TAU try to be multi-platform, but even they require different backends per vendor and may not expose every vendor-specific metric. A truly ",["$","strong",null,{"children":"unified profiling standard"}]," is still lacking. OneAPI is a step in that direction for CPUs/GPUs (with Level Zero and an Instrumentation interface), but adoption is ongoing. Future work could develop a universal trace format or API so that one tool (or a plugin to it) can handle GPUs from multiple vendors seamlessly."]}]}],["$","li",null,{"children":["$","p",null,{"children":[["$","strong",null,{"children":"Depth vs. Overhead Trade-off:"}]," As discussed, capturing everything (full trace of a long run) can be infeasible. So tools either limit duration, frequency, or level of detail. Users need to be mindful that, for example, Nsight Systems might drop events if the buffer overflows, or HPCToolkit might not record every single tiny kernel if they occur too frequently (there’s often filters for very short kernels to avoid drowning in data). This means profiles might not have 100% fidelity. A caveat is that enabling profiling can change performance; e.g., the act of profiling might serialize some GPU operations or change timing, so the user must treat results as approximations. Future profilers might leverage more hardware support to reduce overhead (e.g., hardware trace buffers, as seen in CPUs with Intel PT; maybe someday GPUs have a hardware trace that tools can harvest)."]}]}],["$","li",null,{"children":[["$","p",null,{"children":[["$","strong",null,{"children":"DPU and Heterogeneous Workload Profiling:"}]," Profiling DPUs (data processing units) and tightly integrated systems (APUs where CPU and GPU share memory) is an evolving area. For DPUs, since they combine network, storage, and compute offloads, a profiler needs to capture network events (packets or protocol processing) along with CPU events on the DPU’s Arm cores. Today, one might use standard Linux tools on the DPU (like perf for CPU and maybe specialized NIC counters via ethtool). There isn’t yet a widely-used dedicated DPU profiler that shows, say, “this flow in the NIC caused this CPU processing on the DPU.” ",["$","strong",null,{"children":"Future work"}]," could integrate NIC telemetry with CPU profiling – for instance, using something like DPDK or DOCA libraries to instrument packet processing on DPUs."]}],["$","ul",null,{"children":[["$","li",null,{"children":"For APUs (CPU+GPU on one die, sharing memory), one limitation is that many profiling tools still treat CPU and GPU separately. But an APU could enable novel profiling: since they share memory, one could trace memory accesses system-wide. AMD’s HSA was meant to unify CPU/GPU memory space, and indeed HSA’s tools could measure things like if a CPU access to a buffer and a GPU access interfere. We don’t yet see mainstream tools showing a combined cache profile of CPU+GPU on an APU. Future research might allow tracking cache coherence events or memory traffic between CPU and GPU in an APU, to see contention (e.g., if CPU and GPU fight over memory bandwidth, how to detect that in a profile? Today you might see both CPU and GPU show high memory use but correlating them is manual)."}],"$La1"]}]]}],"$La2","$La3","$La4","$La5","$La6"]}]
7d:["$","p",null,{"children":["In conclusion, ",["$","strong",null,{"children":"future work"}]," in this field will likely focus on unifying and simplifying the toolchain for heterogeneous profiling, reducing overhead further (perhaps with hardware support), integrating profiling with monitoring (to catch issues in production continuously), and smarter analysis to help developers pinpoint problems faster. The landscape is moving toward open standards (like Perfetto for traces, and maybe something akin to PAPI for GPUs one day) which will make it easier to build custom profilers that plug in. The insights from current tools’ implementations – their successes and pain points – will guide these developments. As hardware continues to evolve (with things like chiplet GPUs, unified memory hierarchies, etc.), profiling tools will need to adapt, but the fundamental techniques (instrumentation vs sampling, hardware counter use, multi-stream correlation) will remain cornerstones of any performance analysis toolkit."]}]
7e:["$","h3",null,{"className":"content-header","id":"nvidia-tools","children":[["$","a",null,{"className":"break-words","href":"#nvidia-tools","aria-hidden":"true","tabIndex":"-1","children":["$","span",null,{"className":"content-header-link","children":["$","svg",null,{"className":"h-5 linkicon w-5","fill":"currentColor","viewBox":"0 0 20 20","xmlns":"http://www.w3.org/2000/svg","children":[["$","path",null,{"d":"M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z"}],["$","path",null,{"d":"M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z"}]]}]}]}],"NVIDIA Tools"]}]
7f:["$","ul",null,{"children":[["$","li",null,{"children":["$","a",null,{"className":"break-words","target":"_blank","rel":"noopener noreferrer","href":"https://www.nas.nasa.gov/hecc/support/kb/performance-analysis-of-your-gpu-applications-with-nsight-systems_701.html","children":"Performance Analysis of Your GPU Applications with Nsight Systems"}]}],["$","li",null,{"children":["$","a",null,{"className":"break-words","target":"_blank","rel":"noopener noreferrer","href":"https://www.nas.nasa.gov/hecc/support/kb/performance-analysis-of-your-gpu-cuda-kernels-with-nsight-compute-cli_706.html","children":"Performance Analysis of Your GPU CUDA Kernels with Nsight Compute CLI"}]}],["$","li",null,{"children":["$","a",null,{"className":"break-words","target":"_blank","rel":"noopener noreferrer","href":"https://docs.nvidia.com/nsight-systems/UserGuide/index.html","children":"User Guide — nsight-systems Documentation"}]}],["$","li",null,{"children":["$","a",null,{"className":"break-words","target":"_blank","rel":"noopener noreferrer","href":"https://developer.nvidia.com/dcgm","children":"NVIDIA DCGM"}]}],["$","li",null,{"children":["$","a",null,{"className":"break-words","target":"_blank","rel":"noopener noreferrer","href":"https://developer.nvidia.com/cuda-profiling-tools-interface","children":"CUDA Profiling Tools Interface (CUPTI)"}]}]]}]
80:["$","h3",null,{"className":"content-header","id":"amd-tools","children":[["$","a",null,{"className":"break-words","href":"#amd-tools","aria-hidden":"true","tabIndex":"-1","children":["$","span",null,{"className":"content-header-link","children":["$","svg",null,{"className":"h-5 linkicon w-5","fill":"currentColor","viewBox":"0 0 20 20","xmlns":"http://www.w3.org/2000/svg","children":[["$","path",null,{"d":"M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z"}],["$","path",null,{"d":"M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z"}]]}]}]}],"AMD Tools"]}]
81:["$","ul",null,{"children":[["$","li",null,{"children":["$","a",null,{"className":"break-words","target":"_blank","rel":"noopener noreferrer","href":"https://rocm.github.io/omnitrace/about.html","children":"About — omnitrace Documentation"}]}],["$","li",null,{"children":["$","a",null,{"className":"break-words","target":"_blank","rel":"noopener noreferrer","href":"https://gpuopen.com/gpuperfapi/","children":"GPUPerfAPI - AMD GPUOpen"}]}]]}]
82:["$","h3",null,{"className":"content-header","id":"open-source-profiling-tools","children":[["$","a",null,{"className":"break-words","href":"#open-source-profiling-tools","aria-hidden":"true","tabIndex":"-1","children":["$","span",null,{"className":"content-header-link","children":["$","svg",null,{"className":"h-5 linkicon w-5","fill":"currentColor","viewBox":"0 0 20 20","xmlns":"http://www.w3.org/2000/svg","children":[["$","path",null,{"d":"M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z"}],["$","path",null,{"d":"M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z"}]]}]}]}],"Open Source Profiling Tools"]}]
83:["$","ul",null,{"children":[["$","li",null,{"children":["$","a",null,{"className":"break-words","target":"_blank","rel":"noopener noreferrer","href":"https://hpctoolkit.org/manual/HPCToolkit-users-manual.pdf","children":"HPCToolkit User's Manual"}]}],["$","li",null,{"children":["$","a",null,{"className":"break-words","target":"_blank","rel":"noopener noreferrer","href":"https://dyninst.github.io/scalable_tools_workshop/petascale2018/assets/slides/HPCToolkit-GPU-ScalableTools-2018.pdf","children":"Extending HPCToolkit for GPU-accelerated Systems"}]}],["$","li",null,{"children":["$","a",null,{"className":"break-words","target":"_blank","rel":"noopener noreferrer","href":"https://pop-coe.eu/blog/tool-time-observing-application-performance-using-the-tau-performance-system%C2%AE","children":"Tool Time: Observing Application Performance Using the TAU Performance System"}]}],["$","li",null,{"children":["$","a",null,{"className":"break-words","target":"_blank","rel":"noopener noreferrer","href":"https://www.alcf.anl.gov/sites/default/files/2023-10/TAU_ALCF23.pdf","children":"Overview of TAU - Argonne Leadership Computing Facility"}]}]]}]
84:["$","h3",null,{"className":"content-header","id":"modern-approaches--ebpf","children":[["$","a",null,{"className":"break-words","href":"#modern-approaches--ebpf","aria-hidden":"true","tabIndex":"-1","children":["$","span",null,{"className":"content-header-link","children":["$","svg",null,{"className":"h-5 linkicon w-5","fill":"currentColor","viewBox":"0 0 20 20","xmlns":"http://www.w3.org/2000/svg","children":[["$","path",null,{"d":"M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z"}],["$","path",null,{"d":"M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z"}]]}]}]}],"Modern Approaches & eBPF"]}]
85:["$","ul",null,{"children":["$","li",null,{"children":["$","a",null,{"className":"break-words","target":"_blank","rel":"noopener noreferrer","href":"https://dev.to/ethgraham/snooping-on-your-gpu-using-ebpf-to-build-zero-instrumentation-cuda-monitoring-2hh1","children":"Snooping on your GPU: Using eBPF to Build Zero-instrumentation CUDA Monitoring"}]}]}]
86:["$","h3",null,{"className":"content-header","id":"other-resources","children":[["$","a",null,{"className":"break-words","href":"#other-resources","aria-hidden":"true","tabIndex":"-1","children":["$","span",null,{"className":"content-header-link","children":["$","svg",null,{"className":"h-5 linkicon w-5","fill":"currentColor","viewBox":"0 0 20 20","xmlns":"http://www.w3.org/2000/svg","children":[["$","path",null,{"d":"M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z"}],["$","path",null,{"d":"M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z"}]]}]}]}],"Other Resources"]}]
87:["$","ul",null,{"children":[["$","li",null,{"children":["$","a",null,{"className":"break-words","target":"_blank","rel":"noopener noreferrer","href":"https://stackoverflow.com/questions/12044928/multi-gpu-profiling-several-cpus-mpi-cuda-hybrid","children":"Multi-GPU profiling (Several CPUs, MPI/CUDA Hybrid)"}]}],["$","li",null,{"children":["$","a",null,{"className":"break-words","target":"_blank","rel":"noopener noreferrer","href":"https://www.olcf.ornl.gov/wp-content/uploads/04-GPUProfiling-Performance-timelines.pdf","children":"GPU Profiling: Performance Timelines"}]}],["$","li",null,{"children":["$","a",null,{"className":"break-words","target":"_blank","rel":"noopener noreferrer","href":"https://blog.csdn.net/qq_45032341/article/details/134235988","children":"解决Nsight Systems错误 (Solving Nsight Systems Errors)"}]}]]}]
88:["$","div",null,{"className":"pt-6 pb-6 text-sm text-gray-700 dark:text-gray-300 glass p-4 rounded-xl","children":[["$","a",null,{"className":"hover:text-primary-600 dark:hover:text-primary-400 transition-colors","target":"_blank","rel":"nofollow","href":"https://mobile.twitter.com/search?q=https%3A%2F%2Fwww.yunwei37.com%2Fblog%2Fgpu-profile-tool-impl","children":"Discuss on Twitter"}]," • ",["$","a",null,{"className":"hover:text-primary-600 dark:hover:text-primary-400 transition-colors","target":"_blank","rel":"noopener noreferrer","href":"https://github.com/yunwei37/yunwei37-blog/blob/main/data/blog/gpu-profile-tool-impl.mdx","children":"View on GitHub"}]]}]
89:["$","div",null,{"className":"pt-6 pb-6 text-center text-gray-700 dark:text-gray-300 glass p-6 rounded-xl","id":"comment","children":["$","$La7",null,{"slug":"gpu-profile-tool-impl"}]}]
8a:["$","footer",null,{"children":[["$","div",null,{"className":"divide-gray-200/30 text-sm leading-5 font-medium xl:col-start-1 xl:row-start-2 xl:divide-y dark:divide-gray-700/30","children":[["$","div",null,{"className":"py-4 xl:py-8","children":[["$","h2",null,{"className":"text-xs tracking-wide text-gray-600 uppercase dark:text-gray-300 mb-4","children":"Tags"}],["$","div",null,{"className":"flex flex-wrap gap-2","children":[["$","$L6","ebpf",{"href":"/tags/ebpf","className":"inline-block px-3 py-1 text-xs font-medium uppercase tracking-wide bg-primary-100 hover:bg-primary-200 dark:bg-primary-900/30 dark:hover:bg-primary-800/40 rounded-full transition-all duration-200 text-primary-800 hover:text-primary-900 dark:text-primary-300 dark:hover:text-primary-200 border border-primary-200/50 dark:border-primary-700/50","children":"ebpf"}],["$","$L6","systems",{"href":"/tags/systems","className":"inline-block px-3 py-1 text-xs font-medium uppercase tracking-wide bg-primary-100 hover:bg-primary-200 dark:bg-primary-900/30 dark:hover:bg-primary-800/40 rounded-full transition-all duration-200 text-primary-800 hover:text-primary-900 dark:text-primary-300 dark:hover:text-primary-200 border border-primary-200/50 dark:border-primary-700/50","children":"systems"}]]}]]}],["$","div",null,{"className":"flex justify-between py-4 xl:block xl:space-y-6 xl:py-8","children":[["$","div",null,{"className":"glass p-4 rounded-xl","children":[["$","h2",null,{"className":"text-xs tracking-wide text-gray-600 uppercase dark:text-gray-300 mb-2","children":"Previous Article"}],["$","div",null,{"className":"text-primary-600 hover:text-primary-700 dark:text-primary-400 dark:hover:text-primary-300 transition-colors","children":["$","$L6",null,{"className":"break-words","href":"/blog/gpu-profile-tools-analysis","children":"The Accelerator Toolkit: A Review of Profiling and Tracing for GPUs and other co-processor"}]}]]}],["$","div",null,{"className":"glass p-4 rounded-xl","children":[["$","h2",null,{"className":"text-xs tracking-wide text-gray-600 uppercase dark:text-gray-300 mb-2","children":"Next Article"}],["$","div",null,{"className":"text-primary-600 hover:text-primary-700 dark:text-primary-400 dark:hover:text-primary-300 transition-colors","children":["$","$L6",null,{"className":"break-words","href":"/blog/eurosys-2025","children":"EuroSys 2025 Paper Summaries and Analysis"}]}]]}]]}]]}],["$","div",null,{"className":"pt-4 xl:pt-8","children":["$","$L6",null,{"className":"inline-flex items-center glass px-4 py-2 rounded-xl text-primary-600 hover:text-primary-700 dark:text-primary-400 dark:hover:text-primary-300 transition-all duration-200 hover:scale-105","href":"/blog","aria-label":"Back to the blog","children":"← Back to the blog"}]}]]}]
8b:["$","strong",null,{"children":"hybrid approach"}]
8c:["$","em",null,{"children":"instrumentation wrappers"}]
8d:["$","em",null,{"children":"sampling"}]
8e:["$","strong",null,{"children":"Nsight Systems"}]
8f:["$","strong",null,{"children":"HPCToolkit"}]
90:["$","strong",null,{"children":"Omnitrace"}]
91:["$","code",null,{"children":"sched:sched_switch"}]
92:["$","li",null,{"children":["$","p",null,{"children":[["$","strong",null,{"children":"GPU Instruction Injection:"}]," In some cases, instrumentation can be inserted into GPU code itself. This is not common in commercial tools (due to risk of altering behavior), but research tools or certain debug modes do this. For example, one could modify PTX (CUDA assembly) to call a counter increment at certain points. NVIDIA’s older profiling (CUPTI PC sampling) is a better approach than explicit instrumentation, but NVIDIA ",["$","em",null,{"children":"could"}]," instrument at warp-trap instructions. AMD’s shader profiling might insert “wave timestamps” in code. Typically, however, vendors avoid modifying user kernels for profiling, instead running them under special modes (like single-warp replay or serializing execution for analysis)."]}]}]
93:["$","li",null,{"children":["$","p",null,{"children":[["$","strong",null,{"children":"Uprobe/Uretprobe for arguments/returns:"}]," As seen with GPUprobe, sometimes both entry and exit of a function need to be instrumented to get arguments and return values. Uretprobes catch the function return, but since registers may be clobbered, a combination of an entry uprobe (to stash arguments) and a return probe is used. This technique is needed for measuring things like allocation sizes (argument at entry) and allocated pointer (return at exit) without altering the function."]}]}]
94:["$","li",null,{"children":["$","p",null,{"children":[["$","strong",null,{"children":"Event Buffers and Async Logging:"}]," Instrumentation often logs data into a buffer to minimize interference. CUPTI, for instance, writes activity records into a buffer which the profiler reads later. eBPF writes events into a BPF ring buffer or map, consumed by user-space asynchronously. This decoupling is critical for performance: the instrumented function triggers a quick log write (e.g., just drop an event in a lock-free queue) and then continues, while another thread or process handles heavy work like writing to disk or network."]}]}]
95:["$","li",null,{"children":[["$","strong",null,{"children":"Database"}],": Some tools store data in a SQL or custom database. As mentioned, Nsight Systems uses SQLite under the hood; TAU’s PerfDMF historically used a DB for storing profile data for query. Using a database can facilitate querying specific metrics after the fact."]}]
96:["$","li",null,{"children":[["$","p",null,{"children":[["$","strong",null,{"children":"Export Paths and Telemetry"}],": Tools differ in how you retrieve the data."]}],["$","ul",null,{"children":[["$","li",null,{"children":[["$","strong",null,{"children":"File-based workflow"}],": e.g., run ",["$","code",null,{"children":"nsys profile ..."}]," which produces a file to open later; run ",["$","code",null,{"children":"hpcrun"}]," then ",["$","code",null,{"children":"hpcstruct/hpcprof"}]," to generate an analysis database."]}],["$","li",null,{"children":[["$","strong",null,{"children":"Live UI"}],": e.g., Nsight Systems can also be run with the GUI attached to a remote app, streaming data. Intel VTune can attach to a running process and periodically update stats in the UI."]}],["$","li",null,{"children":[["$","strong",null,{"children":"Web Dashboard"}],": Some modern profilers have web interfaces. For instance, NVIDIA launched ",["$","strong",null,{"children":"Nsight Systems in a web app"}]," for their cloud, and Omnitrace results can be loaded into a Perfetto web viewer easily. DCGM’s Kubernetes integration is specifically for dashboards: DCGM exporter feeds Prometheus, and you visualize in Grafana or others."]}],["$","li",null,{"children":[["$","strong",null,{"children":"Integration into TensorBoard"}],": Tools aimed at ML (like PyTorch’s Kineto or TensorFlow’s profiler) export data that TensorBoard’s profiling plugin can display. These often use the Chrome trace format. As an example, PyTorch with Kineto can capture CUDA kernel timelines and display them in TensorBoard, effectively providing a Nsight-like view but within the ML tooling context. Underneath, Kineto uses CUPTI (so similar data as Nsight Systems) but converts to Chrome format."]}],["$","li",null,{"children":[["$","strong",null,{"children":"Custom Telemetry APIs"}],": A few tools allow programmatic access. DCGM has a C and Python API so you can query metrics from within your application or a script. CUPTI provides a programmatic way to start/stop tracing and get records (so one could embed CUPTI in an app to collect its own traces). Some HPC monitoring systems integrate profiling by hooking these APIs."]}]]}]]}]
97:["$","li",null,{"children":["$","p",null,{"children":[["$","strong",null,{"children":"Data Volume and Reduction"}],": A single run can produce enormous data (traces of every kernel launch, etc.). Tools implement filtering and levels of detail. Nsight Systems has options to limit collection (e.g., only trace CUDA, ignore OS scheduling if not needed). HPCToolkit can be configured to only trace GPU and not collect CPU samples, etc. They also have options to include/exclude specific kernels or API calls. This is important because writing out every event in a long run might be infeasible. Some tools, like ",["$","strong",null,{"children":"Score-P (used with TAU in large-scale scenarios)"}],", allow switching between profiling (aggregating counts) and tracing (detailed event log) modes, because full tracing doesn’t scale beyond a point."]}]}]
98:["$","strong",null,{"children":"Scatter Plots:"}]
99:["$","li",null,{"children":["$","p",null,{"children":[["$","strong",null,{"children":"Heatmaps:"}]," A heatmap might be used to show, for example, a timeline heatmap of GPU unit utilization (some tools in graphics show per-SM utilization over time as a heatmap). Another use is visualizing memory access patterns – e.g., GPU memory addresses vs time. These are specialized and not found in general profilers, but one could generate a heatmap of, say, cache miss rate over time slices to spot phases of computation. In profiling literature, a “GPU utilization heatmap” could refer to showing how busy each engine was over time."]}]}]
9a:["$","li",null,{"children":["$","p",null,{"children":[["$","strong",null,{"children":"Metrics Dashboards:"}]," Some visualizations are simply tables or charts of metrics. Nsight Compute’s GUI shows a table of metrics and may highlight in red those that are problematic. DCGM with Grafana will show line charts of metrics over time (like GPU temperature or utilization %). Those help with monitoring and trend analysis (did a job saturate memory bandwidth? Did GPU utilization drop at some point?)."]}]}]
9b:["$","li",null,{"children":["$","p",null,{"children":[["$","strong",null,{"children":"Limitations on Extensibility:"}]," Notably, closed-source tools like Nsight Systems/Compute do not support plugins – you cannot add a new type of event to Nsight beyond what NVIDIA provided. You are limited to NVTX markers for custom annotations. In contrast, open tools (HPCToolkit, TAU, Omnitrace, GPUprobe) allow you to modify or extend them, but it requires programming. GPUprobe, for example, could be extended to trace new CUDA functions or even other libraries by adding more BPF probes in its source."]}]}]
9c:["$","li",null,{"children":["$","p",null,{"children":[["$","strong",null,{"children":"Scripting and Automation:"}]," Some profiling frameworks (especially in HPC) support automated profiling experiments. For example, using TAU’s Python interface or scripting TAU commander to collect different metrics on different runs, then synthesizing results. This isn’t a plugin, but it shows the tool can be integrated into larger workflows (like auto-tuning loops or continuous integration to catch performance regressions)."]}]}]
9d:["$","li",null,{"children":["$","p",null,{"children":[["$","strong",null,{"children":"Accuracy vs Intrusiveness:"}]," The more accuracy/detail you want, typically the more overhead you incur. E.g., logging every single memory allocation (as GPUprobe does) can slow down an application that allocates frequently, but GPUprobe found in typical CUDA codes, the relative overhead is small because those calls are not extremely frequent and already take time. If someone ran a micro-benchmark that calls ",["$","code",null,{"children":"cudaMalloc"}]," thousands of times a second, GPUprobe’s overhead might become more noticeable (due to context switch on each call). Similarly, sampling has adjustable frequency: if you sample at 10 kHz on every CPU core, you might start seeing a few percent overhead or more, and more disturbingly, you might interfere with CPU turbo frequencies or power usage slightly. Usually, a safe range is chosen as default (like 1 kHz)."]}]}]
9e:["$","li",null,{"children":["$","p",null,{"children":[["$","strong",null,{"children":"Parallel Scalability of Overhead:"}]," In distributed runs (MPI with GPUs), one also considers that a profiling tool might increase communication or I/O overhead. E.g., if each rank tries to write a large trace file, the I/O load can slow things. Some tools merge data to avoid too many outputs (Score-P merges traces, etc.), but that can cause sync overhead at program end. Overheads can multiply with scale if not careful, which is why sampling (with fixed overhead per process) is often preferred in HPC at scale."]}]}]
9f:["$","li",null,{"children":[["$","strong",null,{"children":"DPUs and isolated environments:"}]," In a DPU (like Nvidia BlueField, which has an Arm CPU and runs its own OS), you might profile the DPU similar to any Linux server (since BlueField runs Ubuntu). But if the DPU is running in a mode where the host offloads tasks to it, profiling the interplay is complex. One might need to profile the host and DPU separately then correlate. For security, DPUs often isolate the host view – the host might not easily introspect the DPU’s processes without permission. So profiling a DPU might require logging into the DPU’s OS. This is an emerging area; tools like DCGM currently don’t cover DPUs (DCGM is GPU specific). A future “DPU profiler” might need to consider network security (ensuring tracing network packets doesn’t expose data contents) and multi-tenant (a DPU might serve multiple VMs)."]}]
a0:["$","li",null,{"children":[["$","strong",null,{"children":"Side-effects:"}]," Some instrumentation (like writing to /proc or loading kernel modules) could be disallowed in hardened environments. For example, to profile a GPU, you might load a kernel module (Linux ",["$","code",null,{"children":"nvidia"}]," driver has some profiling hooks). In HPC centers, admins often pre-install and enable needed configs. In cloud, one might need container privileges or special orchestration."]}]
a1:["$","li",null,{"children":"Another APU-specific challenge is timeline alignment – on an APU the CPU and GPU clocks might be more closely coupled, possibly even the same oscillator. This could simplify correlation (no PCIe latency to consider), but tools haven’t exploited any special APU features yet."}]
a2:["$","li",null,{"children":["$","p",null,{"children":[["$","strong",null,{"children":"Scalability:"}]," As HPC moves to exascale, profiling hundreds or thousands of GPUs simultaneously becomes difficult. HPCToolkit and TAU address this by focusing on sampling and profiles (reducing data) rather than giant traces. But even collecting one sample per second from 100,000 nodes is a lot of data. Future directions include ",["$","strong",null,{"children":"online analysis"}]," (profilers that analyze and throw away data on the fly, keeping only summaries or anomalies) and streaming approaches where only interesting events are sent to a central collector. Perhaps AI techniques will be applied to performance data to smartly decide what to log."]}]}]
a3:["$","li",null,{"children":["$","p",null,{"children":[["$","strong",null,{"children":"Automation and Insight Generation:"}]," Right now, profilers present data, and it’s up to the user to infer insights (with some hints from the tool). Nsight Compute has a rules engine to hint at common issues (“memory bound, try increasing concurrency” etc.), but most tools do not automatically pinpoint root causes. A future direction is more automated performance diagnosis. For example, correlating patterns: “Every time GPU is idle, CPU thread X is in a mutex – likely a synchronization bottleneck.” Some research tools or prototype features aim for this kind of expert system. TAU, integrated with tuning frameworks, can perhaps auto-apply instrumentation to suspected hotspots. We might see more ",["$","strong",null,{"children":"performance anomaly detection"}]," built into profilers (especially for always-on monitoring, flagging when an iteration is slower than usual and giving possible reasons)."]}]}]
a4:["$","li",null,{"children":["$","p",null,{"children":[["$","strong",null,{"children":"Energy and Efficiency Metrics:"}]," As accelerators proliferate, power and energy profiling is important. DCGM provides power usage, but few tools integrate that with performance. A limitation in many profilers is they don’t attribute energy usage per kernel easily. In the future, tools could use power telemetry to, for instance, identify phases where energy is wasted (GPU at high power but low utilization). AMD and Intel support some power metrics via RAPL or ROCm-smi; integration of those into performance analysis is ongoing."]}]}]
a5:["$","li",null,{"children":["$","p",null,{"children":[["$","strong",null,{"children":"New Accelerator types:"}]," The term DPU aside, there are also TPUs (Google’s), FPGAs, custom ASICs. Each tends to have its own tools (Google TPU has its profiler integrated in TensorBoard). A challenge for the community is to generalize profiling to any accelerator with a standard interface. A project in this direction is the open-source ",["$","strong",null,{"children":"Kokkos Tools"}]," for node-level profiling which abstract the device. It allows any backend (CUDA, HIP, etc.) to be profiled through a common API. Tools like HPCToolkit and TAU participate in such initiatives to be ready for new devices (if an accelerator vendor provides an OMPT-like or CUPTI-like API, these tools can hook in)."]}]}]
a6:["$","li",null,{"children":["$","p",null,{"children":[["$","strong",null,{"children":"Reverse-engineering Required:"}]," Open-source tools often have to chase undocumented features. E.g., until NVIDIA provided official support, HPCToolkit devs had to figure out how to map GPU PCs to source by parsing ELF symbols in CUBINs, and how to collect GPU metrics via CUPTI’s evolving API. This means open tools lag behind vendor tools when new hardware arrives (the vendors might not update CUPTI immediately, etc.). A future ideal scenario is vendors open up more of their driver’s instrumentation hooks or standardize them. Meanwhile, one can expect that for bleeding-edge features (like new GPU memory types or new DPU offload engines), early profiling might involve creative hacks or limited info until official support catches up."]}]}]
23:[["$","meta","0",{"charSet":"utf-8"}],["$","meta","1",{"name":"viewport","content":"width=device-width, initial-scale=1"}]]
1f:null
21:{"metadata":[["$","title","0",{"children":"GPU Profiling Under the Hood: An Implementation-Focused Survey of Modern Accelerator Tracing Tools | 云微的胡思乱想"}],["$","meta","1",{"name":"robots","content":"index, follow"}],["$","meta","2",{"name":"googlebot","content":"index, follow, max-video-preview:-1, max-image-preview:large, max-snippet:-1"}],["$","link","3",{"rel":"canonical","href":"https://www.yunwei37.com/blog/gpu-profile-tool-impl"}],["$","link","4",{"rel":"alternate","type":"application/rss+xml","href":"https://www.yunwei37.com/feed.xml"}],["$","meta","5",{"property":"og:title","content":"GPU Profiling Under the Hood: An Implementation-Focused Survey of Modern Accelerator Tracing Tools"}],["$","meta","6",{"property":"og:url","content":"https://www.yunwei37.com/blog/gpu-profile-tool-impl"}],["$","meta","7",{"property":"og:site_name","content":"云微的胡思乱想"}],["$","meta","8",{"property":"og:locale","content":"en_US"}],["$","meta","9",{"property":"og:image","content":"https://www.yunwei37.com/static/images/twitter-card.png"}],["$","meta","10",{"property":"og:type","content":"article"}],["$","meta","11",{"property":"article:published_time","content":"2025-04-21T16:00:00.000Z"}],["$","meta","12",{"property":"article:modified_time","content":"2025-04-21T16:00:00.000Z"}],["$","meta","13",{"property":"article:author","content":"Yusheng Zheng (云微)"}],["$","meta","14",{"name":"twitter:card","content":"summary_large_image"}],["$","meta","15",{"name":"twitter:title","content":"GPU Profiling Under the Hood: An Implementation-Focused Survey of Modern Accelerator Tracing Tools"}],["$","meta","16",{"name":"twitter:image","content":"https://www.yunwei37.com/static/images/twitter-card.png"}]],"error":null,"digest":"$undefined"}
26:"$21:metadata"
