<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Towards Agentic OS: An LLM Agent Framework for Linux Schedulers</title>
<!--Generated on Tue Sep 30 02:41:28 2025 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="/static/browse/0.3.4/css/arxiv-html-papers-20250916.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2509.01245v4/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2509.01245v4#S1" title="In Towards Agentic OS: An LLM Agent Framework for Linux Schedulers"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2509.01245v4#S2" title="In Towards Agentic OS: An LLM Agent Framework for Linux Schedulers"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Motivation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2509.01245v4#S3" title="In Towards Agentic OS: An LLM Agent Framework for Linux Schedulers"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>The SchedCP Framework Design and Implementation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2509.01245v4#S4" title="In Towards Agentic OS: An LLM Agent Framework for Linux Schedulers"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>sched-agent: A Multi-Agent Framework for OS Optimization</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2509.01245v4#S5" title="In Towards Agentic OS: An LLM Agent Framework for Linux Schedulers"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Preliminary Evaluation</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line" lang="en">
<h1 class="ltx_title ltx_title_document">Towards Agentic OS: An LLM Agent Framework for Linux Schedulers</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yusheng Zheng<sup class="ltx_sup"><span class="ltx_text ltx_font_italic">1</span></sup>  Yanpeng Hu<sup class="ltx_sup"><span class="ltx_text ltx_font_italic">2</span></sup>  Wei Zhang<sup class="ltx_sup"><span class="ltx_text ltx_font_italic">3</span></sup>  Andi Quinn<sup class="ltx_sup"><span class="ltx_text ltx_font_italic">1</span></sup>
<br class="ltx_break"/><sup class="ltx_sup"><span class="ltx_text ltx_font_italic">1</span></sup>UC Santa Cruz, CA, USA  <sup class="ltx_sup"><span class="ltx_text ltx_font_italic">3</span></sup>University of Connecticut  <sup class="ltx_sup"><span class="ltx_text ltx_font_italic">2</span></sup>ShanghaiTech University, Shanghai, China 
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter">{yzhen165, aquinn1}@ucsc.edu, huyp@shanghaitech.edu.cn</span>
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p">Operating system schedulers suffer from a fundamental semantic gap, where kernel policies fail to understand application-specific needs, leading to suboptimal performance. We introduce SchedCP, the first framework that enables fully autonomous Large Language Model (LLM) agents to safely and efficiently optimize Linux schedulers without human involvement. Our core insight is that the challenge is not merely to <em class="ltx_emph ltx_font_italic">apply</em> a better LLM, but to architect a decoupled control plane that separates the AI’s role of semantic reasoning ("what to optimize") from the system’s role of execution ("how to observe and act"), thereby separating the optimization problem into two stages: goal-inference and policy-synthesis. Implemented as Model Context Protocol(MCP) server, SchedCP provides a stable interface with three key services: a Workload Analysis Engine, an evolving Scheduler Policy Repository, and an Execution Verifier that validates all AI-generated code and configurations before deployment with static and dynamic analysis. We demonstrate this architecture’s power with sched-agent, a multi-agent system that autonomously analyzes workloads, synthesizes custom eBPF scheduling policies, and deploys them via the sched_ext infrastructure. Our evaluation shows that SchedCP achieves up to 1.79x performance improvement and 13x cost reduction compared to naive agentic approaches, all while maintaining high success rate. The code is in <a class="ltx_ref ltx_href" href="https://github.com/eunomia-bpf/schedcp" title="">https://github.com/eunomia-bpf/schedcp</a>.</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p">Operating system schedulers face a fundamental challenge: kernel policies cannot understand what applications need, leading to suboptimal performance as Linux’s EEVDF scheduler <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2509.01245v4#bib.bib9" title="">eevdf2024 </a></cite> applies one-size-fits-all policies to diverse workloads. While sched_ext <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2509.01245v4#bib.bib6" title="">schedext2024 </a></cite> in Linux 6.12 enables custom extended Berkeley Packet Filter(eBPF) schedulers with safety guarantees through verification, developing them still requires both deep kernel expertise and a good understanding of the workloads.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p">Prior reinforcement learning-based schedulers <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2509.01245v4#bib.bib10" title="">mao2019decima </a>; <a class="ltx_ref" href="https://arxiv.org/html/2509.01245v4#bib.bib17" title="">qiu2020firm </a></cite> lack semantic understanding of workloads, are often limited to tweaking configurations within a problem space predefined by human engineers, preventing fully automatic system optimization. While LLMs <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2509.01245v4#bib.bib15" title="">openai2023gpt4 </a>; <a class="ltx_ref" href="https://arxiv.org/html/2509.01245v4#bib.bib1" title="">anthropic2024claude </a></cite> and agent frameworks <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2509.01245v4#bib.bib18" title="">autogen </a>; <a class="ltx_ref" href="https://arxiv.org/html/2509.01245v4#bib.bib14" title="">geminicli </a>; <a class="ltx_ref" href="https://arxiv.org/html/2509.01245v4#bib.bib4" title="">claudecode </a>; <a class="ltx_ref" href="https://arxiv.org/html/2509.01245v4#bib.bib16" title="">qian2024chatdev </a>; <a class="ltx_ref" href="https://arxiv.org/html/2509.01245v4#bib.bib8" title="">hong2023metagpt </a></cite> excel at code generation, naively applying them to scheduler development proves impractical: our experiments show generating a basic scheduler takes 33 minutes, costs $6, and often degrades performance. The gap remains: existing methods lack semantic understanding, while LLMs lack the scaffolding for safe, efficient, and reliable systems integration despite prior LLM-eBPF work <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2509.01245v4#bib.bib20" title="">kgent </a></cite>.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p">Our work enables a system to drive its own optimization by decomposing the problem into two stages: a goal-inference stage that uncovers optimization goals and constraints from workloads, and a policy-synthesis stage that compiles them into eBPF scheduler policies. This approach is embodied in a decoupled architecture with two components. First, SchedCP is a control plane framework providing safe AI-kernel interfaces with profiling, tracing and validation tools, allowing LLMs to customize the kernel scheduler with eBPF. It exposes kernel scheduling features via Model Context Protocol (MCP) through three core services: Workload Analysis Engine, Scheduler Policy Repository, and Execution Verifier. Second, sched-agent is the first autonomous multi-agent system that decomposes scheduler optimization into four specialized agents (Observation, Planning, Execution, Learning), demonstrating how LLMs can bridge the semantic gap between application requirements and kernel scheduling policies. This separation allows SchedCP to provide a generalizable framework for any AI agent, while sched-agent demonstrates semantic workload analysis and policy generation. Our evaluation shows sched-agent achieves up to 1.79× performance on kernel compilation, 2.11× P99 latency improvement and 1.60× throughput gain on schbench, 20% latency reduction for batch workloads, and 13× cost reduction.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Motivation</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p">Linux scheduler optimization faces three barriers. First, a domain knowledge gap exists between developers and users: DevOps engineers lack insight into workload characteristics (latency-sensitive vs. throughput-oriented), while edge/personal device users lack both kernel optimization expertise and understanding of application-specific targets. Second, scheduler development requires mastering kernel programming, limiting innovation to a few experts. Third, modern workloads exhibit complex dynamics: web traffic varies by orders of magnitude daily, build system parallelism changes with dependencies. Prior RL-based schedulers <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2509.01245v4#bib.bib10" title="">mao2019decima </a>; <a class="ltx_ref" href="https://arxiv.org/html/2509.01245v4#bib.bib17" title="">qiu2020firm </a>; <a class="ltx_ref" href="https://arxiv.org/html/2509.01245v4#bib.bib19" title="">zhang2024mrsch </a>; <a class="ltx_ref" href="https://arxiv.org/html/2509.01245v4#bib.bib11" title="">mao2019park </a></cite> require extensive training per workload type, lack semantic understanding to transfer across workloads, and only tweak configurations after engineers have already defined the entire problem space: selecting features, specifying knobs, and writing objective functions. LLMs uniquely bridge these gaps by: (1) using tools to dynamically explore diverse workloads to uncover application intent and structure, which is difficult to capture with conventional static or dynamic analysis; (2) applying a broad, pre-trained understanding of source code semantics to reason about performance trade-offs and structural dependencies, such as data-dependency hints; (3) synthesizing correct eBPF schedulers based on this analysis; and (4) operating in the control plane to generate optimized code that runs natively with negligible runtime overhead, unlike traditional ML models that would cause unacceptable inference latency in the scheduler hot path.</p>
</div>
<div class="ltx_para" id="S2.p2">
<p class="ltx_p">We tested Claude Code<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2509.01245v4#bib.bib4" title="">claudecode </a></cite>, the state-of-the-art LLM agent, with "write a FIFO scheduler in eBPF" from an empty folder, with all permissions and bash access. Of three attempts, only one succeeded. The second attempt produced pseudo-code after 6 minutes trying, and the third generated a scheduler tracer instead after 8 minutes of development. The successful generation required 33 minutes, 221 LLM API calls, and 15+ iterations, costing $6 (vs. 5 minutes typically for an expert developer). The generated code, for some workloads, exhibited poor quality with excessive overhead, performing worse than EEVDF. The agent required root access, could crash the system during testing, and lacked fallback mechanisms, which also raises safety concerns. These experiments reveal three critical challenges: <span class="ltx_text ltx_font_bold">Performance</span>, ensuring AI schedulers outperform existing ones; <span class="ltx_text ltx_font_bold">Safety</span>, preventing crashes, lockups, and starvation while minimizing privileges; and <span class="ltx_text ltx_font_bold">Efficiency</span>, reducing the 33-minute, $6 generation cost for practical deployment.</p>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>The SchedCP Framework Design and Implementation</h2>
<figure class="ltx_figure" id="S3.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="640" id="S3.F1.g1" src="x1.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span class="ltx_text" style="font-size:90%;">
<span class="ltx_text ltx_font_bold">Overall architecture of SchedCP and sched-agent.</span>
SchedCP (bottom) provides the system interface with three core services: Workload Analysis Engine, Scheduler Policy Repository, and Execution Verifier.
sched-agent (top) implements the AI logic through four specialized agents (Observation, Planning, Execution, Learning) working in a closed loop. Red lines show initialization when detecting new workloads, black arrows show optimization loops, and green arrows indicate tool usage by agents.
</span></figcaption>
</figure>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p">As illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2509.01245v4#S3.F1" title="Figure 1 ‣ 3 The SchedCP Framework Design and Implementation ‣ Towards Agentic OS: An LLM Agent Framework for Linux Schedulers"><span class="ltx_text ltx_ref_tag">1</span></a>, SchedCP is a secure control plane acting as an ’API for OS optimization,’ separating systems infrastructure from AI logic, distinguishing “what to optimize” (AI’s domain) from “how to observe and act” (system’s domain). Four key principles govern its design. First, <span class="ltx_text ltx_font_bold">decoupling and role separation</span> ensures future-proofing by treating the AI agent as a performance engineer using a stable set of tools. Second, <span class="ltx_text ltx_font_bold">safety-first interface design</span> addresses the inherent risks of autonomous agents with kernel access by treating AI as potentially non-cautious actors, designing defensive interfaces that prevent catastrophic failures by default, and avoiding granting ‘root’ privileges. Third, <span class="ltx_text ltx_font_bold">adaptive context provisioning</span> addresses LLM agents’ constraints from finite context windows and token costs: agents start with minimal summaries and progressively request details as needed. Finally, <span class="ltx_text ltx_font_bold">composable tool architecture</span> follows Unix philosophy by providing atomic tools that let agents construct complex workflows through their reasoning capabilities, enabling novel solution generation rather than constraining them with rigid workflows. Implemented in  4000 lines of Rust and  6000 lines of Python (including tests), SchedCP provides essential tools for any agent to interact with the Linux kernel’s scheduler, analogous to how an environment in reinforcement learning provides state, actions, and rewards for learning. It exposes its services via the Model Context Protocol (MCP) <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2509.01245v4#bib.bib2" title="">anthropic2024mcp </a></cite> through three primary services:</p>
</div>
<div class="ltx_para" id="S3.p2">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">1. Workload Analysis Engine</span> Provides tiered access to system performance data: (1) cost-effective API endpoints with pre-processed summaries (CPU load, memory usage), (2) secure sandbox access to file reading, application building, Linux profiling tools (<span class="ltx_text ltx_font_typewriter">perf</span>, <span class="ltx_text ltx_font_typewriter">top</span>) and dynamically attachable eBPF probes, (3) feedback channel reporting post-deployment metrics (percentage change in throughput/latency).</p>
</div>
<div class="ltx_para" id="S3.p3">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">2. Scheduler Policy Repository.</span> Database storing executable eBPF scheduler programs with metadata (natural language descriptions, target workloads, historical performance metrics). It provides APIs for semantic search and retrieval, enabling agents to find relevant schedulers or composable code primitives. To support system evolution, it includes endpoints for updating performance metrics and promoting new policies, reducing generation costs by allowing reuse of proven solutions while maintaining a growing library of scheduling strategies.</p>
</div>
<div class="ltx_para" id="S3.p4">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">3. Execution Verifier</span> includes a multi-stage validation pipeline: (1) kernel’s eBPF verifier ensures memory safety and termination, (2) scheduler-specific static analysis detects logic flaws (starvation, unfairness) the standard verifier misses, (3) dynamic validation in secure micro-VM tests correctness and performance. Successful validation issues signed deployment tokens for monitored canary deployments with circuit breakers to revert if performance degrades, eliminating sched-agent’s need for root access.</p>
</div>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>sched-agent: A Multi-Agent Framework for OS Optimization</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p">Building on SchedCP, we developed <span class="ltx_text ltx_font_bold">sched-agent</span>, a multi-agent AI framework implementing in-context reinforcement learning (ICRL)<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2509.01245v4#bib.bib13" title="">incontextrl </a></cite> for scheduler optimization. Using Claude Code’s subagent architecture<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2509.01245v4#bib.bib5" title="">anthropic2024subagents </a></cite>, sched-agent decomposes optimization into four distinct ICRL stages through specialized AI assistants with customized prompts, tools, and separate context windows<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2509.01245v4#bib.bib3" title="">anthropic2024multiagent </a></cite>. The framework integrates with container orchestrators (Kubernetes, Docker) to automatically trigger optimization when applications deploy, enabling adaptive strategy refinement based on performance feedback without model retraining.</p>
</div>
<div class="ltx_para" id="S4.p2">
<p class="ltx_p">The <span class="ltx_text ltx_font_bold">Observation Agent</span> builds Workload Profiles by querying the Workload Analysis Engine strategically, starting with high-level summaries from process name and commands then requesting deeper profiling (<span class="ltx_text ltx_font_typewriter">perf stat</span>, <span class="ltx_text ltx_font_typewriter">top</span>) based on findings, synthesizing data into natural language descriptions and optimization goals while managing cost-precision tradeoffs. For kernel compilation, it produces profiles like “CPU-intensive parallel compilation with short-lived processes, inter-process dependencies, targeting makespan minimization.” The <span class="ltx_text ltx_font_bold">Planning Agent</span> transforms profiles into optimization strategies via the Scheduler Policy Repository, following a decision hierarchy: configuring existing schedulers, generating patches, or composing new schedulers from primitives. The <span class="ltx_text ltx_font_bold">Execution Agent</span> manages development, validation and deployment by synthesizing code artifacts, submitting to the Execution Verifier, interpreting results to refine code or fix logic issues. The <span class="ltx_text ltx_font_bold">Learning Agent</span> completes the ICRL loop by analyzing deployment outcomes (e.g., 45% makespan reduction), enabling in-session adaptation and updating the repository with refined metrics, deployment contexts, and documented antipatterns.</p>
</div>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Preliminary Evaluation</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p">We validate SchedCP’s effectiveness through four research questions: configuring existing schedulers (RQ1), generating new schedulers for specific workloads (RQ2), cost and efficiency of scheduler generation (RQ3), and iterative refinement improvements (RQ4). Evaluation uses two machines: 86-core Intel Xeon 6787P with 758GB RAM running Linux 6.14, and 8-core Intel Core Ultra 7 258V with 30GB RAM running Linux 6.13. Agents use Claude Code (Opus 4), testing each case three times and averaging results. All experiments successfully created working custom scheduler configurations or eBPF programs. Future evaluation requires a complete benchmark.</p>
</div>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Scheduler Configuration</span>: For kernel compilation (tinyconfig, “make -j 172” on 6.14 source), SchedCP achieves 1.63× speedup with scx_rusty initially, then iterative refinement selects scx_layered for 16% additional gain, reaching 1.79× total improvement over EEVDF (Figure <a class="ltx_ref" href="https://arxiv.org/html/2509.01245v4#S5.F2.sf1" title="In Figure 2 ‣ 5 Preliminary Evaluation ‣ Towards Agentic OS: An LLM Agent Framework for Linux Schedulers"><span class="ltx_text ltx_ref_tag">2(a)</span></a>). Pre-trained RL approaches <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2509.01245v4#bib.bib7" title="">corbet2025ml </a></cite> show no improvement, likely because they require costly hardware/workload-specific retraining. On schbench <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2509.01245v4#bib.bib12" title="">schbench2016 </a></cite>, initial AI configuration (scx_bpfland) underperformed, but three refinement iterations identified scx_rusty as superior: 2.11× better P99 latency and 1.60× higher throughput versus EEVDF (Figure <a class="ltx_ref" href="https://arxiv.org/html/2509.01245v4#S5.F2.sf2" title="In Figure 2 ‣ 5 Preliminary Evaluation ‣ Towards Agentic OS: An LLM Agent Framework for Linux Schedulers"><span class="ltx_text ltx_ref_tag">2(b)</span></a>), demonstrating effective learning from feedback.</p>
</div>
<figure class="ltx_figure" id="S5.F2">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" id="S5.F2.fig1" style="width:165.6pt;">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel" id="S5.F2.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="89" id="S5.F2.sf1.g1" src="x2.png" width="317"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">(a)</span> </span><span class="ltx_text" style="font-size:90%;">AI configured scheduler for Linux build time</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel" id="S5.F2.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="78" id="S5.F2.sf2.g1" src="x3.png" width="317"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">(b)</span> </span><span class="ltx_text" style="font-size:90%;">AI configured scheduler for Schbench latency and throughput</span></figcaption>
</figure>
</div>
</div>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" id="S5.F2.sf3" style="width:165.6pt;"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="238" id="S5.F2.sf3.g1" src="x4.png" width="317"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">(c)</span> </span><span class="ltx_text" style="font-size:90%;">AI-generated scheduler for batch workloads</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span class="ltx_text" style="font-size:90%;">Performance evaluation of SchedCP across different workloads</span></figcaption>
</figure>
<div class="ltx_para" id="S5.p3">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">New Scheduler Synthesis</span>: For 8 diverse batch workloads (file compression, video transcoding, software testing, data analytics) with long-tail distributions (40 parallel tasks: 39 short, one long), sched-agent correctly identified the optimization goal and workload pattern, implementing Longest Job First (LJF) scheduling to achieve 20% average latency reduction (Figure <a class="ltx_ref" href="https://arxiv.org/html/2509.01245v4#S5.F2.sf3" title="In Figure 2 ‣ 5 Preliminary Evaluation ‣ Towards Agentic OS: An LLM Agent Framework for Linux Schedulers"><span class="ltx_text ltx_ref_tag">2(c)</span></a>). Claude Opus successfully classified all 8 workloads at $0.15 per analysis, while Claude Sonnet failed. Generation efficiency improved 13× (to 2.5 minutes) with $0.45 synthesis cost per workload, demonstrating economic viability alongside performance gains.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Anthropic.

</span>
<span class="ltx_bibblock">The claude 3 model family: Opus, sonnet, haiku.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">Anthropic Technical Report</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Anthropic.

</span>
<span class="ltx_bibblock">Model context protocol.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.anthropic.com/news/model-context-protocol" title="">https://www.anthropic.com/news/model-context-protocol</a>, 2024.

</span>
<span class="ltx_bibblock">An open standard for connecting AI assistants to data sources.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Anthropic.

</span>
<span class="ltx_bibblock">How we built our multi-agent research system.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.anthropic.com/engineering/built-multi-agent-research-system" title="">https://www.anthropic.com/engineering/built-multi-agent-research-system</a>, 2025.

</span>
<span class="ltx_bibblock">Engineering blog post on multi-agent systems.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Anthropic.

</span>
<span class="ltx_bibblock">Introducing claude code.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.anthropic.com/news/claude-code" title="">https://www.anthropic.com/news/claude-code</a>, Feb 2025.

</span>
<span class="ltx_bibblock">Agentic coding tool announcement, Anthropic blog.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Anthropic.

</span>
<span class="ltx_bibblock">Subagents - claude code documentation.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://docs.anthropic.com/en/docs/claude-code/sub-agents" title="">https://docs.anthropic.com/en/docs/claude-code/sub-agents</a>, 2025.

</span>
<span class="ltx_bibblock">Documentation for Claude Code subagent implementation.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Linux Kernel Community.

</span>
<span class="ltx_bibblock">sched_ext: Bpf scheduler class.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">Linux Kernel Documentation</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Jonathan Corbet.

</span>
<span class="ltx_bibblock">Improved load balancing with machine learning.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">LWN.net</span>, July 2025.

</span>
<span class="ltx_bibblock">Machine learning approaches to Linux kernel scheduling.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Sirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, et al.

</span>
<span class="ltx_bibblock">MetaGPT: Meta programming for multi-agent collaborative framework.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">arXiv preprint arXiv:2308.00352</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Linux Kernel Documentation.

</span>
<span class="ltx_bibblock">Eevdf scheduler - earliest eligible virtual deadline first.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://docs.kernel.org/scheduler/sched-eevdf.html" title="">https://docs.kernel.org/scheduler/sched-eevdf.html</a>, 2024.

</span>
<span class="ltx_bibblock">Introduced in Linux kernel 6.6, replacing CFS.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Hongzi Mao, Malte Schwarzkopf, Shaileshh Bojja Venkatakrishnan, Zili Meng, and Mohammad Alizadeh.

</span>
<span class="ltx_bibblock">Learning scheduling algorithms for data processing clusters.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic">Proceedings of the ACM Special Interest Group on Data Communication (SIGCOMM)</span>, pages 270–288, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Hongzi Mao, Shaileshh Bojja Venkatakrishnan, Malte Schwarzkopf, and Mohammad Alizadeh.

</span>
<span class="ltx_bibblock">Park: An open platform for learning-augmented computer systems.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic">Advances in Neural Information Processing Systems (NeurIPS)</span>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Chris Mason.

</span>
<span class="ltx_bibblock">schbench: Scheduler benchmark.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://kernel.googlesource.com/pub/scm/linux/kernel/git/mason/schbench" title="">https://kernel.googlesource.com/pub/scm/linux/kernel/git/mason/schbench</a>, 2016.

</span>
<span class="ltx_bibblock">A scheduler benchmark that measures wakeup latency and throughput.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Amir Moeini, Jiuqi Wang, Jacob Beck, Ethan Blaser, Shimon Whiteson, Rohan Chandra, and Shangtong Zhang.

</span>
<span class="ltx_bibblock">A survey of in-context reinforcement learning.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">arXiv preprint arXiv:2502.07978</span>, 2025.

</span>
<span class="ltx_bibblock">Version 1, February 11.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Taylor Mullen and Ryan J. Salva.

</span>
<span class="ltx_bibblock">Gemini cli: Your open source ai agent.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://blog.google/technology/developers/introducing-gemini-cli-open-source-ai-agent/" title="">https://blog.google/technology/developers/introducing-gemini-cli-open-source-ai-agent/</a>, 2025.

</span>
<span class="ltx_bibblock">Google Developers Blog, Jun 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
OpenAI.

</span>
<span class="ltx_bibblock">Gpt-4 technical report.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">arXiv preprint arXiv:2303.08774</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Chen Qian, Wei Liu, Hongzhang Liu, Nuo Chen, et al.

</span>
<span class="ltx_bibblock">ChatDev: Communicative agents for software development.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic">Proc. ACL</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Haoran Qiu, Siddhartha Banerjee, Saurabh Jha, Shivaram Kalyanaraman, and Chuan Tang.

</span>
<span class="ltx_bibblock">Firm: An intelligent fine-grained resource management framework for slo-oriented microservices.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic">14th USENIX Symposium on Operating Systems Design and Implementation (OSDI)</span>, pages 805–825, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang, Erkang Zhu, Beibin Li, Li Jiang, Xiaoyun Zhang, and Chi Wang.

</span>
<span class="ltx_bibblock">Autogen: Enable next-gen large language model applications, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Wei Zhang et al.

</span>
<span class="ltx_bibblock">Multi-resource scheduling with reinforcement learning.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">IEEE Transactions on Parallel and Distributed Systems</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Yusheng Zheng, Yiwei Yang, Maolin Chen, and Andrew Quinn.

</span>
<span class="ltx_bibblock">Kgent: Kernel extensions large language model agent.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic">Proceedings of the ACM SIGCOMM 2024 Workshop on EBPF and Kernel Extensions</span>, eBPF ’24, page 30–36, New York, NY, USA, 2024. Association for Computing Machinery.

</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Tue Sep 30 02:41:28 2025 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
